{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"BERTopic \u00b6 BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Corresponding medium post can be found here . About the Project \u00b6 The initial purpose of this project was to generalize Top2Vec such that it could be used with state-of-art pre-trained transformer models. However, this proved difficult due to the different natures of Doc2Vec and transformer models. Instead, I decided to come up with a different algorithm that could use BERT and \ud83e\udd17 transformers embeddings. The results is BERTopic , an algorithm for generating topics using state-of-the-art embeddings. Installation \u00b6 PyTorch 1.2.0 or higher is recommended. If the install below gives an error, please install pytorch first here . Installation can be done using pypi : pip install bertopic Usage \u00b6 Below is an example of how to use the model. The example uses the 20 newsgroups dataset. from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics = model . fit_transform ( docs ) The resulting topics can be accessed through model.get_topic(topic) : >>> model . get_topic ( 9 ) [( 'game' , 0.005251396890032802 ), ( 'team' , 0.00482651185323754 ), ( 'hockey' , 0.004335032060690186 ), ( 'players' , 0.0034782716706978963 ), ( 'games' , 0.0032873248432630227 ), ( 'season' , 0.003218987432255393 ), ( 'play' , 0.0031855141725669637 ), ( 'year' , 0.002962343114817677 ), ( 'nhl' , 0.0029577648449943144 ), ( 'baseball' , 0.0029245163154193524 )] You can find an overview of all models currently in BERTopic here and here . Overview \u00b6 Methods Code Returns Access single topic model.get_topic(12) Tuple[Word, Score] Access all topics model.get_topic() List[Tuple[Word, Score]] Get single topic freq model.get_topic_freq(12) int Get all topic freq model.get_topics_freq() DataFrame Fit the model model.fit(docs]) - Predict new documents model.transform([new_doc]) List[int] Save model model.save(\"my_model\") - Load model BERTopic.load(\"my_model\") - NOTE : The embeddings itself are not preserved in the model as they are only vital for creating the clusters. Therefore, it is advised to only use fit and then transform if you are looking to generalize the model to new documents. For existing documents, it is best to use fit_transform directly as it only needs to generate the document embeddings once. Google Colaboratory \u00b6 Since we are using transformer-based embeddings you might want to leverage gpu-acceleration to speed up the model. For that, I have created a tutorial Google Colab Notebook that you can use to run the model as shown above. If you want to tweak the inner workings or follow along with the medium post, use this notebook instead. References \u00b6 Angelov, D. (2020). Top2Vec: Distributed Representations of Topics. arXiv preprint arXiv :2008.09470.","title":"Index"},{"location":"index.html#bertopic","text":"BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Corresponding medium post can be found here .","title":"BERTopic"},{"location":"index.html#about-the-project","text":"The initial purpose of this project was to generalize Top2Vec such that it could be used with state-of-art pre-trained transformer models. However, this proved difficult due to the different natures of Doc2Vec and transformer models. Instead, I decided to come up with a different algorithm that could use BERT and \ud83e\udd17 transformers embeddings. The results is BERTopic , an algorithm for generating topics using state-of-the-art embeddings.","title":"About the Project"},{"location":"index.html#installation","text":"PyTorch 1.2.0 or higher is recommended. If the install below gives an error, please install pytorch first here . Installation can be done using pypi : pip install bertopic","title":"Installation"},{"location":"index.html#usage","text":"Below is an example of how to use the model. The example uses the 20 newsgroups dataset. from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics = model . fit_transform ( docs ) The resulting topics can be accessed through model.get_topic(topic) : >>> model . get_topic ( 9 ) [( 'game' , 0.005251396890032802 ), ( 'team' , 0.00482651185323754 ), ( 'hockey' , 0.004335032060690186 ), ( 'players' , 0.0034782716706978963 ), ( 'games' , 0.0032873248432630227 ), ( 'season' , 0.003218987432255393 ), ( 'play' , 0.0031855141725669637 ), ( 'year' , 0.002962343114817677 ), ( 'nhl' , 0.0029577648449943144 ), ( 'baseball' , 0.0029245163154193524 )] You can find an overview of all models currently in BERTopic here and here .","title":"Usage"},{"location":"index.html#overview","text":"Methods Code Returns Access single topic model.get_topic(12) Tuple[Word, Score] Access all topics model.get_topic() List[Tuple[Word, Score]] Get single topic freq model.get_topic_freq(12) int Get all topic freq model.get_topics_freq() DataFrame Fit the model model.fit(docs]) - Predict new documents model.transform([new_doc]) List[int] Save model model.save(\"my_model\") - Load model BERTopic.load(\"my_model\") - NOTE : The embeddings itself are not preserved in the model as they are only vital for creating the clusters. Therefore, it is advised to only use fit and then transform if you are looking to generalize the model to new documents. For existing documents, it is best to use fit_transform directly as it only needs to generate the document embeddings once.","title":"Overview"},{"location":"index.html#google-colaboratory","text":"Since we are using transformer-based embeddings you might want to leverage gpu-acceleration to speed up the model. For that, I have created a tutorial Google Colab Notebook that you can use to run the model as shown above. If you want to tweak the inner workings or follow along with the medium post, use this notebook instead.","title":"Google Colaboratory"},{"location":"index.html#references","text":"Angelov, D. (2020). Top2Vec: Distributed Representations of Topics. arXiv preprint arXiv :2008.09470.","title":"References"},{"location":"algorithm.html","text":"The Algorithm \u00b6 The algorithm contains, roughly, 3 stages: Extract document embeddings with Sentence Transformers Cluster document embeddings to create groups of similar documents with UMAP and HDBSCAN Extract and reduce topics with c-TF-IDF Sentence Transformer \u00b6 We start by creating document embeddings from a set of documents using sentence-transformer . These models are pre-trained for many language and are great for creating either document- or sentence-embeddings. If you have long documents, I would advise you to split up your documents into paragraphs or sentences as a BERT-based model in sentence-transformer typically has a token limit. UMAP + HDBSCAN \u00b6 Next, in order to cluster the documents using a clustering algorithm such as HDBSCAN we first need to reduce its dimensionality as HDBCAN is prone to the curse of dimensionality. Thus, we first lower dimensionality with UMAP as it preserves local structure well after which we can use HDBSCAN to cluster similar documents. c-TF-IDF \u00b6 What we want to know from the clusters that we generated, is what makes one cluster, based on their content, different from another? To solve this, we can modify TF-IDF such that it allows for interesting words per topic instead of per document. When you apply TF-IDF as usual on a set of documents, what you are basically doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of topics ! Each cluster is converted to a single document instead of a set of documents. Then, the frequency of word t are extracted for each class i and divided by the total number of words w . This action can now be seen as a form of regularization of frequent words in the class. Next, the total, unjoined, number of documents m is divided by the total frequency of word t across all classes n .","title":"Algorithm"},{"location":"algorithm.html#the-algorithm","text":"The algorithm contains, roughly, 3 stages: Extract document embeddings with Sentence Transformers Cluster document embeddings to create groups of similar documents with UMAP and HDBSCAN Extract and reduce topics with c-TF-IDF","title":"The Algorithm"},{"location":"algorithm.html#sentence-transformer","text":"We start by creating document embeddings from a set of documents using sentence-transformer . These models are pre-trained for many language and are great for creating either document- or sentence-embeddings. If you have long documents, I would advise you to split up your documents into paragraphs or sentences as a BERT-based model in sentence-transformer typically has a token limit.","title":"Sentence Transformer"},{"location":"algorithm.html#umap-hdbscan","text":"Next, in order to cluster the documents using a clustering algorithm such as HDBSCAN we first need to reduce its dimensionality as HDBCAN is prone to the curse of dimensionality. Thus, we first lower dimensionality with UMAP as it preserves local structure well after which we can use HDBSCAN to cluster similar documents.","title":"UMAP + HDBSCAN"},{"location":"algorithm.html#c-tf-idf","text":"What we want to know from the clusters that we generated, is what makes one cluster, based on their content, different from another? To solve this, we can modify TF-IDF such that it allows for interesting words per topic instead of per document. When you apply TF-IDF as usual on a set of documents, what you are basically doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of topics ! Each cluster is converted to a single document instead of a set of documents. Then, the frequency of word t are extracted for each class i and divided by the total number of words w . This action can now be seen as a form of regularization of frequent words in the class. Next, the total, unjoined, number of documents m is divided by the total frequency of word t across all classes n .","title":"c-TF-IDF"},{"location":"api/bertopic.html","text":"BERTopic \u00b6 Transformer-based model for Topic Modeling Parameters bert_model, str, default 'distilbert-base-nli-mean-tokens' Model to use. Overview of options: https://www.sbert.net/docs/pretrained_models.html top_n_words : int, default 20 The number of words per topic to extract nr_topics : int, default None Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. IF this is set to None, no reduction is applied. n_gram_range : Tuple[int (low), int (high)], default (1, 1) The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. min_topic_size : int, optional (default=30) The minimum size of the topic. n_neighbors: int, default 15 The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation (UMAP). n_components: int, default 5 The dimension of the space to embed into when reducing dimensionality with UMAP. verbose, bool, optional (default=False) Changes the verbosity of the model, Set to True if you want to track the stages of the model. fit ( self , documents ) \u00b6 Show source code in bertopic\\model.py 88 89 90 91 92 93 94 95 96 97 98 def fit ( self , documents : List [ str ]): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Parameters ---------- documents : List[str] A list of documents to fit on \"\"\" check_documents_type ( documents ) self . fit_transform ( documents ) return self Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Parameters \u00b6 documents : List[str] A list of documents to fit on fit_transform ( self , documents , debug = False ) \u00b6 Show source code in bertopic\\model.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def fit_transform ( self , documents : List [ str ], debug : bool = False ) -> Union [ List [ int ], Tuple [ pd . DataFrame , np . ndarray , np . ndarray , np . ndarray ]]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Parameters ---------- documents : List[str] A list of documents to fit on debug : bool, default False Whether to return all intermediate results Returns ------- predictions : List[int] Topic predictions for each documents \"\"\" check_documents_type ( documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract BERT sentence embeddings embeddings = self . _extract_embeddings ( documents . Document ) # Reduce dimensionality with UMAP umap_embeddings = self . _reduce_dimensionality ( embeddings ) # Cluster UMAP embeddings with HDBSCAN documents = self . _cluster_embeddings ( umap_embeddings , documents ) # Extract topics by calculating c-TF-IDF c_tf_idf = self . _extract_topics ( documents ) if self . nr_topics : documents = self . _reduce_topics ( documents , c_tf_idf ) predictions = documents . Topic . to_list () if debug : return documents , embeddings , umap_embeddings , c_tf_idf return predictions Fit the models on a collection of documents, generate topics, and return the docs with topics Parameters \u00b6 documents : List[str] A list of documents to fit on debug : bool, default False Whether to return all intermediate results Returns \u00b6 predictions : List[int] Topic predictions for each documents get_topic ( self , topic ) \u00b6 Show source code in bertopic\\model.py 336 337 338 def get_topic ( self , topic : int ) -> Dict [ str , Tuple [ str , float ]]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores \"\"\" return self . topics [ topic ] Return top n words for a specific topic and their c-TF-IDF scores get_topic_freq ( self , topic ) \u00b6 Show source code in bertopic\\model.py 344 345 346 def get_topic_freq ( self , topic : int ) -> int : \"\"\" Return the the size of a topic \"\"\" return self . topic_sizes . items ()[ topic ] Return the the size of a topic get_topics ( self ) \u00b6 Show source code in bertopic\\model.py 332 333 334 def get_topics ( self ) -> Dict [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score \"\"\" return self . topics Return topics with top n words and their c-TF-IDF score get_topics_freq ( self ) \u00b6 Show source code in bertopic\\model.py 340 341 342 def get_topics_freq ( self ) -> pd . DataFrame : \"\"\" Return the the size of topics (descending order) \"\"\" return pd . DataFrame ( self . topic_sizes . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False ) Return the the size of topics (descending order) load ( path ) (classmethod) \u00b6 Show source code in bertopic\\model.py 397 398 399 400 401 @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path \"\"\" with open ( path , 'rb' ) as file : return joblib . load ( file ) Loads the model from the specified path save ( self , path ) \u00b6 Show source code in bertopic\\model.py 392 393 394 395 def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file ) Saves the model to the specified path transform ( self , documents ) \u00b6 Show source code in bertopic\\model.py 149 150 151 152 153 154 155 156 157 158 159 160 161 def transform ( self , documents : Union [ str , List [ str ]]) -> List [ int ]: \"\"\" After having fit a model, use transform to predict new instances \"\"\" if isinstance ( documents , str ): documents = [ documents ] embeddings = self . _extract_embeddings ( documents ) umap_embeddings = self . umap_model . transform ( embeddings ) predictions , strengths = hdbscan . approximate_predict ( self . cluster_model , umap_embeddings ) if self . mapped_topics : predictions = self . _map_predictions ( predictions ) return predictions After having fit a model, use transform to predict new instances","title":"BERTopic"},{"location":"api/bertopic.html#bertopic","text":"Transformer-based model for Topic Modeling Parameters bert_model, str, default 'distilbert-base-nli-mean-tokens' Model to use. Overview of options: https://www.sbert.net/docs/pretrained_models.html top_n_words : int, default 20 The number of words per topic to extract nr_topics : int, default None Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. IF this is set to None, no reduction is applied. n_gram_range : Tuple[int (low), int (high)], default (1, 1) The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. min_topic_size : int, optional (default=30) The minimum size of the topic. n_neighbors: int, default 15 The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation (UMAP). n_components: int, default 5 The dimension of the space to embed into when reducing dimensionality with UMAP. verbose, bool, optional (default=False) Changes the verbosity of the model, Set to True if you want to track the stages of the model.","title":"BERTopic"},{"location":"api/bertopic.html#bertopic.model.BERTopic.fit","text":"Show source code in bertopic\\model.py 88 89 90 91 92 93 94 95 96 97 98 def fit ( self , documents : List [ str ]): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Parameters ---------- documents : List[str] A list of documents to fit on \"\"\" check_documents_type ( documents ) self . fit_transform ( documents ) return self Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics","title":"fit()"},{"location":"api/bertopic.html#parameters","text":"documents : List[str] A list of documents to fit on","title":"Parameters"},{"location":"api/bertopic.html#bertopic.model.BERTopic.fit_transform","text":"Show source code in bertopic\\model.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def fit_transform ( self , documents : List [ str ], debug : bool = False ) -> Union [ List [ int ], Tuple [ pd . DataFrame , np . ndarray , np . ndarray , np . ndarray ]]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Parameters ---------- documents : List[str] A list of documents to fit on debug : bool, default False Whether to return all intermediate results Returns ------- predictions : List[int] Topic predictions for each documents \"\"\" check_documents_type ( documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract BERT sentence embeddings embeddings = self . _extract_embeddings ( documents . Document ) # Reduce dimensionality with UMAP umap_embeddings = self . _reduce_dimensionality ( embeddings ) # Cluster UMAP embeddings with HDBSCAN documents = self . _cluster_embeddings ( umap_embeddings , documents ) # Extract topics by calculating c-TF-IDF c_tf_idf = self . _extract_topics ( documents ) if self . nr_topics : documents = self . _reduce_topics ( documents , c_tf_idf ) predictions = documents . Topic . to_list () if debug : return documents , embeddings , umap_embeddings , c_tf_idf return predictions Fit the models on a collection of documents, generate topics, and return the docs with topics","title":"fit_transform()"},{"location":"api/bertopic.html#parameters_1","text":"documents : List[str] A list of documents to fit on debug : bool, default False Whether to return all intermediate results","title":"Parameters"},{"location":"api/bertopic.html#returns","text":"predictions : List[int] Topic predictions for each documents","title":"Returns"},{"location":"api/bertopic.html#bertopic.model.BERTopic.get_topic","text":"Show source code in bertopic\\model.py 336 337 338 def get_topic ( self , topic : int ) -> Dict [ str , Tuple [ str , float ]]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores \"\"\" return self . topics [ topic ] Return top n words for a specific topic and their c-TF-IDF scores","title":"get_topic()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.get_topic_freq","text":"Show source code in bertopic\\model.py 344 345 346 def get_topic_freq ( self , topic : int ) -> int : \"\"\" Return the the size of a topic \"\"\" return self . topic_sizes . items ()[ topic ] Return the the size of a topic","title":"get_topic_freq()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.get_topics","text":"Show source code in bertopic\\model.py 332 333 334 def get_topics ( self ) -> Dict [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score \"\"\" return self . topics Return topics with top n words and their c-TF-IDF score","title":"get_topics()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.get_topics_freq","text":"Show source code in bertopic\\model.py 340 341 342 def get_topics_freq ( self ) -> pd . DataFrame : \"\"\" Return the the size of topics (descending order) \"\"\" return pd . DataFrame ( self . topic_sizes . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False ) Return the the size of topics (descending order)","title":"get_topics_freq()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.load","text":"Show source code in bertopic\\model.py 397 398 399 400 401 @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path \"\"\" with open ( path , 'rb' ) as file : return joblib . load ( file ) Loads the model from the specified path","title":"load()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.save","text":"Show source code in bertopic\\model.py 392 393 394 395 def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file ) Saves the model to the specified path","title":"save()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.transform","text":"Show source code in bertopic\\model.py 149 150 151 152 153 154 155 156 157 158 159 160 161 def transform ( self , documents : Union [ str , List [ str ]]) -> List [ int ]: \"\"\" After having fit a model, use transform to predict new instances \"\"\" if isinstance ( documents , str ): documents = [ documents ] embeddings = self . _extract_embeddings ( documents ) umap_embeddings = self . umap_model . transform ( embeddings ) predictions , strengths = hdbscan . approximate_predict ( self . cluster_model , umap_embeddings ) if self . mapped_topics : predictions = self . _map_predictions ( predictions ) return predictions After having fit a model, use transform to predict new instances","title":"transform()"},{"location":"api/ctfidf.html","text":"c-TF-IDF \u00b6 fit ( self , X , n_samples ) \u00b6 Show source code in bertopic\\ctfidf.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def fit ( self , X , n_samples ): \"\"\"Learn the idf vector (global term weights). Parameters ---------- X : sparse matrix of shape n_samples, n_features) A matrix of term/token counts. \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape self . df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) idf = np . log ( n_samples / self . df ) self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self Learn the idf vector (global term weights). Parameters X : sparse matrix of shape n_samples, n_features) A matrix of term/token counts. transform ( self , X , copy = True ) \u00b6 Show source code in bertopic\\ctfidf.py 36 37 38 39 40 41 42 def transform ( self , X , copy = True ): if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) X = X * self . _idf_diag return X Transform a count matrix to a tf or tf-idf representation Parameters \u00b6 X : sparse matrix of (n_samples, n_features) a matrix of term/token counts copy : bool, default=True Whether to copy X and operate on the copy or perform in-place operations. Returns \u00b6 vectors : sparse matrix of shape (n_samples, n_features)","title":"cTFIDF"},{"location":"api/ctfidf.html#c-tf-idf","text":"","title":"c-TF-IDF"},{"location":"api/ctfidf.html#bertopic.ctfidf.ClassTFIDF.fit","text":"Show source code in bertopic\\ctfidf.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def fit ( self , X , n_samples ): \"\"\"Learn the idf vector (global term weights). Parameters ---------- X : sparse matrix of shape n_samples, n_features) A matrix of term/token counts. \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape self . df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) idf = np . log ( n_samples / self . df ) self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self Learn the idf vector (global term weights). Parameters X : sparse matrix of shape n_samples, n_features) A matrix of term/token counts.","title":"fit()"},{"location":"api/ctfidf.html#bertopic.ctfidf.ClassTFIDF.transform","text":"Show source code in bertopic\\ctfidf.py 36 37 38 39 40 41 42 def transform ( self , X , copy = True ): if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) X = X * self . _idf_diag return X Transform a count matrix to a tf or tf-idf representation","title":"transform()"},{"location":"api/ctfidf.html#parameters","text":"X : sparse matrix of (n_samples, n_features) a matrix of term/token counts copy : bool, default=True Whether to copy X and operate on the copy or perform in-place operations.","title":"Parameters"},{"location":"api/ctfidf.html#returns","text":"vectors : sparse matrix of shape (n_samples, n_features)","title":"Returns"}]}