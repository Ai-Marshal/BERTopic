{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"BERTopic \u00b6 BERTopic is a topic modeling technique that leverages transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. It even supports visualizations similar to LDAvis! Corresponding medium posts can be found here and here . Installation \u00b6 Installation, with sentence-transformers, can be done using pypi : pip install bertopic You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: To use Flair embeddings, install BERTopic as follows: pip install bertopic [ flair ] pip install bertopic [ gensim ] pip install bertopic [ spacy ] pip install bertopic [ use ] To install all backends: pip install bertopic [ all ] Quick Start \u00b6 We start by extracting topics from the well-known 20 newsgroups dataset which is comprised of english documents: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) After generating topics and their probabilities, we can access the frequent topics that were generated: >>> topic_model . get_topic_info () Topic Count Name - 1 4630 - 1 _can_your_will_any 0 693 49 _windows_drive_dos_file 1 466 32 _jesus_bible_christian_faith 2 441 2 _space_launch_orbit_lunar 3 381 22 _key_encryption_keys_encrypted -1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated, topic 0: >>> topic_model . get_topic ( 0 ) [( 'windows' , 0.006152228076250982 ), ( 'drive' , 0.004982897610645755 ), ( 'dos' , 0.004845038866360651 ), ( 'file' , 0.004140142872194834 ), ( 'disk' , 0.004131678774810884 ), ( 'mac' , 0.003624848635985097 ), ( 'memory' , 0.0034840976976789903 ), ( 'software' , 0.0034415334250699077 ), ( 'email' , 0.0034239554442333257 ), ( 'pc' , 0.003047105930670237 )] NOTE : Use BERTopic(language=\"multilingual\") to select a model that supports 50+ languages. Overview \u00b6 For quick access to common functions, here is an overview of BERTopic's main methods: Method Code Fit the model BERTopic().fit(docs) Fit the model and predict documents BERTopic().fit_transform(docs) Predict new documents BERTopic().transform([new_doc]) Access single topic BERTopic().get_topic(topic=12) Access all topics BERTopic().get_topics() Get topic freq BERTopic().get_topic_freq() Get all topic information BERTopic().get_topic_info() Get topics per class BERTopic().topics_per_class(docs, topics, classes) Dynamic Topic Modeling BERTopic().topics_over_time(docs, topics, timestamps) Update topic representation BERTopic().update_topics(docs, topics, n_gram_range=(1, 3)) Reduce nr of topics BERTopic().reduce_topics(docs, topics, nr_topics=30) Find topics BERTopic().find_topics(\"vehicle\") Save model BERTopic().save(\"my_model\") Load model BERTopic.load(\"my_model\") Get parameters BERTopic().get_params() For an overview of BERTopic's visualization methods: Method Code Visualize Topics BERTopic().visualize_topics() Visualize Topic Hierarchy BERTopic().visualize_hierarchy() Visualize Topic Terms BERTopic().visualize_barchart() Visualize Topic Similarity BERTopic().visualize_heatmap() Visualize Term Score Decline BERTopic().visualize_term_rank() Visualize Topic Probability Distribution BERTopic().visualize_distribution(probs[0]) Visualize Topics over Time BERTopic().visualize_topics_over_time(topics_over_time) Visualize Topics per Class BERTopic().visualize_topics_per_class(topics_per_class) Citation \u00b6 To cite BERTopic in your work, please use the following bibtex reference: @misc { grootendorst2020bertopic , author = {Maarten Grootendorst} , title = {BERTopic: Leveraging BERT and c-TF-IDF to create easily interpretable topics.} , year = 2020 , publisher = {Zenodo} , version = {v0.7.0} , doi = {10.5281/zenodo.4381785} , url = {https://doi.org/10.5281/zenodo.4381785} }","title":"Home"},{"location":"index.html#bertopic","text":"BERTopic is a topic modeling technique that leverages transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. It even supports visualizations similar to LDAvis! Corresponding medium posts can be found here and here .","title":"BERTopic"},{"location":"index.html#installation","text":"Installation, with sentence-transformers, can be done using pypi : pip install bertopic You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: To use Flair embeddings, install BERTopic as follows: pip install bertopic [ flair ] pip install bertopic [ gensim ] pip install bertopic [ spacy ] pip install bertopic [ use ] To install all backends: pip install bertopic [ all ]","title":"Installation"},{"location":"index.html#quick-start","text":"We start by extracting topics from the well-known 20 newsgroups dataset which is comprised of english documents: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) After generating topics and their probabilities, we can access the frequent topics that were generated: >>> topic_model . get_topic_info () Topic Count Name - 1 4630 - 1 _can_your_will_any 0 693 49 _windows_drive_dos_file 1 466 32 _jesus_bible_christian_faith 2 441 2 _space_launch_orbit_lunar 3 381 22 _key_encryption_keys_encrypted -1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated, topic 0: >>> topic_model . get_topic ( 0 ) [( 'windows' , 0.006152228076250982 ), ( 'drive' , 0.004982897610645755 ), ( 'dos' , 0.004845038866360651 ), ( 'file' , 0.004140142872194834 ), ( 'disk' , 0.004131678774810884 ), ( 'mac' , 0.003624848635985097 ), ( 'memory' , 0.0034840976976789903 ), ( 'software' , 0.0034415334250699077 ), ( 'email' , 0.0034239554442333257 ), ( 'pc' , 0.003047105930670237 )] NOTE : Use BERTopic(language=\"multilingual\") to select a model that supports 50+ languages.","title":"Quick Start"},{"location":"index.html#overview","text":"For quick access to common functions, here is an overview of BERTopic's main methods: Method Code Fit the model BERTopic().fit(docs) Fit the model and predict documents BERTopic().fit_transform(docs) Predict new documents BERTopic().transform([new_doc]) Access single topic BERTopic().get_topic(topic=12) Access all topics BERTopic().get_topics() Get topic freq BERTopic().get_topic_freq() Get all topic information BERTopic().get_topic_info() Get topics per class BERTopic().topics_per_class(docs, topics, classes) Dynamic Topic Modeling BERTopic().topics_over_time(docs, topics, timestamps) Update topic representation BERTopic().update_topics(docs, topics, n_gram_range=(1, 3)) Reduce nr of topics BERTopic().reduce_topics(docs, topics, nr_topics=30) Find topics BERTopic().find_topics(\"vehicle\") Save model BERTopic().save(\"my_model\") Load model BERTopic.load(\"my_model\") Get parameters BERTopic().get_params() For an overview of BERTopic's visualization methods: Method Code Visualize Topics BERTopic().visualize_topics() Visualize Topic Hierarchy BERTopic().visualize_hierarchy() Visualize Topic Terms BERTopic().visualize_barchart() Visualize Topic Similarity BERTopic().visualize_heatmap() Visualize Term Score Decline BERTopic().visualize_term_rank() Visualize Topic Probability Distribution BERTopic().visualize_distribution(probs[0]) Visualize Topics over Time BERTopic().visualize_topics_over_time(topics_over_time) Visualize Topics per Class BERTopic().visualize_topics_per_class(topics_per_class)","title":"Overview"},{"location":"index.html#citation","text":"To cite BERTopic in your work, please use the following bibtex reference: @misc { grootendorst2020bertopic , author = {Maarten Grootendorst} , title = {BERTopic: Leveraging BERT and c-TF-IDF to create easily interpretable topics.} , year = 2020 , publisher = {Zenodo} , version = {v0.7.0} , doi = {10.5281/zenodo.4381785} , url = {https://doi.org/10.5281/zenodo.4381785} }","title":"Citation"},{"location":"changelog.html","text":"Version 0.9 \u00b6 Release date: 9 August, 2021 Highlights : Implemented a Guided BERTopic -> Use seeds to steer the Topic Modeling Get the most representative documents per topic: topic_model.get_representative_docs(topic=1) This allows users to see which documents are good representations of a topic and better understand the topics that were created Added normalize_frequency parameter to visualize_topics_per_class and visualize_topics_over_time in order to better compare the relative topic frequencies between topics Return flat probabilities as default, only calculate the probabilities of all topics per document if calculate_probabilities is True Added several FAQs Fixes : Fix loading pre-trained BERTopic model Fix mapping of probabilities Fix #190 Guided BERTopic : Guided BERTopic works in two ways: First, we create embeddings for each seeded topics by joining them and passing them through the document embedder. These embeddings will be compared with the existing document embeddings through cosine similarity and assigned a label. If the document is most similar to a seeded topic, then it will get that topic's label. If it is most similar to the average document embedding, it will get the -1 label. These labels are then passed through UMAP to create a semi-supervised approach that should nudge the topic creation to the seeded topics. Second, we take all words in seed_topic_list and assign them a multiplier larger than 1. Those multipliers will be used to increase the IDF values of the words across all topics thereby increasing the likelihood that a seeded topic word will appear in a topic. This does, however, also increase the chance of an irrelevant topic having unrelated words. In practice, this should not be an issue since the IDF value is likely to remain low regardless of the multiplier. The multiplier is now a fixed value but may change to something more elegant, like taking the distribution of IDF values and its position into account when defining the multiplier. seed_topic_list = [[ \"company\" , \"billion\" , \"quarter\" , \"shrs\" , \"earnings\" ], [ \"acquisition\" , \"procurement\" , \"merge\" ], [ \"exchange\" , \"currency\" , \"trading\" , \"rate\" , \"euro\" ], [ \"grain\" , \"wheat\" , \"corn\" ], [ \"coffee\" , \"cocoa\" ], [ \"natural\" , \"gas\" , \"oil\" , \"fuel\" , \"products\" , \"petrol\" ]] topic_model = BERTopic ( seed_topic_list = seed_topic_list ) topics , probs = topic_model . fit_transform ( docs ) Version 0.8.1 \u00b6 Release date: 08 June, 2021 Highlights : Improved models: For English documents the default is now: \"paraphrase-MiniLM-L6-v2\" For Non-English or multi-lingual documents the default is now: \"paraphrase-multilingual-MiniLM-L12-v2\" Both models show not only great performance but are much faster! Add interactive visualizations to the plotting API documentation For better performance, please use the following models: English: \"paraphrase-mpnet-base-v2\" Non-English or multi-lingual: \"paraphrase-multilingual-mpnet-base-v2\" Fixes : Improved unit testing for more stability Set transformers version for Flair Version 0.8.0 \u00b6 Release date: 31 May, 2021 Highlights : Additional visualizations: Topic Hierarchy: topic_model.visualize_hierarchy() Topic Similarity Heatmap: topic_model.visualize_heatmap() Topic Representation Barchart: topic_model.visualize_barchart() Term Score Decline: topic_model.visualize_term_rank() Created bertopic.plotting library to easily extend visualizations Improved automatic topic reduction by using HDBSCAN to detect similar topics Sort topic ids by their frequency. -1 is the outlier class and contains typically the most documents. After that 0 is the largest topic, 1 the second largest, etc. Fixes : Fix typo #113 , #117 Fix #121 by removing these two lines Fix mapping of topics after reduction (it now excludes 0) ( #103 ) Version 0.7.0 \u00b6 Release date: 26 April, 2021 The two main features are (semi-)supervised topic modeling and several backends to use instead of Flair and SentenceTransformers! Highlights : (semi-)supervised topic modeling by leveraging supervised options in UMAP model.fit(docs, y=target_classes) Backends: Added Spacy, Gensim, USE (TFHub) Use a different backend for document embeddings and word embeddings Create your own backends with bertopic.backend.BaseEmbedder Click here for an overview of all new backends Calculate and visualize topics per class Calculate: topics_per_class = topic_model.topics_per_class(docs, topics, classes) Visualize: topic_model.visualize_topics_per_class(topics_per_class) Several tutorials were updated and added: Name Link Topic Modeling with BERTopic (Custom) Embedding Models in BERTopic Advanced Customization in BERTopic (semi-)Supervised Topic Modeling with BERTopic Dynamic Topic Modeling with Trump's Tweets Fixes : Fixed issues with Torch req Prevent saving term frequency matrix in CTFIDF class Fixed DTM not working when reducing topics ( #96 ) Moved visualization dependencies to base BERTopic pip install bertopic[visualization] becomes pip install bertopic Allow precomputed embeddings in bertopic.find_topics() ( #79 ): model = BERTopic ( embedding_model = my_embedding_model ) model . fit ( docs , my_precomputed_embeddings ) model . find_topics ( search_term ) Version 0.6.0 \u00b6 Release date: 1 March, 2021 Highlights : DTM: Added a basic dynamic topic modeling technique based on the global c-TF-IDF representation model.topics_over_time(docs, timestamps, global_tuning=True) DTM: Option to evolve topics based on t-1 c-TF-IDF representation which results in evolving topics over time Only uses topics at t-1 and skips evolution if there is a gap model.topics_over_time(docs, timestamps, evolution_tuning=True) DTM: Function to visualize topics over time model.visualize_topics_over_time(topics_over_time) DTM: Add binning of timestamps model.topics_over_time(docs, timestamps, nr_bins=10) Add function get general information about topics (id, frequency, name, etc.) get_topic_info() Improved stability of c-TF-IDF by taking the average number of words across all topics instead of the number of documents Fixes : _map_probabilities() does not take into account that there is no probability of the outlier class and the probabilities are mutated instead of copied (#63, #64) Version 0.5.0 \u00b6 Release date: 8 Februari, 2021 Highlights : Add Flair to allow for more (custom) token/document embeddings, including \ud83e\udd17 transformers Option to use custom UMAP, HDBSCAN, and CountVectorizer Added low_memory parameter to reduce memory during computation Improved verbosity (shows progress bar) Return the figure of visualize_topics() Expose all parameters with a single function: get_params() Fixes : To simplify the API, the parameters stop_words and n_neighbors were removed. These can still be used when a custom UMAP or CountVectorizer is used. Set calculate_probabilities to False as a default. Calculating probabilities with HDBSCAN significantly increases computation time and memory usage. Better to remove calculating probabilities or only allow it by manually turning this on. Use the newest version of sentence-transformers as it speeds ups encoding significantly Version 0.4.2 \u00b6 Release date: 10 Januari, 2021 Fixes : Selecting embedding_model did not work when language was also used. This led to the user needing to set language to None before being able to use embedding_model . Fixed by using embedding_model when language is used (as a default parameter). Version 0.4.1 \u00b6 Release date: 07 Januari, 2021 Fixes : Simple fix by lowering the languages variable to match the lowered input language. Version 0.4.0 \u00b6 Release date: 21 December, 2020 Highlights : Visualize Topics similar to LDAvis Added option to reduce topics after training Added option to update topic representation after training Added option to search topics using a search term Significantly improved the stability of generating clusters Finetune the topic words by selecting the most coherent words with the highest c-TF-IDF values More extensive tutorials in the documentation Notable Changes : Option to select language instead of sentence-transformers models to minimize the complexity of using BERTopic Improved logging (remove duplicates) Check if BERTopic is fitted Added TF-IDF as an embedder instead of transformer models (see tutorial) Numpy for Python 3.6 will be dropped and was therefore removed from the workflow. Preprocess text before passing it through c-TF-IDF Merged get_topics_freq() with get_topic_freq() Fixes : Fix error handling topic probabilities Version 0.3.2 \u00b6 Release date: 16 November, 2020 Highlights : Fixed a bug with the topic reduction method that seems to reduce the number of topics but not to the nr_topics as defined in the class. Since this was, to a certain extend, breaking the topic reduction method a new release was necessary. Version 0.3.1 \u00b6 Release date: 4 November, 2020 Highlights : Adding the option to use custom embeddings or embeddings that you generated beforehand with whatever package you'd like to use. This allows users to further customize BERTopic to their liking. Version 0.3.0 \u00b6 Release date: 29 October, 2020 Highlights : transform() and fit_transform() now also return the topic probability distributions Added visualize_distribution() which visualizes the topic probability distribution for a single document Version 0.2.2 \u00b6 Release date: 17 October, 2020 Highlights : Fixed n_gram_range not being used Added option for using stopwords Version 0.2.1 \u00b6 Release date: 11 October, 2020 Highlights : Improved the calculation of the class-based TF-IDF procedure by limiting the calculation to sparse matrices. This prevents out-of-memory problems when faced with large datasets. Version 0.2.0 \u00b6 Release date: 11 October, 2020 Highlights : Changed c-TF-IDF procedure such that it implements a version of scikit-learns procedure. This should also speed up the calculation of the sparse matrix and prevent memory errors. Added automated unit tests Version 0.1.2 \u00b6 Release date: 1 October, 2020 Highlights : When transforming new documents, self.mapped_topics seemed to be missing. Added to the init. Version 0.1.1 \u00b6 Release date: 24 September, 2020 Highlights : Fixed requirements --> Issue with pytorch Update documentation Version 0.1.0 \u00b6 Release date: 24 September, 2020 Highlights : First release of BERTopic Added parameters for UMAP and HDBSCAN Option to choose sentence-transformer model Method for transforming unseen documents Save and load trained models (UMAP and HDBSCAN) Extract topics and their sizes Notable Changes : Optimized c-TF-IDF Improved documentation Improved topic reduction","title":"Changelog"},{"location":"changelog.html#version-09","text":"Release date: 9 August, 2021 Highlights : Implemented a Guided BERTopic -> Use seeds to steer the Topic Modeling Get the most representative documents per topic: topic_model.get_representative_docs(topic=1) This allows users to see which documents are good representations of a topic and better understand the topics that were created Added normalize_frequency parameter to visualize_topics_per_class and visualize_topics_over_time in order to better compare the relative topic frequencies between topics Return flat probabilities as default, only calculate the probabilities of all topics per document if calculate_probabilities is True Added several FAQs Fixes : Fix loading pre-trained BERTopic model Fix mapping of probabilities Fix #190 Guided BERTopic : Guided BERTopic works in two ways: First, we create embeddings for each seeded topics by joining them and passing them through the document embedder. These embeddings will be compared with the existing document embeddings through cosine similarity and assigned a label. If the document is most similar to a seeded topic, then it will get that topic's label. If it is most similar to the average document embedding, it will get the -1 label. These labels are then passed through UMAP to create a semi-supervised approach that should nudge the topic creation to the seeded topics. Second, we take all words in seed_topic_list and assign them a multiplier larger than 1. Those multipliers will be used to increase the IDF values of the words across all topics thereby increasing the likelihood that a seeded topic word will appear in a topic. This does, however, also increase the chance of an irrelevant topic having unrelated words. In practice, this should not be an issue since the IDF value is likely to remain low regardless of the multiplier. The multiplier is now a fixed value but may change to something more elegant, like taking the distribution of IDF values and its position into account when defining the multiplier. seed_topic_list = [[ \"company\" , \"billion\" , \"quarter\" , \"shrs\" , \"earnings\" ], [ \"acquisition\" , \"procurement\" , \"merge\" ], [ \"exchange\" , \"currency\" , \"trading\" , \"rate\" , \"euro\" ], [ \"grain\" , \"wheat\" , \"corn\" ], [ \"coffee\" , \"cocoa\" ], [ \"natural\" , \"gas\" , \"oil\" , \"fuel\" , \"products\" , \"petrol\" ]] topic_model = BERTopic ( seed_topic_list = seed_topic_list ) topics , probs = topic_model . fit_transform ( docs )","title":"Version 0.9"},{"location":"changelog.html#version-081","text":"Release date: 08 June, 2021 Highlights : Improved models: For English documents the default is now: \"paraphrase-MiniLM-L6-v2\" For Non-English or multi-lingual documents the default is now: \"paraphrase-multilingual-MiniLM-L12-v2\" Both models show not only great performance but are much faster! Add interactive visualizations to the plotting API documentation For better performance, please use the following models: English: \"paraphrase-mpnet-base-v2\" Non-English or multi-lingual: \"paraphrase-multilingual-mpnet-base-v2\" Fixes : Improved unit testing for more stability Set transformers version for Flair","title":"Version 0.8.1"},{"location":"changelog.html#version-080","text":"Release date: 31 May, 2021 Highlights : Additional visualizations: Topic Hierarchy: topic_model.visualize_hierarchy() Topic Similarity Heatmap: topic_model.visualize_heatmap() Topic Representation Barchart: topic_model.visualize_barchart() Term Score Decline: topic_model.visualize_term_rank() Created bertopic.plotting library to easily extend visualizations Improved automatic topic reduction by using HDBSCAN to detect similar topics Sort topic ids by their frequency. -1 is the outlier class and contains typically the most documents. After that 0 is the largest topic, 1 the second largest, etc. Fixes : Fix typo #113 , #117 Fix #121 by removing these two lines Fix mapping of topics after reduction (it now excludes 0) ( #103 )","title":"Version 0.8.0"},{"location":"changelog.html#version-070","text":"Release date: 26 April, 2021 The two main features are (semi-)supervised topic modeling and several backends to use instead of Flair and SentenceTransformers! Highlights : (semi-)supervised topic modeling by leveraging supervised options in UMAP model.fit(docs, y=target_classes) Backends: Added Spacy, Gensim, USE (TFHub) Use a different backend for document embeddings and word embeddings Create your own backends with bertopic.backend.BaseEmbedder Click here for an overview of all new backends Calculate and visualize topics per class Calculate: topics_per_class = topic_model.topics_per_class(docs, topics, classes) Visualize: topic_model.visualize_topics_per_class(topics_per_class) Several tutorials were updated and added: Name Link Topic Modeling with BERTopic (Custom) Embedding Models in BERTopic Advanced Customization in BERTopic (semi-)Supervised Topic Modeling with BERTopic Dynamic Topic Modeling with Trump's Tweets Fixes : Fixed issues with Torch req Prevent saving term frequency matrix in CTFIDF class Fixed DTM not working when reducing topics ( #96 ) Moved visualization dependencies to base BERTopic pip install bertopic[visualization] becomes pip install bertopic Allow precomputed embeddings in bertopic.find_topics() ( #79 ): model = BERTopic ( embedding_model = my_embedding_model ) model . fit ( docs , my_precomputed_embeddings ) model . find_topics ( search_term )","title":"Version 0.7.0"},{"location":"changelog.html#version-060","text":"Release date: 1 March, 2021 Highlights : DTM: Added a basic dynamic topic modeling technique based on the global c-TF-IDF representation model.topics_over_time(docs, timestamps, global_tuning=True) DTM: Option to evolve topics based on t-1 c-TF-IDF representation which results in evolving topics over time Only uses topics at t-1 and skips evolution if there is a gap model.topics_over_time(docs, timestamps, evolution_tuning=True) DTM: Function to visualize topics over time model.visualize_topics_over_time(topics_over_time) DTM: Add binning of timestamps model.topics_over_time(docs, timestamps, nr_bins=10) Add function get general information about topics (id, frequency, name, etc.) get_topic_info() Improved stability of c-TF-IDF by taking the average number of words across all topics instead of the number of documents Fixes : _map_probabilities() does not take into account that there is no probability of the outlier class and the probabilities are mutated instead of copied (#63, #64)","title":"Version 0.6.0"},{"location":"changelog.html#version-050","text":"Release date: 8 Februari, 2021 Highlights : Add Flair to allow for more (custom) token/document embeddings, including \ud83e\udd17 transformers Option to use custom UMAP, HDBSCAN, and CountVectorizer Added low_memory parameter to reduce memory during computation Improved verbosity (shows progress bar) Return the figure of visualize_topics() Expose all parameters with a single function: get_params() Fixes : To simplify the API, the parameters stop_words and n_neighbors were removed. These can still be used when a custom UMAP or CountVectorizer is used. Set calculate_probabilities to False as a default. Calculating probabilities with HDBSCAN significantly increases computation time and memory usage. Better to remove calculating probabilities or only allow it by manually turning this on. Use the newest version of sentence-transformers as it speeds ups encoding significantly","title":"Version 0.5.0"},{"location":"changelog.html#version-042","text":"Release date: 10 Januari, 2021 Fixes : Selecting embedding_model did not work when language was also used. This led to the user needing to set language to None before being able to use embedding_model . Fixed by using embedding_model when language is used (as a default parameter).","title":"Version 0.4.2"},{"location":"changelog.html#version-041","text":"Release date: 07 Januari, 2021 Fixes : Simple fix by lowering the languages variable to match the lowered input language.","title":"Version 0.4.1"},{"location":"changelog.html#version-040","text":"Release date: 21 December, 2020 Highlights : Visualize Topics similar to LDAvis Added option to reduce topics after training Added option to update topic representation after training Added option to search topics using a search term Significantly improved the stability of generating clusters Finetune the topic words by selecting the most coherent words with the highest c-TF-IDF values More extensive tutorials in the documentation Notable Changes : Option to select language instead of sentence-transformers models to minimize the complexity of using BERTopic Improved logging (remove duplicates) Check if BERTopic is fitted Added TF-IDF as an embedder instead of transformer models (see tutorial) Numpy for Python 3.6 will be dropped and was therefore removed from the workflow. Preprocess text before passing it through c-TF-IDF Merged get_topics_freq() with get_topic_freq() Fixes : Fix error handling topic probabilities","title":"Version 0.4.0"},{"location":"changelog.html#version-032","text":"Release date: 16 November, 2020 Highlights : Fixed a bug with the topic reduction method that seems to reduce the number of topics but not to the nr_topics as defined in the class. Since this was, to a certain extend, breaking the topic reduction method a new release was necessary.","title":"Version 0.3.2"},{"location":"changelog.html#version-031","text":"Release date: 4 November, 2020 Highlights : Adding the option to use custom embeddings or embeddings that you generated beforehand with whatever package you'd like to use. This allows users to further customize BERTopic to their liking.","title":"Version 0.3.1"},{"location":"changelog.html#version-030","text":"Release date: 29 October, 2020 Highlights : transform() and fit_transform() now also return the topic probability distributions Added visualize_distribution() which visualizes the topic probability distribution for a single document","title":"Version 0.3.0"},{"location":"changelog.html#version-022","text":"Release date: 17 October, 2020 Highlights : Fixed n_gram_range not being used Added option for using stopwords","title":"Version 0.2.2"},{"location":"changelog.html#version-021","text":"Release date: 11 October, 2020 Highlights : Improved the calculation of the class-based TF-IDF procedure by limiting the calculation to sparse matrices. This prevents out-of-memory problems when faced with large datasets.","title":"Version 0.2.1"},{"location":"changelog.html#version-020","text":"Release date: 11 October, 2020 Highlights : Changed c-TF-IDF procedure such that it implements a version of scikit-learns procedure. This should also speed up the calculation of the sparse matrix and prevent memory errors. Added automated unit tests","title":"Version 0.2.0"},{"location":"changelog.html#version-012","text":"Release date: 1 October, 2020 Highlights : When transforming new documents, self.mapped_topics seemed to be missing. Added to the init.","title":"Version 0.1.2"},{"location":"changelog.html#version-011","text":"Release date: 24 September, 2020 Highlights : Fixed requirements --> Issue with pytorch Update documentation","title":"Version 0.1.1"},{"location":"changelog.html#version-010","text":"Release date: 24 September, 2020 Highlights : First release of BERTopic Added parameters for UMAP and HDBSCAN Option to choose sentence-transformer model Method for transforming unseen documents Save and load trained models (UMAP and HDBSCAN) Extract topics and their sizes Notable Changes : Optimized c-TF-IDF Improved documentation Improved topic reduction","title":"Version 0.1.0"},{"location":"faq.html","text":"Why are the results not consistent between runs? \u00b6 Due to the stochastic nature of UMAP, the results from BERTopic might differ even if you run the same code multiple times. Using custom embeddings allows you to try out BERTopic several times until you find the topics that suit you best. You only need to generate the embeddings itself once and run BERTopic several times with different parameters. Which embedding model works best for which language? \u00b6 Unfortunately, there is not a definitive list of the best models for each language, this highly depends on your data, the model, and your specific use-case. However, the default model in BERTopic ( \"paraphrase-MiniLM-L6-v2\" ) works great for English documents. In contrast, for multi-lingual documents or any other language, \"paraphrase-multilingual-MiniLM-L12-v2\"\" has shown great performance. If you want to use a model that provides a higher quality, but takes more compute time, then I would advise using paraphrase-mpnet-base-v2 and paraphrase-multilingual-mpnet-base-v2 instead. SentenceTransformers SentenceTransformers work typically quite well and are the preferred models to use. They are great at generating document embeddings and have several multi-lingual versions available. \ud83e\udd17 transformers BERTopic allows you to use any \ud83e\udd17 transformers model. These models are typically embeddings created on a word/sentence level but can easily be pooled using Flair (see Guides/Embeddings). If you have a specific language for which you want to generate embeddings, you can choose the model here . How can I speed up BERTopic? \u00b6 You can speed up BERTopic by either generating your embeddings beforehand, which is not advised, or by setting calculate_probabilities to False. Calculating the probabilities is quite expensive and can significantly increase the computation time. Thus, only use it if you do not mind waiting a bit before the model is done running or if you have less than 50_000 documents. Also, make sure to use a GPU when extracting the sentence/document embeddings. Transformer models typically require a GPU and using only a CPU can slow down computation time quite a lot. However, if you do not have access to a GPU, looking into quantization might help. I am facing memory issues. Help! \u00b6 There are several ways to perform computation with large datasets. First, you can set low_memory to True when instantiating BERTopic. This may prevent blowing up the memory in UMAP. Second, setting calculate_probabilities to False when instantiating BERTopic prevents a huge document-topic probability matrix from being created. Moreover, HDBSCAN is quite slow when it tries to calculate probabilities on large datasets. Third, you can set the minimum frequency of words in the CountVectorizer class to reduce the size of the resulting sparse c-TF-IDF matrix. You can do this as follows: from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( ngram_range = ( 1 , 2 ), stop_words = \"english\" , min_df = 10 ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) The min_df parameter is used to indicate the minimum frequency of words. Setting this value larger than 1 can significantly reduce memory. If the problem persists, then this could be an issue related to your available memory. The processing of millions of documents is quite computationally expensive and sufficient RAM is necessary. I have only a few topics, how do I increase them? \u00b6 There are several reasons why your topic model results in only a few topics. First, you might only have a few documents (~1000). This makes it very difficult to properly extract topics due to the little amount of data available. Increasing the number of documents might solve your issues. Second, min_topic_size might be simply too large for your number of documents. If you decrease the minimum size of topics, then you are much more likely to increase the number of topics generated. You could also decrease the n_neighbors parameter used in UMAP if this does not work. Third, although this does not happen very often, there simply aren't that many topics to be found in your documents. You can often see this when you have many -1 topics, which is actually not a topic but a category of outliers. I have too many topics, how do I decrease them? \u00b6 If you have a large dataset, then it is possible to generate thousands of topics. Especially with large datasets, there is a good chance they actually contain many small topics. In practice, you might want a few hundred topics at most in order to interpret them nicely. There are a few ways of increasing the number of generated topics: First, we can set the min_topic_size in the BERTopic initialization much higher (e.g., 300) to make sure that those small clusters will not be generated. This is a HDBSCAN parameter that specifies what the minimum number of documents are needed in a cluster. More documents in a cluster means less topics will be generated. Second, you can create a custom UMAP model and set n_neighbors much higher than the default 15 (e.g., 200). This also prevents those micro clusters to be generated as it will needs quite a number of neighboring documents to create a cluster. Third, we can set nr_topics to a value that seems logical to the user. Do note that topics are forced to merge together which might result in a lower quality of topics. In practice, I would advise using nr_topic=\"auto\" as that will merge topics together that are very similar. Dissimilar topics will therefore remain separated. How do I calculate the probabilities of all topics in a document? \u00b6 Although it is possible to calculate all the probabilities, the process of doing so is quite computationally inefficient and might significantly increase the computation time. To prevent this, the probabilities are not calculated as a default. In order to calculate, you will have to set calculate_probabilities to True: from bertopic import BERTopic topic_model = BERTopic ( calculate_probabilities = True ) topics , probs = topic_model . fit_transform ( docs ) Numpy gives me an error when running BERTopic \u00b6 With the release of Numpy 1.20.0, there have been significant issues with using that version (and previous) due to compilation issues and pypi. This is a known issue with the order of install using pypi. You can find more details about this issue here and here . I would suggest doing one of the following: Install the newest version from BERTopic (>= v0.5). You can install hdbscan with pip install hdbscan --no-cache-dir --no-binary :all: --no-build-isolation which might resolve the issue Use the above step also with numpy as it is part of the issue Install BERTopic in a fresh environment using these steps. How can I run BERTopic without an internet connection? \u00b6 The great thing about using sentence-transformers is that it searches automatically for an embedding model locally. If it cannot find one, it will download the pre-trained model from its servers. Make sure that you set the correct path for sentence-transformers to work. You can find a bit more about that here . You can download the corresponding model here and unzip it. Then, simply use the following to create your embedding model: from sentence_transformers import SentenceTransformer embedding_model = SentenceTransformer ( 'path/to/unzipped/model' ) Then, pass it to BERTopic: from bertopic import BERTopic topic_model = BERTopic ( embedding_model = embedding_model ) Can I use the GPU to speed up the model? \u00b6 Yes and no. The GPU is automatically used when you use a SentenceTransformer or Flair embedding model. Using a CPU would then definitely slow things down. However, UMAP and HDBSCAN are not GPU-accelerated and are likely not so in the near future. For now, a GPU does help tremendously for extracting embeddings but does not speed up all aspects of BERtopic. How can I use BERTopic with Chinese documents? \u00b6 Currently, CountVectorizer tokenizes text by splitting whitespace which does not work for Chinese. In order to get it to work, you will have to create a custom CountVectorizer with jieba : from sklearn.feature_extraction.text import CountVectorizer import jieba def tokenize_zh ( text ): words = jieba . lcut ( text ) return words vectorizer = CountVectorizer ( tokenizer = tokenize_zh ) Next, we pass our custom vectorizer to BERTopic and create our topic model: from bertopic import BERTopic topic_model = BERTopic ( embedding_model = model , verbose = True , vectorizer_model = vectorizer ) topics , _ = topic_model . fit_transform ( docs , embeddings = embeddings ) Why does it take so long to import BERTopic? \u00b6 The main culprit here seems to be UMAP. After running tests with Tuna we can see that most of the resources when importing BERTopic can be dedicated to UMAP: Unfortunately, there currently is no fix for this issue. The most recent ticket regarding this issue can be found here . Should I preprocess the data? \u00b6 No. By using document embeddings there is typically no need to preprocess the data as all parts of a document are important in understanding the general topic of the document. Although this holds true in 99% of cases, if you have data that contains a lot of noise, for example, HTML-tags, then it would be best to remove them. HTML-tags typically do not contribute to the meaning of a document and should therefore be removed. However, if you apply topic modeling to HTML-code to extract topics of code, then it becomes important.","title":"FAQ"},{"location":"faq.html#why-are-the-results-not-consistent-between-runs","text":"Due to the stochastic nature of UMAP, the results from BERTopic might differ even if you run the same code multiple times. Using custom embeddings allows you to try out BERTopic several times until you find the topics that suit you best. You only need to generate the embeddings itself once and run BERTopic several times with different parameters.","title":"Why are the results not consistent between runs?"},{"location":"faq.html#which-embedding-model-works-best-for-which-language","text":"Unfortunately, there is not a definitive list of the best models for each language, this highly depends on your data, the model, and your specific use-case. However, the default model in BERTopic ( \"paraphrase-MiniLM-L6-v2\" ) works great for English documents. In contrast, for multi-lingual documents or any other language, \"paraphrase-multilingual-MiniLM-L12-v2\"\" has shown great performance. If you want to use a model that provides a higher quality, but takes more compute time, then I would advise using paraphrase-mpnet-base-v2 and paraphrase-multilingual-mpnet-base-v2 instead. SentenceTransformers SentenceTransformers work typically quite well and are the preferred models to use. They are great at generating document embeddings and have several multi-lingual versions available. \ud83e\udd17 transformers BERTopic allows you to use any \ud83e\udd17 transformers model. These models are typically embeddings created on a word/sentence level but can easily be pooled using Flair (see Guides/Embeddings). If you have a specific language for which you want to generate embeddings, you can choose the model here .","title":"Which embedding model works best for which language?"},{"location":"faq.html#how-can-i-speed-up-bertopic","text":"You can speed up BERTopic by either generating your embeddings beforehand, which is not advised, or by setting calculate_probabilities to False. Calculating the probabilities is quite expensive and can significantly increase the computation time. Thus, only use it if you do not mind waiting a bit before the model is done running or if you have less than 50_000 documents. Also, make sure to use a GPU when extracting the sentence/document embeddings. Transformer models typically require a GPU and using only a CPU can slow down computation time quite a lot. However, if you do not have access to a GPU, looking into quantization might help.","title":"How can I speed up BERTopic?"},{"location":"faq.html#i-am-facing-memory-issues-help","text":"There are several ways to perform computation with large datasets. First, you can set low_memory to True when instantiating BERTopic. This may prevent blowing up the memory in UMAP. Second, setting calculate_probabilities to False when instantiating BERTopic prevents a huge document-topic probability matrix from being created. Moreover, HDBSCAN is quite slow when it tries to calculate probabilities on large datasets. Third, you can set the minimum frequency of words in the CountVectorizer class to reduce the size of the resulting sparse c-TF-IDF matrix. You can do this as follows: from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( ngram_range = ( 1 , 2 ), stop_words = \"english\" , min_df = 10 ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) The min_df parameter is used to indicate the minimum frequency of words. Setting this value larger than 1 can significantly reduce memory. If the problem persists, then this could be an issue related to your available memory. The processing of millions of documents is quite computationally expensive and sufficient RAM is necessary.","title":"I am facing memory issues. Help!"},{"location":"faq.html#i-have-only-a-few-topics-how-do-i-increase-them","text":"There are several reasons why your topic model results in only a few topics. First, you might only have a few documents (~1000). This makes it very difficult to properly extract topics due to the little amount of data available. Increasing the number of documents might solve your issues. Second, min_topic_size might be simply too large for your number of documents. If you decrease the minimum size of topics, then you are much more likely to increase the number of topics generated. You could also decrease the n_neighbors parameter used in UMAP if this does not work. Third, although this does not happen very often, there simply aren't that many topics to be found in your documents. You can often see this when you have many -1 topics, which is actually not a topic but a category of outliers.","title":"I have only a few topics, how do I increase them?"},{"location":"faq.html#i-have-too-many-topics-how-do-i-decrease-them","text":"If you have a large dataset, then it is possible to generate thousands of topics. Especially with large datasets, there is a good chance they actually contain many small topics. In practice, you might want a few hundred topics at most in order to interpret them nicely. There are a few ways of increasing the number of generated topics: First, we can set the min_topic_size in the BERTopic initialization much higher (e.g., 300) to make sure that those small clusters will not be generated. This is a HDBSCAN parameter that specifies what the minimum number of documents are needed in a cluster. More documents in a cluster means less topics will be generated. Second, you can create a custom UMAP model and set n_neighbors much higher than the default 15 (e.g., 200). This also prevents those micro clusters to be generated as it will needs quite a number of neighboring documents to create a cluster. Third, we can set nr_topics to a value that seems logical to the user. Do note that topics are forced to merge together which might result in a lower quality of topics. In practice, I would advise using nr_topic=\"auto\" as that will merge topics together that are very similar. Dissimilar topics will therefore remain separated.","title":"I have too many topics, how do I decrease them?"},{"location":"faq.html#how-do-i-calculate-the-probabilities-of-all-topics-in-a-document","text":"Although it is possible to calculate all the probabilities, the process of doing so is quite computationally inefficient and might significantly increase the computation time. To prevent this, the probabilities are not calculated as a default. In order to calculate, you will have to set calculate_probabilities to True: from bertopic import BERTopic topic_model = BERTopic ( calculate_probabilities = True ) topics , probs = topic_model . fit_transform ( docs )","title":"How do I calculate the probabilities of all topics in a document?"},{"location":"faq.html#numpy-gives-me-an-error-when-running-bertopic","text":"With the release of Numpy 1.20.0, there have been significant issues with using that version (and previous) due to compilation issues and pypi. This is a known issue with the order of install using pypi. You can find more details about this issue here and here . I would suggest doing one of the following: Install the newest version from BERTopic (>= v0.5). You can install hdbscan with pip install hdbscan --no-cache-dir --no-binary :all: --no-build-isolation which might resolve the issue Use the above step also with numpy as it is part of the issue Install BERTopic in a fresh environment using these steps.","title":"Numpy gives me an error when running BERTopic"},{"location":"faq.html#how-can-i-run-bertopic-without-an-internet-connection","text":"The great thing about using sentence-transformers is that it searches automatically for an embedding model locally. If it cannot find one, it will download the pre-trained model from its servers. Make sure that you set the correct path for sentence-transformers to work. You can find a bit more about that here . You can download the corresponding model here and unzip it. Then, simply use the following to create your embedding model: from sentence_transformers import SentenceTransformer embedding_model = SentenceTransformer ( 'path/to/unzipped/model' ) Then, pass it to BERTopic: from bertopic import BERTopic topic_model = BERTopic ( embedding_model = embedding_model )","title":"How can I run BERTopic without an internet connection?"},{"location":"faq.html#can-i-use-the-gpu-to-speed-up-the-model","text":"Yes and no. The GPU is automatically used when you use a SentenceTransformer or Flair embedding model. Using a CPU would then definitely slow things down. However, UMAP and HDBSCAN are not GPU-accelerated and are likely not so in the near future. For now, a GPU does help tremendously for extracting embeddings but does not speed up all aspects of BERtopic.","title":"Can I use the GPU to speed up the model?"},{"location":"faq.html#how-can-i-use-bertopic-with-chinese-documents","text":"Currently, CountVectorizer tokenizes text by splitting whitespace which does not work for Chinese. In order to get it to work, you will have to create a custom CountVectorizer with jieba : from sklearn.feature_extraction.text import CountVectorizer import jieba def tokenize_zh ( text ): words = jieba . lcut ( text ) return words vectorizer = CountVectorizer ( tokenizer = tokenize_zh ) Next, we pass our custom vectorizer to BERTopic and create our topic model: from bertopic import BERTopic topic_model = BERTopic ( embedding_model = model , verbose = True , vectorizer_model = vectorizer ) topics , _ = topic_model . fit_transform ( docs , embeddings = embeddings )","title":"How can I use BERTopic with Chinese documents?"},{"location":"faq.html#why-does-it-take-so-long-to-import-bertopic","text":"The main culprit here seems to be UMAP. After running tests with Tuna we can see that most of the resources when importing BERTopic can be dedicated to UMAP: Unfortunately, there currently is no fix for this issue. The most recent ticket regarding this issue can be found here .","title":"Why does it take so long to import BERTopic?"},{"location":"faq.html#should-i-preprocess-the-data","text":"No. By using document embeddings there is typically no need to preprocess the data as all parts of a document are important in understanding the general topic of the document. Although this holds true in 99% of cases, if you have data that contains a lot of noise, for example, HTML-tags, then it would be best to remove them. HTML-tags typically do not contribute to the meaning of a document and should therefore be removed. However, if you apply topic modeling to HTML-code to extract topics of code, then it becomes important.","title":"Should I preprocess the data?"},{"location":"api/bertopic.html","text":"BERTopic \u00b6 BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () topics , probabilities = topic_model . fit_transform ( docs ) If you want to use your own embedding model, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) topic_model = BERTopic ( embedding_model = sentence_model ) Due to the stochastisch nature of UMAP, the results from BERTopic might differ and the quality can degrade. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best. __init__ ( self , language = 'english' , top_n_words = 10 , n_gram_range = ( 1 , 1 ), min_topic_size = 10 , nr_topics = None , low_memory = False , calculate_probabilities = False , seed_topic_list = None , embedding_model = None , umap_model = None , hdbscan_model = None , vectorizer_model = None , verbose = False ) special \u00b6 BERTopic initialization Parameters: Name Type Description Default language str The main language used in your documents. For a full overview of supported languages see bertopic.backends.languages. Select \"multilingual\" to load in a sentence-tranformers model that supports 50+ languages. 'english' top_n_words int The number of words per topic to extract. Setting this too high can negatively impact topic embeddings as topics are typically best represented by at most 10 words. 10 n_gram_range Tuple[int, int] The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. NOTE: This param will not be used if you pass in your own CountVectorizer. (1, 1) min_topic_size int The minimum size of the topic. Increasing this value will lead to a lower number of clusters/topics. 10 nr_topics Union[int, str] Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that have a similarity of at least 0.9, do not maps all others. None low_memory bool Sets UMAP low memory to True to make sure less memory is used. False calculate_probabilities bool Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. This could slow down the extraction of topics if you have many documents (> 100_000). Set this only to True if you have a low amount of documents or if you do not mind more computation time. NOTE: If false you cannot use the corresponding visualization method visualize_probabilities . False seed_topic_list List[List[str]] A list of seed words per topic to converge around None verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False embedding_model Use a custom embedding model. The following backends are currently supported * SentenceTransformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html None umap_model UMAP Pass in a UMAP model to be used instead of the default None hdbscan_model HDBSCAN Pass in a hdbscan.HDBSCAN model to be used instead of the default None vectorizer_model CountVectorizer Pass in a CountVectorizer instead of the default None Source code in bertopic\\_bertopic.py def __init__ ( self , language : str = \"english\" , top_n_words : int = 10 , n_gram_range : Tuple [ int , int ] = ( 1 , 1 ), min_topic_size : int = 10 , nr_topics : Union [ int , str ] = None , low_memory : bool = False , calculate_probabilities : bool = False , seed_topic_list : List [ List [ str ]] = None , embedding_model = None , umap_model : UMAP = None , hdbscan_model : hdbscan . HDBSCAN = None , vectorizer_model : CountVectorizer = None , verbose : bool = False , ): \"\"\"BERTopic initialization Arguments: language: The main language used in your documents. For a full overview of supported languages see bertopic.backends.languages. Select \"multilingual\" to load in a sentence-tranformers model that supports 50+ languages. top_n_words: The number of words per topic to extract. Setting this too high can negatively impact topic embeddings as topics are typically best represented by at most 10 words. n_gram_range: The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. NOTE: This param will not be used if you pass in your own CountVectorizer. min_topic_size: The minimum size of the topic. Increasing this value will lead to a lower number of clusters/topics. nr_topics: Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that have a similarity of at least 0.9, do not maps all others. low_memory: Sets UMAP low memory to True to make sure less memory is used. calculate_probabilities: Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. This could slow down the extraction of topics if you have many documents (> 100_000). Set this only to True if you have a low amount of documents or if you do not mind more computation time. NOTE: If false you cannot use the corresponding visualization method `visualize_probabilities`. seed_topic_list: A list of seed words per topic to converge around verbose: Changes the verbosity of the model, Set to True if you want to track the stages of the model. embedding_model: Use a custom embedding model. The following backends are currently supported * SentenceTransformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html umap_model: Pass in a UMAP model to be used instead of the default hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default vectorizer_model: Pass in a CountVectorizer instead of the default \"\"\" # Topic-based parameters if top_n_words > 30 : raise ValueError ( \"top_n_words should be lower or equal to 30. The preferred value is 10.\" ) self . top_n_words = top_n_words self . min_topic_size = min_topic_size self . nr_topics = nr_topics self . low_memory = low_memory self . calculate_probabilities = calculate_probabilities self . verbose = verbose self . seed_topic_list = seed_topic_list # Embedding model self . language = language if not embedding_model else None self . embedding_model = embedding_model # Vectorizer self . n_gram_range = n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = self . n_gram_range ) # UMAP self . umap_model = umap_model or UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' , low_memory = self . low_memory ) # HDBSCAN self . hdbscan_model = hdbscan_model or hdbscan . HDBSCAN ( min_cluster_size = self . min_topic_size , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) self . topics = None self . topic_sizes = None self . mapped_topics = None self . merged_topics = None self . topic_embeddings = None self . topic_sim_matrix = None self . representative_docs = None if verbose : logger . set_level ( \"DEBUG\" ) __str__ ( self ) special \u00b6 Get a string representation of the current object. Returns: Type Description str Human readable representation of the most important model parameters. The parameters that represent models are ignored due to their Source code in bertopic\\_bertopic.py def __str__ ( self ): \"\"\"Get a string representation of the current object. Returns: str: Human readable representation of the most important model parameters. The parameters that represent models are ignored due to their \"\"\" parameters = \"\" for parameter , value in self . get_params () . items (): value = str ( value ) if \"(\" in value and value [ 0 ] != \"(\" : value = value . split ( \"(\" )[ 0 ] + \"(...)\" parameters += f \" { parameter } = { value } , \" return f \"BERTopic( { parameters [: - 2 ] } )\" find_topics ( self , search_term , top_n = 5 ) \u00b6 Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Parameters: Name Type Description Default search_term str the term you want to use to search for topics required top_n int the number of topics to return 5 Returns: Type Description Tuple[List[int], List[float]] similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low Usage: You can use the underlying embedding model to find topics that best represent the search term: topics , similarity = topic_model . find_topics ( \"sports\" , top_n = 5 ) Note that the search query is typically more accurate if the search_term consists of a phrase or multiple words. Source code in bertopic\\_bertopic.py def find_topics ( self , search_term : str , top_n : int = 5 ) -> Tuple [ List [ int ], List [ float ]]: \"\"\" Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Arguments: search_term: the term you want to use to search for topics top_n: the number of topics to return Returns: similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low Usage: You can use the underlying embedding model to find topics that best represent the search term: ```python topics, similarity = topic_model.find_topics(\"sports\", top_n=5) ``` Note that the search query is typically more accurate if the search_term consists of a phrase or multiple words. \"\"\" if self . embedding_model is None : raise Exception ( \"This method can only be used if you did not use custom embeddings.\" ) topic_list = list ( self . topics . keys ()) topic_list . sort () # Extract search_term embeddings and compare with topic embeddings search_embedding = self . _extract_embeddings ([ search_term ], method = \"word\" , verbose = False ) . flatten () sims = cosine_similarity ( search_embedding . reshape ( 1 , - 1 ), self . topic_embeddings ) . flatten () # Extract topics most similar to search_term ids = np . argsort ( sims )[ - top_n :] similarity = [ sims [ i ] for i in ids ][:: - 1 ] similar_topics = [ topic_list [ index ] for index in ids ][:: - 1 ] return similar_topics , similarity fit ( self , documents , embeddings = None , y = None ) \u00b6 Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Parameters: Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None y Union[List[int], numpy.ndarray] The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. None Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () . fit ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () . fit ( docs , embeddings ) Source code in bertopic\\_bertopic.py def fit ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) ``` \"\"\" self . fit_transform ( documents , embeddings , y ) return self fit_transform ( self , documents , embeddings = None , y = None ) \u00b6 Fit the models on a collection of documents, generate topics, and return the docs with topics Parameters: Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None y Union[List[int], numpy.ndarray] The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. None Returns: Type Description Tuple[List[int], Optional[numpy.ndarray]] predictions: Topic predictions for each documents probabilities: The probability of the assigned topic per document. If calculate_probabilities in BERTopic is set to True, then it calculates the probabilities of all topics across all documents instead of only the assigned topic. This, however, slows down computation and may increase memory usage. Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py def fit_transform ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ) -> Tuple [ List [ int ], Union [ np . ndarray , None ]]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Returns: predictions: Topic predictions for each documents probabilities: The probability of the assigned topic per document. If `calculate_probabilities` in BERTopic is set to True, then it calculates the probabilities of all topics across all documents instead of only the assigned topic. This, however, slows down computation and may increase memory usage. Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) check_embeddings_shape ( embeddings , documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if embeddings is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) embeddings = self . _extract_embeddings ( documents . Document , method = \"document\" , verbose = self . verbose ) logger . info ( \"Transformed documents to Embeddings\" ) else : if self . embedding_model is not None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) # Reduce dimensionality with UMAP if self . seed_topic_list is not None and self . embedding_model is not None : y , embeddings = self . _guided_topic_modeling ( embeddings ) umap_embeddings = self . _reduce_dimensionality ( embeddings , y ) # Cluster UMAP embeddings with HDBSCAN documents , probabilities = self . _cluster_embeddings ( umap_embeddings , documents ) # Sort and Map Topic IDs by their frequency if not self . nr_topics : documents = self . _sort_mappings_by_frequency ( documents ) # Extract topics by calculating c-TF-IDF self . _extract_topics ( documents ) # Reduce topics if self . nr_topics : documents = self . _reduce_topics ( documents ) self . _map_representative_docs () probabilities = self . _map_probabilities ( probabilities ) predictions = documents . Topic . to_list () return predictions , probabilities get_params ( self , deep = False ) \u00b6 Get parameters for this estimator. Adapted from: https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 Parameters: Name Type Description Default deep bool bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. False Returns: Type Description Mapping[str, Any] out: Parameter names mapped to their values. Source code in bertopic\\_bertopic.py def get_params ( self , deep : bool = False ) -> Mapping [ str , Any ]: \"\"\" Get parameters for this estimator. Adapted from: https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 Arguments: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: out: Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names (): value = getattr ( self , key ) if deep and hasattr ( value , 'get_params' ): deep_items = value . get_params () . items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out get_representative_docs ( self , topic ) \u00b6 Extract representative documents per topic Parameters: Name Type Description Default topic int A specific topic for which you want the representative documents required Returns: Type Description List[str] Representative documents of the chosen topic Usage: To extract the representative docs of all topics: representative_docs = topic_model . get_representative_docs () To get the representative docs of a single topic: representative_docs = topic_model . get_representative_docs ( 12 ) Source code in bertopic\\_bertopic.py def get_representative_docs ( self , topic : int ) -> List [ str ]: \"\"\" Extract representative documents per topic Arguments: topic: A specific topic for which you want the representative documents Returns: Representative documents of the chosen topic Usage: To extract the representative docs of all topics: ```python representative_docs = topic_model.get_representative_docs() ``` To get the representative docs of a single topic: ```python representative_docs = topic_model.get_representative_docs(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . representative_docs [ topic ] else : return self . representative_docs get_topic ( self , topic ) \u00b6 Return top n words for a specific topic and their c-TF-IDF scores Parameters: Name Type Description Default topic int A specific topic for which you want its representation required Returns: Type Description Union[Mapping[str, Tuple[str, float]], bool] The top n words for a specific word and its respective c-TF-IDF scores Usage: topic = topic_model . get_topic ( 12 ) Source code in bertopic\\_bertopic.py def get_topic ( self , topic : int ) -> Union [ Mapping [ str , Tuple [ str , float ]], bool ]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores Arguments: topic: A specific topic for which you want its representation Returns: The top n words for a specific word and its respective c-TF-IDF scores Usage: ```python topic = topic_model.get_topic(12) ``` \"\"\" check_is_fitted ( self ) if topic in self . topics : return self . topics [ topic ] else : return False get_topic_freq ( self , topic = None ) \u00b6 Return the the size of topics (descending order) Parameters: Name Type Description Default topic int A specific topic for which you want the frequency None Returns: Type Description Union[pandas.core.frame.DataFrame, int] Either the frequency of a single topic or dataframe with the frequencies of all topics Usage: To extract the frequency of all topics: frequency = topic_model . get_topic_freq () To get the frequency of a single topic: frequency = topic_model . get_topic_freq ( 12 ) Source code in bertopic\\_bertopic.py def get_topic_freq ( self , topic : int = None ) -> Union [ pd . DataFrame , int ]: \"\"\" Return the the size of topics (descending order) Arguments: topic: A specific topic for which you want the frequency Returns: Either the frequency of a single topic or dataframe with the frequencies of all topics Usage: To extract the frequency of all topics: ```python frequency = topic_model.get_topic_freq() ``` To get the frequency of a single topic: ```python frequency = topic_model.get_topic_freq(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . topic_sizes [ topic ] else : return pd . DataFrame ( self . topic_sizes . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False ) get_topic_info ( self , topic = None ) \u00b6 Get information about each topic including its id, frequency, and name Parameters: Name Type Description Default topic int A specific topic for which you want the frequency None Returns: Type Description DataFrame info: The information relating to either a single topic or all topics Usage: info_df = topic_model . get_topic_info () Source code in bertopic\\_bertopic.py def get_topic_info ( self , topic : int = None ) -> pd . DataFrame : \"\"\" Get information about each topic including its id, frequency, and name Arguments: topic: A specific topic for which you want the frequency Returns: info: The information relating to either a single topic or all topics Usage: ```python info_df = topic_model.get_topic_info() ``` \"\"\" check_is_fitted ( self ) info = pd . DataFrame ( self . topic_sizes . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False ) info [ \"Name\" ] = info . Topic . map ( self . topic_names ) if topic : info = info . loc [ info . Topic == topic , :] return info get_topics ( self ) \u00b6 Return topics with top n words and their c-TF-IDF score Returns: Type Description Mapping[str, Tuple[str, float]] self.topic: The top n words per topic and the corresponding c-TF-IDF score Usage: all_topics = topic_model . get_topics () Source code in bertopic\\_bertopic.py def get_topics ( self ) -> Mapping [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score Returns: self.topic: The top n words per topic and the corresponding c-TF-IDF score Usage: ```python all_topics = topic_model.get_topics() ``` \"\"\" check_is_fitted ( self ) return self . topics load ( path , embedding_model = None ) classmethod \u00b6 Loads the model from the specified path Parameters: Name Type Description Default path str the location and name of the BERTopic file you want to load required embedding_model If the embedding_model was not saved to save space or to load it in from the cloud, you can load it in by specifying it here. None Usage: BERTopic . load ( \"my_model\" ) or if you did not save the embedding model: BERTopic . load ( \"my_model\" , embedding_model = \"paraphrase-MiniLM-L6-v2\" ) Source code in bertopic\\_bertopic.py @classmethod def load ( cls , path : str , embedding_model = None ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the BERTopic file you want to load embedding_model: If the embedding_model was not saved to save space or to load it in from the cloud, you can load it in by specifying it here. Usage: ```python BERTopic.load(\"my_model\") ``` or if you did not save the embedding model: ```python BERTopic.load(\"my_model\", embedding_model=\"paraphrase-MiniLM-L6-v2\") ``` \"\"\" with open ( path , 'rb' ) as file : if embedding_model : topic_model = joblib . load ( file ) topic_model . embedding_model = select_backend ( embedding_model ) else : topic_model = joblib . load ( file ) return topic_model reduce_topics ( self , docs , topics , probabilities = None , nr_topics = 20 ) \u00b6 Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. The reasoning for putting docs , topics , and probs as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic instead of a dedicated database. Parameters: Name Type Description Default docs List[str] The docs you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required probabilities ndarray The probabilities that were returned when calling either fit or fit_transform None nr_topics int The number of topics you want reduced to 20 Returns: Type Description Tuple[List[int], numpy.ndarray] new_topics: Updated topics new_probabilities: Updated probabilities Usage: You can further reduce the topics by passing the documents with its topics and probabilities (if they were calculated): new_topics , new_probs = topic_model . reduce_topics ( docs , topics , probabilities , nr_topics = 30 ) If probabilities were not calculated simply run the function without them: new_topics , new_probs = topic_model . reduce_topics ( docs , topics , nr_topics = 30 ) Source code in bertopic\\_bertopic.py def reduce_topics ( self , docs : List [ str ], topics : List [ int ], probabilities : np . ndarray = None , nr_topics : int = 20 ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. The reasoning for putting `docs`, `topics`, and `probs` as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic instead of a dedicated database. Arguments: docs: The docs you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` probabilities: The probabilities that were returned when calling either `fit` or `fit_transform` nr_topics: The number of topics you want reduced to Returns: new_topics: Updated topics new_probabilities: Updated probabilities Usage: You can further reduce the topics by passing the documents with its topics and probabilities (if they were calculated): ```python new_topics, new_probs = topic_model.reduce_topics(docs, topics, probabilities, nr_topics=30) ``` If probabilities were not calculated simply run the function without them: ```python new_topics, new_probs = topic_model.reduce_topics(docs, topics, nr_topics=30) ``` \"\"\" check_is_fitted ( self ) self . nr_topics = nr_topics documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) # Reduce number of topics self . _extract_topics ( documents ) documents = self . _reduce_topics ( documents ) # Extract topics and map probabilities new_topics = documents . Topic . to_list () new_probabilities = self . _map_probabilities ( probabilities ) return new_topics , new_probabilities save ( self , path , save_embedding_model = True ) \u00b6 Saves the model to the specified path Parameters: Name Type Description Default path str the location and name of the file you want to save required save_embedding_model bool Whether to save the embedding model in this class as you might have selected a local model or one that is downloaded automatically from the cloud. True Usage: topic_model . save ( \"my_model\" ) or if you do not want the embedding_model to be saved locally: topic_model . save ( \"my_model\" , save_embedding_model = False ) Source code in bertopic\\_bertopic.py def save ( self , path : str , save_embedding_model : bool = True ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save save_embedding_model: Whether to save the embedding model in this class as you might have selected a local model or one that is downloaded automatically from the cloud. Usage: ```python topic_model.save(\"my_model\") ``` or if you do not want the embedding_model to be saved locally: ```python topic_model.save(\"my_model\", save_embedding_model=False) ``` \"\"\" with open ( path , 'wb' ) as file : if not save_embedding_model : embedding_model = self . embedding_model self . embedding_model = None joblib . dump ( self , file ) self . embedding_model = embedding_model else : joblib . dump ( self , file ) topics_over_time ( self , docs , topics , timestamps , nr_bins = None , datetime_format = None , evolution_tuning = True , global_tuning = True ) \u00b6 Create topics over time To create the topics over time, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each timestamp t. Then, the c-TF-IDF representations at timestamp t are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. !!! note Make sure to use a limited number of unique timestamps (<100) as the c-TF-IDF representation will be calculated at each single unique timestamp. Having a large number of unique timestamps can take some time to be calculated. Moreover, there aren't many use-cased where you would like to see the difference in topic representations over more than 100 different timestamps. Parameters: Name Type Description Default docs List[str] The documents you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required timestamps Union[List[str], List[int]] The timestamp of each document. This can be either a list of strings or ints. If it is a list of strings, then the datetime format will be automatically inferred. If it is a list of ints, then the documents will be ordered by ascending order. required nr_bins int The number of bins you want to create for the timestamps. The left interval will be chosen as the timestamp. An additional column will be created with the entire interval. None datetime_format str The datetime format of the timestamps if they are strings, eg \u201c%d/%m/%Y\u201d. Set this to None if you want to have it automatically detect the format. See strftime documentation for more information on choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior. None evolution_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1. This creates evolutionary topic representations. True global_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. True Returns: Type Description DataFrame topics_over_time: A dataframe that contains the topic, words, and frequency of topic at timestamp t. Usage: The timestamps variable represent the timestamp of each document. If you have over 100 unique timestamps, it is advised to bin the timestamps as shown below: from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_over_time = topic_model . topics_over_time ( docs , topics , timestamps , nr_bins = 20 ) Source code in bertopic\\_bertopic.py def topics_over_time ( self , docs : List [ str ], topics : List [ int ], timestamps : Union [ List [ str ], List [ int ]], nr_bins : int = None , datetime_format : str = None , evolution_tuning : bool = True , global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics over time To create the topics over time, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each timestamp t. Then, the c-TF-IDF representations at timestamp t are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. NOTE: Make sure to use a limited number of unique timestamps (<100) as the c-TF-IDF representation will be calculated at each single unique timestamp. Having a large number of unique timestamps can take some time to be calculated. Moreover, there aren't many use-cased where you would like to see the difference in topic representations over more than 100 different timestamps. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` timestamps: The timestamp of each document. This can be either a list of strings or ints. If it is a list of strings, then the datetime format will be automatically inferred. If it is a list of ints, then the documents will be ordered by ascending order. nr_bins: The number of bins you want to create for the timestamps. The left interval will be chosen as the timestamp. An additional column will be created with the entire interval. datetime_format: The datetime format of the timestamps if they are strings, eg \u201c%d/%m/%Y\u201d. Set this to None if you want to have it automatically detect the format. See strftime documentation for more information on choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior. evolution_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1. This creates evolutionary topic representations. global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. Returns: topics_over_time: A dataframe that contains the topic, words, and frequency of topic at timestamp t. Usage: The timestamps variable represent the timestamp of each document. If you have over 100 unique timestamps, it is advised to bin the timestamps as shown below: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_over_time = topic_model.topics_over_time(docs, topics, timestamps, nr_bins=20) ``` \"\"\" check_is_fitted ( self ) check_documents_type ( docs ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics , \"Timestamps\" : timestamps }) global_c_tf_idf = normalize ( self . c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) all_topics = sorted ( list ( documents . Topic . unique ())) all_topics_indices = { topic : index for index , topic in enumerate ( all_topics )} if isinstance ( timestamps [ 0 ], str ): infer_datetime_format = True if not datetime_format else False documents [ \"Timestamps\" ] = pd . to_datetime ( documents [ \"Timestamps\" ], infer_datetime_format = infer_datetime_format , format = datetime_format ) if nr_bins : documents [ \"Bins\" ] = pd . cut ( documents . Timestamps , bins = nr_bins ) documents [ \"Timestamps\" ] = documents . apply ( lambda row : row . Bins . left , 1 ) # Sort documents in chronological order documents = documents . sort_values ( \"Timestamps\" ) timestamps = documents . Timestamps . unique () if len ( timestamps ) > 100 : warnings . warn ( f \"There are more than 100 unique timestamps (i.e., { len ( timestamps ) } ) \" \"which significantly slows down the application. Consider setting `nr_bins` \" \"to a value lower than 100 to speed up calculation. \" ) # For each unique timestamp, create topic representations topics_over_time = [] for index , timestamp in tqdm ( enumerate ( timestamps ), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Timestamps == timestamp , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Timestamps\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , m = len ( selection ), fit = False ) if global_tuning or evolution_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) # Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the c-TF-IDF # matrix at timestamp t-1 if evolution_tuning and index != 0 : current_topics = sorted ( list ( documents_per_topic . Topic . values )) overlapping_topics = sorted ( list ( set ( previous_topics ) . intersection ( set ( current_topics )))) current_overlap_idx = [ current_topics . index ( topic ) for topic in overlapping_topics ] previous_overlap_idx = [ previous_topics . index ( topic ) for topic in overlapping_topics ] c_tf_idf . tolil ()[ current_overlap_idx ] = (( c_tf_idf [ current_overlap_idx ] + previous_c_tf_idf [ previous_overlap_idx ]) / 2.0 ) . tolil () # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : selected_topics = [ all_topics_indices [ topic ] for topic in documents_per_topic . Topic . values ] c_tf_idf = ( global_c_tf_idf [ selected_topics ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Timestamps . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_timestamp = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], timestamp ) for topic , values in words_per_topic . items ()] topics_over_time . extend ( topics_at_timestamp ) if evolution_tuning : previous_topics = sorted ( list ( documents_per_topic . Topic . values )) previous_c_tf_idf = c_tf_idf . copy () return pd . DataFrame ( topics_over_time , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Timestamp\" ]) topics_per_class ( self , docs , topics , classes , global_tuning = True ) \u00b6 Create topics per class To create the topics per class, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each class c. Then, the c-TF-IDF representations at class c are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. This can be turned off if the pure representation is needed. !!! note Make sure to use a limited number of unique classes (<100) as the c-TF-IDF representation will be calculated at each single unique class. Having a large number of unique classes can take some time to be calculated. Parameters: Name Type Description Default docs List[str] The documents you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required classes Union[List[int], List[str]] The class of each document. This can be either a list of strings or ints. required global_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. True Returns: Type Description DataFrame topics_per_class: A dataframe that contains the topic, words, and frequency of topics for each class. Usage: from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_per_class = topic_model . topics_per_class ( docs , topics , classes ) Source code in bertopic\\_bertopic.py def topics_per_class ( self , docs : List [ str ], topics : List [ int ], classes : Union [ List [ int ], List [ str ]], global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics per class To create the topics per class, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each class c. Then, the c-TF-IDF representations at class c are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. This can be turned off if the pure representation is needed. NOTE: Make sure to use a limited number of unique classes (<100) as the c-TF-IDF representation will be calculated at each single unique class. Having a large number of unique classes can take some time to be calculated. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` classes: The class of each document. This can be either a list of strings or ints. global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. Returns: topics_per_class: A dataframe that contains the topic, words, and frequency of topics for each class. Usage: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_per_class = topic_model.topics_per_class(docs, topics, classes) ``` \"\"\" documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics , \"Class\" : classes }) global_c_tf_idf = normalize ( self . c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) # For each unique timestamp, create topic representations topics_per_class = [] for index , class_ in tqdm ( enumerate ( set ( classes )), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Class == class_ , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Class\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , m = len ( selection ), fit = False ) # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) c_tf_idf = ( global_c_tf_idf [ documents_per_topic . Topic . values + 1 ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Class . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_class = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], class_ ) for topic , values in words_per_topic . items ()] topics_per_class . extend ( topics_at_class ) topics_per_class = pd . DataFrame ( topics_per_class , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Class\" ]) return topics_per_class transform ( self , documents , embeddings = None ) \u00b6 After having fit a model, use transform to predict new instances Parameters: Name Type Description Default documents Union[str, List[str]] A single document or a list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model. None Returns: Type Description Tuple[List[int], numpy.ndarray] predictions: Topic predictions for each documents probabilities: The topic probability distribution which is returned by default. If calculate_probabilities in BERTopic is set to False, then the probabilities are not calculated to speed up computation and decrease memory usage. Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () . fit ( docs ) topics , probs = topic_model . transform ( docs ) If you want to use your own embeddings: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () . fit ( docs , embeddings ) topics , probs = topic_model . transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py def transform ( self , documents : Union [ str , List [ str ]], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" After having fit a model, use transform to predict new instances Arguments: documents: A single document or a list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution which is returned by default. If `calculate_probabilities` in BERTopic is set to False, then the probabilities are not calculated to speed up computation and decrease memory usage. Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) topics, probs = topic_model.transform(docs) ``` If you want to use your own embeddings: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) topics, probs = topic_model.transform(docs, embeddings) ``` \"\"\" check_is_fitted ( self ) check_embeddings_shape ( embeddings , documents ) if isinstance ( documents , str ): documents = [ documents ] if embeddings is None : embeddings = self . _extract_embeddings ( documents , method = \"document\" , verbose = self . verbose ) umap_embeddings = self . umap_model . transform ( embeddings ) predictions , probabilities = hdbscan . approximate_predict ( self . hdbscan_model , umap_embeddings ) if self . calculate_probabilities : probabilities = hdbscan . membership_vector ( self . hdbscan_model , umap_embeddings ) else : probabilities = None if self . mapped_topics : predictions = self . _map_predictions ( predictions ) probabilities = self . _map_probabilities ( probabilities ) return predictions , probabilities update_topics ( self , docs , topics , n_gram_range = None , vectorizer_model = None ) \u00b6 Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Parameters: Name Type Description Default docs List[str] The documents you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required n_gram_range Tuple[int, int] The n-gram range for the CountVectorizer. None vectorizer_model CountVectorizer Pass in your own CountVectorizer from scikit-learn None Usage: In order to update the topic representation, you will need to first fit the topic model and extract topics from them. Based on these, you can update the representation: topic_model . update_topics ( docs , topics , n_gram_range = ( 2 , 3 )) YOu can also use a custom vectorizer to update the representation: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( ngram_range = ( 1 , 2 ), stop_words = \"english\" ) topic_model . update_topics ( docs , topics , vectorizer_model = vectorizer_model ) Source code in bertopic\\_bertopic.py def update_topics ( self , docs : List [ str ], topics : List [ int ], n_gram_range : Tuple [ int , int ] = None , vectorizer_model : CountVectorizer = None ): \"\"\" Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` n_gram_range: The n-gram range for the CountVectorizer. vectorizer_model: Pass in your own CountVectorizer from scikit-learn Usage: In order to update the topic representation, you will need to first fit the topic model and extract topics from them. Based on these, you can update the representation: ```python topic_model.update_topics(docs, topics, n_gram_range=(2, 3)) ``` YOu can also use a custom vectorizer to update the representation: ```python from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\") topic_model.update_topics(docs, topics, vectorizer_model=vectorizer_model) ``` \"\"\" check_is_fitted ( self ) if not n_gram_range : n_gram_range = self . n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = n_gram_range ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) self . _extract_topics ( documents ) visualize_barchart ( self , topics = None , top_n_topics = 6 , n_words = 5 , width = 800 , height = 600 ) \u00b6 Visualize a barchart of selected topics Parameters: Name Type Description Default topics List[int] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. 6 n_words int Number of words to show in a topic 5 width int The width of the figure. 800 height int The height of the figure. 600 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the barchart of selected topics simply run: topic_model . visualize_barchart () Or if you want to save the resulting figure: fig = topic_model . visualize_barchart () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_barchart ( self , topics : List [ int ] = None , top_n_topics : int = 6 , n_words : int = 5 , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize a barchart of selected topics Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_words: Number of words to show in a topic width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the barchart of selected topics simply run: ```python topic_model.visualize_barchart() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_barchart() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_barchart ( self , topics = topics , top_n_topics = top_n_topics , n_words = n_words , width = width , height = height ) visualize_distribution ( self , probabilities , min_probability = 0.015 , width = 800 , height = 600 ) \u00b6 Visualize the distribution of topic probabilities Parameters: Name Type Description Default probabilities ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 width int The width of the figure. 800 height int The height of the figure. 600 Usage: Make sure to fit the model before and only input the probabilities of a single document: topic_model . visualize_distribution ( probabilities [ 0 ]) Or if you want to save the resulting figure: fig = topic_model . visualize_distribution ( probabilities [ 0 ]) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_distribution ( self , probabilities : np . ndarray , min_probability : float = 0.015 , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize the distribution of topic probabilities Arguments: probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. width: The width of the figure. height: The height of the figure. Usage: Make sure to fit the model before and only input the probabilities of a single document: ```python topic_model.visualize_distribution(probabilities[0]) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_distribution(probabilities[0]) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_distribution ( self , probabilities = probabilities , min_probability = min_probability , width = width , height = height ) visualize_heatmap ( self , topics = None , top_n_topics = None , n_clusters = None , width = 800 , height = 800 ) \u00b6 Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Parameters: Name Type Description Default topics List[int] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. None n_clusters int Create n clusters and order the similarity matrix by those clusters. None width int The width of the figure. 800 height int The height of the figure. 800 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the similarity matrix of topics simply run: topic_model . visualize_heatmap () Or if you want to save the resulting figure: fig = topic_model . visualize_heatmap () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_heatmap ( self , topics : List [ int ] = None , top_n_topics : int = None , n_clusters : int = None , width : int = 800 , height : int = 800 ) -> go . Figure : \"\"\" Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_clusters: Create n clusters and order the similarity matrix by those clusters. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the similarity matrix of topics simply run: ```python topic_model.visualize_heatmap() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_heatmap() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_heatmap ( self , topics = topics , top_n_topics = top_n_topics , n_clusters = n_clusters , width = width , height = height ) visualize_hierarchy ( self , orientation = 'left' , topics = None , top_n_topics = None , width = 1000 , height = 600 ) \u00b6 Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Parameters: Name Type Description Default orientation str The orientation of the figure. Either 'left' or 'bottom' 'left' topics List[int] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 1000 height int The height of the figure. 600 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the hierarchical structure of topics simply run: topic_model . visualize_hierarchy () Or if you want to save the resulting figure: fig = topic_model . visualize_hierarchy () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_hierarchy ( self , orientation : str = \"left\" , topics : List [ int ] = None , top_n_topics : int = None , width : int = 1000 , height : int = 600 ) -> go . Figure : \"\"\" Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Arguments: orientation: The orientation of the figure. Either 'left' or 'bottom' topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the hierarchical structure of topics simply run: ```python topic_model.visualize_hierarchy() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchy() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_hierarchy ( self , orientation = orientation , topics = topics , top_n_topics = top_n_topics , width = width , height = height ) visualize_term_rank ( self , topics = None , log_scale = False , width = 800 , height = 500 ) \u00b6 Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Parameters: Name Type Description Default topics List[int] A selection of topics to visualize. These will be colored red where all others will be colored black. None log_scale bool Whether to represent the ranking on a log scale False width int The width of the figure. 800 height int The height of the figure. 500 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the ranks of all words across all topics simply run: topic_model . visualize_word_rank () Or if you want to save the resulting figure: fig = topic_model . visualize_word_rank () fig . write_html ( \"path/to/file.html\" ) Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here . Source code in bertopic\\_bertopic.py def visualize_term_rank ( self , topics : List [ int ] = None , log_scale : bool = False , width : int = 800 , height : int = 500 ) -> go . Figure : \"\"\" Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Arguments: topics: A selection of topics to visualize. These will be colored red where all others will be colored black. log_scale: Whether to represent the ranking on a log scale width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the ranks of all words across all topics simply run: ```python topic_model.visualize_word_rank() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_word_rank() fig.write_html(\"path/to/file.html\") ``` Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/). Reference to that specific analysis can be found [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html). \"\"\" check_is_fitted ( self ) return plotting . visualize_term_rank ( self , topics = topics , log_scale = log_scale , width = width , height = height ) visualize_topics ( self , topics = None , top_n_topics = None , width = 650 , height = 650 ) \u00b6 Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Parameters: Name Type Description Default topics List[int] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 650 height int The height of the figure. 650 Usage: To visualize the topics simply run: topic_model . visualize_topics () Or if you want to save the resulting figure: fig = topic_model . visualize_topics () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_topics ( self , topics : List [ int ] = None , top_n_topics : int = None , width : int = 650 , height : int = 650 ) -> go . Figure : \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Arguments: topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Usage: To visualize the topics simply run: ```python topic_model.visualize_topics() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics ( self , topics = topics , top_n_topics = top_n_topics , width = width , height = height ) visualize_topics_over_time ( self , topics_over_time , top_n_topics = None , topics = None , width = 1250 , height = 450 ) \u00b6 Visualize topics over time Parameters: Name Type Description Default topics_over_time DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all None topics List[int] Select which topics you would like to be visualized None width int The width of the figure. 1250 height int The height of the figure. 450 Returns: Type Description Figure A plotly.graph_objects.Figure including all traces Usage: To visualize the topics over time, simply run: topics_over_time = topic_model . topics_over_time ( docs , topics , timestamps ) topic_model . visualize_topics_over_time ( topics_over_time ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_over_time ( topics_over_time ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_topics_over_time ( self , topics_over_time : pd . DataFrame , top_n_topics : int = None , topics : List [ int ] = None , width : int = 1250 , height : int = 450 ) -> go . Figure : \"\"\" Visualize topics over time Arguments: topics_over_time: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Usage: To visualize the topics over time, simply run: ```python topics_over_time = topic_model.topics_over_time(docs, topics, timestamps) topic_model.visualize_topics_over_time(topics_over_time) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_over_time(topics_over_time) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_over_time ( self , topics_over_time = topics_over_time , top_n_topics = top_n_topics , topics = topics , width = width , height = height ) visualize_topics_per_class ( self , topics_per_class , top_n_topics = 10 , topics = None , width = 1250 , height = 900 ) \u00b6 Visualize topics per class Parameters: Name Type Description Default topics_per_class DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all 10 topics List[int] Select which topics you would like to be visualized None width int The width of the figure. 1250 height int The height of the figure. 900 Returns: Type Description Figure A plotly.graph_objects.Figure including all traces Usage: To visualize the topics per class, simply run: topics_per_class = topic_model . topics_per_class ( docs , topics , classes ) topic_model . visualize_topics_per_class ( topics_per_class ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_per_class ( topics_per_class ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_topics_per_class ( self , topics_per_class : pd . DataFrame , top_n_topics : int = 10 , topics : List [ int ] = None , width : int = 1250 , height : int = 900 ) -> go . Figure : \"\"\" Visualize topics per class Arguments: topics_per_class: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Usage: To visualize the topics per class, simply run: ```python topics_per_class = topic_model.topics_per_class(docs, topics, classes) topic_model.visualize_topics_per_class(topics_per_class) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_per_class(topics_per_class) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_per_class ( self , topics_per_class = topics_per_class , top_n_topics = top_n_topics , topics = topics , width = width , height = height )","title":"BERTopic"},{"location":"api/bertopic.html#bertopic","text":"BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () topics , probabilities = topic_model . fit_transform ( docs ) If you want to use your own embedding model, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) topic_model = BERTopic ( embedding_model = sentence_model ) Due to the stochastisch nature of UMAP, the results from BERTopic might differ and the quality can degrade. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best.","title":"BERTopic"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.__init__","text":"BERTopic initialization Parameters: Name Type Description Default language str The main language used in your documents. For a full overview of supported languages see bertopic.backends.languages. Select \"multilingual\" to load in a sentence-tranformers model that supports 50+ languages. 'english' top_n_words int The number of words per topic to extract. Setting this too high can negatively impact topic embeddings as topics are typically best represented by at most 10 words. 10 n_gram_range Tuple[int, int] The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. NOTE: This param will not be used if you pass in your own CountVectorizer. (1, 1) min_topic_size int The minimum size of the topic. Increasing this value will lead to a lower number of clusters/topics. 10 nr_topics Union[int, str] Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that have a similarity of at least 0.9, do not maps all others. None low_memory bool Sets UMAP low memory to True to make sure less memory is used. False calculate_probabilities bool Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. This could slow down the extraction of topics if you have many documents (> 100_000). Set this only to True if you have a low amount of documents or if you do not mind more computation time. NOTE: If false you cannot use the corresponding visualization method visualize_probabilities . False seed_topic_list List[List[str]] A list of seed words per topic to converge around None verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False embedding_model Use a custom embedding model. The following backends are currently supported * SentenceTransformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html None umap_model UMAP Pass in a UMAP model to be used instead of the default None hdbscan_model HDBSCAN Pass in a hdbscan.HDBSCAN model to be used instead of the default None vectorizer_model CountVectorizer Pass in a CountVectorizer instead of the default None Source code in bertopic\\_bertopic.py def __init__ ( self , language : str = \"english\" , top_n_words : int = 10 , n_gram_range : Tuple [ int , int ] = ( 1 , 1 ), min_topic_size : int = 10 , nr_topics : Union [ int , str ] = None , low_memory : bool = False , calculate_probabilities : bool = False , seed_topic_list : List [ List [ str ]] = None , embedding_model = None , umap_model : UMAP = None , hdbscan_model : hdbscan . HDBSCAN = None , vectorizer_model : CountVectorizer = None , verbose : bool = False , ): \"\"\"BERTopic initialization Arguments: language: The main language used in your documents. For a full overview of supported languages see bertopic.backends.languages. Select \"multilingual\" to load in a sentence-tranformers model that supports 50+ languages. top_n_words: The number of words per topic to extract. Setting this too high can negatively impact topic embeddings as topics are typically best represented by at most 10 words. n_gram_range: The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. NOTE: This param will not be used if you pass in your own CountVectorizer. min_topic_size: The minimum size of the topic. Increasing this value will lead to a lower number of clusters/topics. nr_topics: Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that have a similarity of at least 0.9, do not maps all others. low_memory: Sets UMAP low memory to True to make sure less memory is used. calculate_probabilities: Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. This could slow down the extraction of topics if you have many documents (> 100_000). Set this only to True if you have a low amount of documents or if you do not mind more computation time. NOTE: If false you cannot use the corresponding visualization method `visualize_probabilities`. seed_topic_list: A list of seed words per topic to converge around verbose: Changes the verbosity of the model, Set to True if you want to track the stages of the model. embedding_model: Use a custom embedding model. The following backends are currently supported * SentenceTransformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html umap_model: Pass in a UMAP model to be used instead of the default hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default vectorizer_model: Pass in a CountVectorizer instead of the default \"\"\" # Topic-based parameters if top_n_words > 30 : raise ValueError ( \"top_n_words should be lower or equal to 30. The preferred value is 10.\" ) self . top_n_words = top_n_words self . min_topic_size = min_topic_size self . nr_topics = nr_topics self . low_memory = low_memory self . calculate_probabilities = calculate_probabilities self . verbose = verbose self . seed_topic_list = seed_topic_list # Embedding model self . language = language if not embedding_model else None self . embedding_model = embedding_model # Vectorizer self . n_gram_range = n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = self . n_gram_range ) # UMAP self . umap_model = umap_model or UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' , low_memory = self . low_memory ) # HDBSCAN self . hdbscan_model = hdbscan_model or hdbscan . HDBSCAN ( min_cluster_size = self . min_topic_size , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) self . topics = None self . topic_sizes = None self . mapped_topics = None self . merged_topics = None self . topic_embeddings = None self . topic_sim_matrix = None self . representative_docs = None if verbose : logger . set_level ( \"DEBUG\" )","title":"__init__()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.__str__","text":"Get a string representation of the current object. Returns: Type Description str Human readable representation of the most important model parameters. The parameters that represent models are ignored due to their Source code in bertopic\\_bertopic.py def __str__ ( self ): \"\"\"Get a string representation of the current object. Returns: str: Human readable representation of the most important model parameters. The parameters that represent models are ignored due to their \"\"\" parameters = \"\" for parameter , value in self . get_params () . items (): value = str ( value ) if \"(\" in value and value [ 0 ] != \"(\" : value = value . split ( \"(\" )[ 0 ] + \"(...)\" parameters += f \" { parameter } = { value } , \" return f \"BERTopic( { parameters [: - 2 ] } )\"","title":"__str__()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.find_topics","text":"Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Parameters: Name Type Description Default search_term str the term you want to use to search for topics required top_n int the number of topics to return 5 Returns: Type Description Tuple[List[int], List[float]] similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low Usage: You can use the underlying embedding model to find topics that best represent the search term: topics , similarity = topic_model . find_topics ( \"sports\" , top_n = 5 ) Note that the search query is typically more accurate if the search_term consists of a phrase or multiple words. Source code in bertopic\\_bertopic.py def find_topics ( self , search_term : str , top_n : int = 5 ) -> Tuple [ List [ int ], List [ float ]]: \"\"\" Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Arguments: search_term: the term you want to use to search for topics top_n: the number of topics to return Returns: similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low Usage: You can use the underlying embedding model to find topics that best represent the search term: ```python topics, similarity = topic_model.find_topics(\"sports\", top_n=5) ``` Note that the search query is typically more accurate if the search_term consists of a phrase or multiple words. \"\"\" if self . embedding_model is None : raise Exception ( \"This method can only be used if you did not use custom embeddings.\" ) topic_list = list ( self . topics . keys ()) topic_list . sort () # Extract search_term embeddings and compare with topic embeddings search_embedding = self . _extract_embeddings ([ search_term ], method = \"word\" , verbose = False ) . flatten () sims = cosine_similarity ( search_embedding . reshape ( 1 , - 1 ), self . topic_embeddings ) . flatten () # Extract topics most similar to search_term ids = np . argsort ( sims )[ - top_n :] similarity = [ sims [ i ] for i in ids ][:: - 1 ] similar_topics = [ topic_list [ index ] for index in ids ][:: - 1 ] return similar_topics , similarity","title":"find_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.fit","text":"Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Parameters: Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None y Union[List[int], numpy.ndarray] The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. None Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () . fit ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () . fit ( docs , embeddings ) Source code in bertopic\\_bertopic.py def fit ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) ``` \"\"\" self . fit_transform ( documents , embeddings , y ) return self","title":"fit()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.fit_transform","text":"Fit the models on a collection of documents, generate topics, and return the docs with topics Parameters: Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None y Union[List[int], numpy.ndarray] The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. None Returns: Type Description Tuple[List[int], Optional[numpy.ndarray]] predictions: Topic predictions for each documents probabilities: The probability of the assigned topic per document. If calculate_probabilities in BERTopic is set to True, then it calculates the probabilities of all topics across all documents instead of only the assigned topic. This, however, slows down computation and may increase memory usage. Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py def fit_transform ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ) -> Tuple [ List [ int ], Union [ np . ndarray , None ]]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Returns: predictions: Topic predictions for each documents probabilities: The probability of the assigned topic per document. If `calculate_probabilities` in BERTopic is set to True, then it calculates the probabilities of all topics across all documents instead of only the assigned topic. This, however, slows down computation and may increase memory usage. Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) check_embeddings_shape ( embeddings , documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if embeddings is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) embeddings = self . _extract_embeddings ( documents . Document , method = \"document\" , verbose = self . verbose ) logger . info ( \"Transformed documents to Embeddings\" ) else : if self . embedding_model is not None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) # Reduce dimensionality with UMAP if self . seed_topic_list is not None and self . embedding_model is not None : y , embeddings = self . _guided_topic_modeling ( embeddings ) umap_embeddings = self . _reduce_dimensionality ( embeddings , y ) # Cluster UMAP embeddings with HDBSCAN documents , probabilities = self . _cluster_embeddings ( umap_embeddings , documents ) # Sort and Map Topic IDs by their frequency if not self . nr_topics : documents = self . _sort_mappings_by_frequency ( documents ) # Extract topics by calculating c-TF-IDF self . _extract_topics ( documents ) # Reduce topics if self . nr_topics : documents = self . _reduce_topics ( documents ) self . _map_representative_docs () probabilities = self . _map_probabilities ( probabilities ) predictions = documents . Topic . to_list () return predictions , probabilities","title":"fit_transform()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_params","text":"Get parameters for this estimator. Adapted from: https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 Parameters: Name Type Description Default deep bool bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. False Returns: Type Description Mapping[str, Any] out: Parameter names mapped to their values. Source code in bertopic\\_bertopic.py def get_params ( self , deep : bool = False ) -> Mapping [ str , Any ]: \"\"\" Get parameters for this estimator. Adapted from: https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 Arguments: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: out: Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names (): value = getattr ( self , key ) if deep and hasattr ( value , 'get_params' ): deep_items = value . get_params () . items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"get_params()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_representative_docs","text":"Extract representative documents per topic Parameters: Name Type Description Default topic int A specific topic for which you want the representative documents required Returns: Type Description List[str] Representative documents of the chosen topic Usage: To extract the representative docs of all topics: representative_docs = topic_model . get_representative_docs () To get the representative docs of a single topic: representative_docs = topic_model . get_representative_docs ( 12 ) Source code in bertopic\\_bertopic.py def get_representative_docs ( self , topic : int ) -> List [ str ]: \"\"\" Extract representative documents per topic Arguments: topic: A specific topic for which you want the representative documents Returns: Representative documents of the chosen topic Usage: To extract the representative docs of all topics: ```python representative_docs = topic_model.get_representative_docs() ``` To get the representative docs of a single topic: ```python representative_docs = topic_model.get_representative_docs(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . representative_docs [ topic ] else : return self . representative_docs","title":"get_representative_docs()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topic","text":"Return top n words for a specific topic and their c-TF-IDF scores Parameters: Name Type Description Default topic int A specific topic for which you want its representation required Returns: Type Description Union[Mapping[str, Tuple[str, float]], bool] The top n words for a specific word and its respective c-TF-IDF scores Usage: topic = topic_model . get_topic ( 12 ) Source code in bertopic\\_bertopic.py def get_topic ( self , topic : int ) -> Union [ Mapping [ str , Tuple [ str , float ]], bool ]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores Arguments: topic: A specific topic for which you want its representation Returns: The top n words for a specific word and its respective c-TF-IDF scores Usage: ```python topic = topic_model.get_topic(12) ``` \"\"\" check_is_fitted ( self ) if topic in self . topics : return self . topics [ topic ] else : return False","title":"get_topic()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topic_freq","text":"Return the the size of topics (descending order) Parameters: Name Type Description Default topic int A specific topic for which you want the frequency None Returns: Type Description Union[pandas.core.frame.DataFrame, int] Either the frequency of a single topic or dataframe with the frequencies of all topics Usage: To extract the frequency of all topics: frequency = topic_model . get_topic_freq () To get the frequency of a single topic: frequency = topic_model . get_topic_freq ( 12 ) Source code in bertopic\\_bertopic.py def get_topic_freq ( self , topic : int = None ) -> Union [ pd . DataFrame , int ]: \"\"\" Return the the size of topics (descending order) Arguments: topic: A specific topic for which you want the frequency Returns: Either the frequency of a single topic or dataframe with the frequencies of all topics Usage: To extract the frequency of all topics: ```python frequency = topic_model.get_topic_freq() ``` To get the frequency of a single topic: ```python frequency = topic_model.get_topic_freq(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . topic_sizes [ topic ] else : return pd . DataFrame ( self . topic_sizes . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False )","title":"get_topic_freq()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topic_info","text":"Get information about each topic including its id, frequency, and name Parameters: Name Type Description Default topic int A specific topic for which you want the frequency None Returns: Type Description DataFrame info: The information relating to either a single topic or all topics Usage: info_df = topic_model . get_topic_info () Source code in bertopic\\_bertopic.py def get_topic_info ( self , topic : int = None ) -> pd . DataFrame : \"\"\" Get information about each topic including its id, frequency, and name Arguments: topic: A specific topic for which you want the frequency Returns: info: The information relating to either a single topic or all topics Usage: ```python info_df = topic_model.get_topic_info() ``` \"\"\" check_is_fitted ( self ) info = pd . DataFrame ( self . topic_sizes . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False ) info [ \"Name\" ] = info . Topic . map ( self . topic_names ) if topic : info = info . loc [ info . Topic == topic , :] return info","title":"get_topic_info()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topics","text":"Return topics with top n words and their c-TF-IDF score Returns: Type Description Mapping[str, Tuple[str, float]] self.topic: The top n words per topic and the corresponding c-TF-IDF score Usage: all_topics = topic_model . get_topics () Source code in bertopic\\_bertopic.py def get_topics ( self ) -> Mapping [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score Returns: self.topic: The top n words per topic and the corresponding c-TF-IDF score Usage: ```python all_topics = topic_model.get_topics() ``` \"\"\" check_is_fitted ( self ) return self . topics","title":"get_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.load","text":"Loads the model from the specified path Parameters: Name Type Description Default path str the location and name of the BERTopic file you want to load required embedding_model If the embedding_model was not saved to save space or to load it in from the cloud, you can load it in by specifying it here. None Usage: BERTopic . load ( \"my_model\" ) or if you did not save the embedding model: BERTopic . load ( \"my_model\" , embedding_model = \"paraphrase-MiniLM-L6-v2\" ) Source code in bertopic\\_bertopic.py @classmethod def load ( cls , path : str , embedding_model = None ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the BERTopic file you want to load embedding_model: If the embedding_model was not saved to save space or to load it in from the cloud, you can load it in by specifying it here. Usage: ```python BERTopic.load(\"my_model\") ``` or if you did not save the embedding model: ```python BERTopic.load(\"my_model\", embedding_model=\"paraphrase-MiniLM-L6-v2\") ``` \"\"\" with open ( path , 'rb' ) as file : if embedding_model : topic_model = joblib . load ( file ) topic_model . embedding_model = select_backend ( embedding_model ) else : topic_model = joblib . load ( file ) return topic_model","title":"load()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.reduce_topics","text":"Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. The reasoning for putting docs , topics , and probs as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic instead of a dedicated database. Parameters: Name Type Description Default docs List[str] The docs you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required probabilities ndarray The probabilities that were returned when calling either fit or fit_transform None nr_topics int The number of topics you want reduced to 20 Returns: Type Description Tuple[List[int], numpy.ndarray] new_topics: Updated topics new_probabilities: Updated probabilities Usage: You can further reduce the topics by passing the documents with its topics and probabilities (if they were calculated): new_topics , new_probs = topic_model . reduce_topics ( docs , topics , probabilities , nr_topics = 30 ) If probabilities were not calculated simply run the function without them: new_topics , new_probs = topic_model . reduce_topics ( docs , topics , nr_topics = 30 ) Source code in bertopic\\_bertopic.py def reduce_topics ( self , docs : List [ str ], topics : List [ int ], probabilities : np . ndarray = None , nr_topics : int = 20 ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. The reasoning for putting `docs`, `topics`, and `probs` as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic instead of a dedicated database. Arguments: docs: The docs you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` probabilities: The probabilities that were returned when calling either `fit` or `fit_transform` nr_topics: The number of topics you want reduced to Returns: new_topics: Updated topics new_probabilities: Updated probabilities Usage: You can further reduce the topics by passing the documents with its topics and probabilities (if they were calculated): ```python new_topics, new_probs = topic_model.reduce_topics(docs, topics, probabilities, nr_topics=30) ``` If probabilities were not calculated simply run the function without them: ```python new_topics, new_probs = topic_model.reduce_topics(docs, topics, nr_topics=30) ``` \"\"\" check_is_fitted ( self ) self . nr_topics = nr_topics documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) # Reduce number of topics self . _extract_topics ( documents ) documents = self . _reduce_topics ( documents ) # Extract topics and map probabilities new_topics = documents . Topic . to_list () new_probabilities = self . _map_probabilities ( probabilities ) return new_topics , new_probabilities","title":"reduce_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.save","text":"Saves the model to the specified path Parameters: Name Type Description Default path str the location and name of the file you want to save required save_embedding_model bool Whether to save the embedding model in this class as you might have selected a local model or one that is downloaded automatically from the cloud. True Usage: topic_model . save ( \"my_model\" ) or if you do not want the embedding_model to be saved locally: topic_model . save ( \"my_model\" , save_embedding_model = False ) Source code in bertopic\\_bertopic.py def save ( self , path : str , save_embedding_model : bool = True ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save save_embedding_model: Whether to save the embedding model in this class as you might have selected a local model or one that is downloaded automatically from the cloud. Usage: ```python topic_model.save(\"my_model\") ``` or if you do not want the embedding_model to be saved locally: ```python topic_model.save(\"my_model\", save_embedding_model=False) ``` \"\"\" with open ( path , 'wb' ) as file : if not save_embedding_model : embedding_model = self . embedding_model self . embedding_model = None joblib . dump ( self , file ) self . embedding_model = embedding_model else : joblib . dump ( self , file )","title":"save()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.topics_over_time","text":"Create topics over time To create the topics over time, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each timestamp t. Then, the c-TF-IDF representations at timestamp t are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. !!! note Make sure to use a limited number of unique timestamps (<100) as the c-TF-IDF representation will be calculated at each single unique timestamp. Having a large number of unique timestamps can take some time to be calculated. Moreover, there aren't many use-cased where you would like to see the difference in topic representations over more than 100 different timestamps. Parameters: Name Type Description Default docs List[str] The documents you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required timestamps Union[List[str], List[int]] The timestamp of each document. This can be either a list of strings or ints. If it is a list of strings, then the datetime format will be automatically inferred. If it is a list of ints, then the documents will be ordered by ascending order. required nr_bins int The number of bins you want to create for the timestamps. The left interval will be chosen as the timestamp. An additional column will be created with the entire interval. None datetime_format str The datetime format of the timestamps if they are strings, eg \u201c%d/%m/%Y\u201d. Set this to None if you want to have it automatically detect the format. See strftime documentation for more information on choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior. None evolution_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1. This creates evolutionary topic representations. True global_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. True Returns: Type Description DataFrame topics_over_time: A dataframe that contains the topic, words, and frequency of topic at timestamp t. Usage: The timestamps variable represent the timestamp of each document. If you have over 100 unique timestamps, it is advised to bin the timestamps as shown below: from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_over_time = topic_model . topics_over_time ( docs , topics , timestamps , nr_bins = 20 ) Source code in bertopic\\_bertopic.py def topics_over_time ( self , docs : List [ str ], topics : List [ int ], timestamps : Union [ List [ str ], List [ int ]], nr_bins : int = None , datetime_format : str = None , evolution_tuning : bool = True , global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics over time To create the topics over time, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each timestamp t. Then, the c-TF-IDF representations at timestamp t are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. NOTE: Make sure to use a limited number of unique timestamps (<100) as the c-TF-IDF representation will be calculated at each single unique timestamp. Having a large number of unique timestamps can take some time to be calculated. Moreover, there aren't many use-cased where you would like to see the difference in topic representations over more than 100 different timestamps. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` timestamps: The timestamp of each document. This can be either a list of strings or ints. If it is a list of strings, then the datetime format will be automatically inferred. If it is a list of ints, then the documents will be ordered by ascending order. nr_bins: The number of bins you want to create for the timestamps. The left interval will be chosen as the timestamp. An additional column will be created with the entire interval. datetime_format: The datetime format of the timestamps if they are strings, eg \u201c%d/%m/%Y\u201d. Set this to None if you want to have it automatically detect the format. See strftime documentation for more information on choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior. evolution_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1. This creates evolutionary topic representations. global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. Returns: topics_over_time: A dataframe that contains the topic, words, and frequency of topic at timestamp t. Usage: The timestamps variable represent the timestamp of each document. If you have over 100 unique timestamps, it is advised to bin the timestamps as shown below: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_over_time = topic_model.topics_over_time(docs, topics, timestamps, nr_bins=20) ``` \"\"\" check_is_fitted ( self ) check_documents_type ( docs ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics , \"Timestamps\" : timestamps }) global_c_tf_idf = normalize ( self . c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) all_topics = sorted ( list ( documents . Topic . unique ())) all_topics_indices = { topic : index for index , topic in enumerate ( all_topics )} if isinstance ( timestamps [ 0 ], str ): infer_datetime_format = True if not datetime_format else False documents [ \"Timestamps\" ] = pd . to_datetime ( documents [ \"Timestamps\" ], infer_datetime_format = infer_datetime_format , format = datetime_format ) if nr_bins : documents [ \"Bins\" ] = pd . cut ( documents . Timestamps , bins = nr_bins ) documents [ \"Timestamps\" ] = documents . apply ( lambda row : row . Bins . left , 1 ) # Sort documents in chronological order documents = documents . sort_values ( \"Timestamps\" ) timestamps = documents . Timestamps . unique () if len ( timestamps ) > 100 : warnings . warn ( f \"There are more than 100 unique timestamps (i.e., { len ( timestamps ) } ) \" \"which significantly slows down the application. Consider setting `nr_bins` \" \"to a value lower than 100 to speed up calculation. \" ) # For each unique timestamp, create topic representations topics_over_time = [] for index , timestamp in tqdm ( enumerate ( timestamps ), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Timestamps == timestamp , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Timestamps\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , m = len ( selection ), fit = False ) if global_tuning or evolution_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) # Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the c-TF-IDF # matrix at timestamp t-1 if evolution_tuning and index != 0 : current_topics = sorted ( list ( documents_per_topic . Topic . values )) overlapping_topics = sorted ( list ( set ( previous_topics ) . intersection ( set ( current_topics )))) current_overlap_idx = [ current_topics . index ( topic ) for topic in overlapping_topics ] previous_overlap_idx = [ previous_topics . index ( topic ) for topic in overlapping_topics ] c_tf_idf . tolil ()[ current_overlap_idx ] = (( c_tf_idf [ current_overlap_idx ] + previous_c_tf_idf [ previous_overlap_idx ]) / 2.0 ) . tolil () # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : selected_topics = [ all_topics_indices [ topic ] for topic in documents_per_topic . Topic . values ] c_tf_idf = ( global_c_tf_idf [ selected_topics ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Timestamps . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_timestamp = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], timestamp ) for topic , values in words_per_topic . items ()] topics_over_time . extend ( topics_at_timestamp ) if evolution_tuning : previous_topics = sorted ( list ( documents_per_topic . Topic . values )) previous_c_tf_idf = c_tf_idf . copy () return pd . DataFrame ( topics_over_time , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Timestamp\" ])","title":"topics_over_time()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.topics_per_class","text":"Create topics per class To create the topics per class, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each class c. Then, the c-TF-IDF representations at class c are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. This can be turned off if the pure representation is needed. !!! note Make sure to use a limited number of unique classes (<100) as the c-TF-IDF representation will be calculated at each single unique class. Having a large number of unique classes can take some time to be calculated. Parameters: Name Type Description Default docs List[str] The documents you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required classes Union[List[int], List[str]] The class of each document. This can be either a list of strings or ints. required global_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. True Returns: Type Description DataFrame topics_per_class: A dataframe that contains the topic, words, and frequency of topics for each class. Usage: from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_per_class = topic_model . topics_per_class ( docs , topics , classes ) Source code in bertopic\\_bertopic.py def topics_per_class ( self , docs : List [ str ], topics : List [ int ], classes : Union [ List [ int ], List [ str ]], global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics per class To create the topics per class, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each class c. Then, the c-TF-IDF representations at class c are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. This can be turned off if the pure representation is needed. NOTE: Make sure to use a limited number of unique classes (<100) as the c-TF-IDF representation will be calculated at each single unique class. Having a large number of unique classes can take some time to be calculated. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` classes: The class of each document. This can be either a list of strings or ints. global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. Returns: topics_per_class: A dataframe that contains the topic, words, and frequency of topics for each class. Usage: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_per_class = topic_model.topics_per_class(docs, topics, classes) ``` \"\"\" documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics , \"Class\" : classes }) global_c_tf_idf = normalize ( self . c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) # For each unique timestamp, create topic representations topics_per_class = [] for index , class_ in tqdm ( enumerate ( set ( classes )), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Class == class_ , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Class\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , m = len ( selection ), fit = False ) # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) c_tf_idf = ( global_c_tf_idf [ documents_per_topic . Topic . values + 1 ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Class . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_class = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], class_ ) for topic , values in words_per_topic . items ()] topics_per_class . extend ( topics_at_class ) topics_per_class = pd . DataFrame ( topics_per_class , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Class\" ]) return topics_per_class","title":"topics_per_class()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.transform","text":"After having fit a model, use transform to predict new instances Parameters: Name Type Description Default documents Union[str, List[str]] A single document or a list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model. None Returns: Type Description Tuple[List[int], numpy.ndarray] predictions: Topic predictions for each documents probabilities: The topic probability distribution which is returned by default. If calculate_probabilities in BERTopic is set to False, then the probabilities are not calculated to speed up computation and decrease memory usage. Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () . fit ( docs ) topics , probs = topic_model . transform ( docs ) If you want to use your own embeddings: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () . fit ( docs , embeddings ) topics , probs = topic_model . transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py def transform ( self , documents : Union [ str , List [ str ]], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" After having fit a model, use transform to predict new instances Arguments: documents: A single document or a list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution which is returned by default. If `calculate_probabilities` in BERTopic is set to False, then the probabilities are not calculated to speed up computation and decrease memory usage. Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) topics, probs = topic_model.transform(docs) ``` If you want to use your own embeddings: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) topics, probs = topic_model.transform(docs, embeddings) ``` \"\"\" check_is_fitted ( self ) check_embeddings_shape ( embeddings , documents ) if isinstance ( documents , str ): documents = [ documents ] if embeddings is None : embeddings = self . _extract_embeddings ( documents , method = \"document\" , verbose = self . verbose ) umap_embeddings = self . umap_model . transform ( embeddings ) predictions , probabilities = hdbscan . approximate_predict ( self . hdbscan_model , umap_embeddings ) if self . calculate_probabilities : probabilities = hdbscan . membership_vector ( self . hdbscan_model , umap_embeddings ) else : probabilities = None if self . mapped_topics : predictions = self . _map_predictions ( predictions ) probabilities = self . _map_probabilities ( probabilities ) return predictions , probabilities","title":"transform()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.update_topics","text":"Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Parameters: Name Type Description Default docs List[str] The documents you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required n_gram_range Tuple[int, int] The n-gram range for the CountVectorizer. None vectorizer_model CountVectorizer Pass in your own CountVectorizer from scikit-learn None Usage: In order to update the topic representation, you will need to first fit the topic model and extract topics from them. Based on these, you can update the representation: topic_model . update_topics ( docs , topics , n_gram_range = ( 2 , 3 )) YOu can also use a custom vectorizer to update the representation: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( ngram_range = ( 1 , 2 ), stop_words = \"english\" ) topic_model . update_topics ( docs , topics , vectorizer_model = vectorizer_model ) Source code in bertopic\\_bertopic.py def update_topics ( self , docs : List [ str ], topics : List [ int ], n_gram_range : Tuple [ int , int ] = None , vectorizer_model : CountVectorizer = None ): \"\"\" Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` n_gram_range: The n-gram range for the CountVectorizer. vectorizer_model: Pass in your own CountVectorizer from scikit-learn Usage: In order to update the topic representation, you will need to first fit the topic model and extract topics from them. Based on these, you can update the representation: ```python topic_model.update_topics(docs, topics, n_gram_range=(2, 3)) ``` YOu can also use a custom vectorizer to update the representation: ```python from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\") topic_model.update_topics(docs, topics, vectorizer_model=vectorizer_model) ``` \"\"\" check_is_fitted ( self ) if not n_gram_range : n_gram_range = self . n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = n_gram_range ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) self . _extract_topics ( documents )","title":"update_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_barchart","text":"Visualize a barchart of selected topics Parameters: Name Type Description Default topics List[int] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. 6 n_words int Number of words to show in a topic 5 width int The width of the figure. 800 height int The height of the figure. 600 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the barchart of selected topics simply run: topic_model . visualize_barchart () Or if you want to save the resulting figure: fig = topic_model . visualize_barchart () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_barchart ( self , topics : List [ int ] = None , top_n_topics : int = 6 , n_words : int = 5 , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize a barchart of selected topics Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_words: Number of words to show in a topic width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the barchart of selected topics simply run: ```python topic_model.visualize_barchart() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_barchart() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_barchart ( self , topics = topics , top_n_topics = top_n_topics , n_words = n_words , width = width , height = height )","title":"visualize_barchart()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_distribution","text":"Visualize the distribution of topic probabilities Parameters: Name Type Description Default probabilities ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 width int The width of the figure. 800 height int The height of the figure. 600 Usage: Make sure to fit the model before and only input the probabilities of a single document: topic_model . visualize_distribution ( probabilities [ 0 ]) Or if you want to save the resulting figure: fig = topic_model . visualize_distribution ( probabilities [ 0 ]) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_distribution ( self , probabilities : np . ndarray , min_probability : float = 0.015 , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize the distribution of topic probabilities Arguments: probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. width: The width of the figure. height: The height of the figure. Usage: Make sure to fit the model before and only input the probabilities of a single document: ```python topic_model.visualize_distribution(probabilities[0]) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_distribution(probabilities[0]) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_distribution ( self , probabilities = probabilities , min_probability = min_probability , width = width , height = height )","title":"visualize_distribution()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_heatmap","text":"Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Parameters: Name Type Description Default topics List[int] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. None n_clusters int Create n clusters and order the similarity matrix by those clusters. None width int The width of the figure. 800 height int The height of the figure. 800 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the similarity matrix of topics simply run: topic_model . visualize_heatmap () Or if you want to save the resulting figure: fig = topic_model . visualize_heatmap () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_heatmap ( self , topics : List [ int ] = None , top_n_topics : int = None , n_clusters : int = None , width : int = 800 , height : int = 800 ) -> go . Figure : \"\"\" Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_clusters: Create n clusters and order the similarity matrix by those clusters. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the similarity matrix of topics simply run: ```python topic_model.visualize_heatmap() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_heatmap() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_heatmap ( self , topics = topics , top_n_topics = top_n_topics , n_clusters = n_clusters , width = width , height = height )","title":"visualize_heatmap()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_hierarchy","text":"Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Parameters: Name Type Description Default orientation str The orientation of the figure. Either 'left' or 'bottom' 'left' topics List[int] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 1000 height int The height of the figure. 600 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the hierarchical structure of topics simply run: topic_model . visualize_hierarchy () Or if you want to save the resulting figure: fig = topic_model . visualize_hierarchy () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_hierarchy ( self , orientation : str = \"left\" , topics : List [ int ] = None , top_n_topics : int = None , width : int = 1000 , height : int = 600 ) -> go . Figure : \"\"\" Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Arguments: orientation: The orientation of the figure. Either 'left' or 'bottom' topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the hierarchical structure of topics simply run: ```python topic_model.visualize_hierarchy() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchy() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_hierarchy ( self , orientation = orientation , topics = topics , top_n_topics = top_n_topics , width = width , height = height )","title":"visualize_hierarchy()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_term_rank","text":"Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Parameters: Name Type Description Default topics List[int] A selection of topics to visualize. These will be colored red where all others will be colored black. None log_scale bool Whether to represent the ranking on a log scale False width int The width of the figure. 800 height int The height of the figure. 500 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the ranks of all words across all topics simply run: topic_model . visualize_word_rank () Or if you want to save the resulting figure: fig = topic_model . visualize_word_rank () fig . write_html ( \"path/to/file.html\" ) Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here . Source code in bertopic\\_bertopic.py def visualize_term_rank ( self , topics : List [ int ] = None , log_scale : bool = False , width : int = 800 , height : int = 500 ) -> go . Figure : \"\"\" Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Arguments: topics: A selection of topics to visualize. These will be colored red where all others will be colored black. log_scale: Whether to represent the ranking on a log scale width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the ranks of all words across all topics simply run: ```python topic_model.visualize_word_rank() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_word_rank() fig.write_html(\"path/to/file.html\") ``` Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/). Reference to that specific analysis can be found [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html). \"\"\" check_is_fitted ( self ) return plotting . visualize_term_rank ( self , topics = topics , log_scale = log_scale , width = width , height = height )","title":"visualize_term_rank()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_topics","text":"Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Parameters: Name Type Description Default topics List[int] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 650 height int The height of the figure. 650 Usage: To visualize the topics simply run: topic_model . visualize_topics () Or if you want to save the resulting figure: fig = topic_model . visualize_topics () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_topics ( self , topics : List [ int ] = None , top_n_topics : int = None , width : int = 650 , height : int = 650 ) -> go . Figure : \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Arguments: topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Usage: To visualize the topics simply run: ```python topic_model.visualize_topics() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics ( self , topics = topics , top_n_topics = top_n_topics , width = width , height = height )","title":"visualize_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_topics_over_time","text":"Visualize topics over time Parameters: Name Type Description Default topics_over_time DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all None topics List[int] Select which topics you would like to be visualized None width int The width of the figure. 1250 height int The height of the figure. 450 Returns: Type Description Figure A plotly.graph_objects.Figure including all traces Usage: To visualize the topics over time, simply run: topics_over_time = topic_model . topics_over_time ( docs , topics , timestamps ) topic_model . visualize_topics_over_time ( topics_over_time ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_over_time ( topics_over_time ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_topics_over_time ( self , topics_over_time : pd . DataFrame , top_n_topics : int = None , topics : List [ int ] = None , width : int = 1250 , height : int = 450 ) -> go . Figure : \"\"\" Visualize topics over time Arguments: topics_over_time: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Usage: To visualize the topics over time, simply run: ```python topics_over_time = topic_model.topics_over_time(docs, topics, timestamps) topic_model.visualize_topics_over_time(topics_over_time) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_over_time(topics_over_time) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_over_time ( self , topics_over_time = topics_over_time , top_n_topics = top_n_topics , topics = topics , width = width , height = height )","title":"visualize_topics_over_time()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_topics_per_class","text":"Visualize topics per class Parameters: Name Type Description Default topics_per_class DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all 10 topics List[int] Select which topics you would like to be visualized None width int The width of the figure. 1250 height int The height of the figure. 900 Returns: Type Description Figure A plotly.graph_objects.Figure including all traces Usage: To visualize the topics per class, simply run: topics_per_class = topic_model . topics_per_class ( docs , topics , classes ) topic_model . visualize_topics_per_class ( topics_per_class ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_per_class ( topics_per_class ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py def visualize_topics_per_class ( self , topics_per_class : pd . DataFrame , top_n_topics : int = 10 , topics : List [ int ] = None , width : int = 1250 , height : int = 900 ) -> go . Figure : \"\"\" Visualize topics per class Arguments: topics_per_class: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Usage: To visualize the topics per class, simply run: ```python topics_per_class = topic_model.topics_per_class(docs, topics, classes) topic_model.visualize_topics_per_class(topics_per_class) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_per_class(topics_per_class) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_per_class ( self , topics_per_class = topics_per_class , top_n_topics = top_n_topics , topics = topics , width = width , height = height )","title":"visualize_topics_per_class()"},{"location":"api/ctfidf.html","text":"c-TF-IDF \u00b6 A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base. C-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes by joining all documents per class. Thus, each class is converted to a single document instead of set of documents. Then, the frequency of words t are extracted for each class i and divided by the total number of words w . Next, the total, unjoined, number of documents across all classes m is divided by the total sum of word i across all classes. fit ( self , X , n_samples , multiplier = None ) \u00b6 Learn the idf vector (global term weights). Parameters: Name Type Description Default X csr_matrix A matrix of term/token counts. required n_samples int Number of total documents required Source code in bertopic\\_ctfidf.py def fit ( self , X : sp . csr_matrix , n_samples : int , multiplier : np . ndarray = None ): \"\"\"Learn the idf vector (global term weights). Arguments: X: A matrix of term/token counts. n_samples: Number of total documents \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) avg_nr_samples = int ( X . sum ( axis = 1 ) . mean ()) idf = np . log ( avg_nr_samples / df ) if multiplier is not None : idf = idf * multiplier self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self transform ( self , X , copy = True ) \u00b6 Transform a count-based matrix to c-TF-IDF Parameters: Name Type Description Default X csr_matrix A matrix of term/token counts. required Returns: Type Description X (sparse matrix) A c-TF-IDF matrix Source code in bertopic\\_ctfidf.py def transform ( self , X : sp . csr_matrix , copy = True ): \"\"\"Transform a count-based matrix to c-TF-IDF Arguments: X (sparse matrix): A matrix of term/token counts. Returns: X (sparse matrix): A c-TF-IDF matrix \"\"\" if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) X = X * self . _idf_diag return X","title":"cTFIDF"},{"location":"api/ctfidf.html#c-tf-idf","text":"A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base. C-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes by joining all documents per class. Thus, each class is converted to a single document instead of set of documents. Then, the frequency of words t are extracted for each class i and divided by the total number of words w . Next, the total, unjoined, number of documents across all classes m is divided by the total sum of word i across all classes.","title":"c-TF-IDF"},{"location":"api/ctfidf.html#bertopic._ctfidf.ClassTFIDF.fit","text":"Learn the idf vector (global term weights). Parameters: Name Type Description Default X csr_matrix A matrix of term/token counts. required n_samples int Number of total documents required Source code in bertopic\\_ctfidf.py def fit ( self , X : sp . csr_matrix , n_samples : int , multiplier : np . ndarray = None ): \"\"\"Learn the idf vector (global term weights). Arguments: X: A matrix of term/token counts. n_samples: Number of total documents \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) avg_nr_samples = int ( X . sum ( axis = 1 ) . mean ()) idf = np . log ( avg_nr_samples / df ) if multiplier is not None : idf = idf * multiplier self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self","title":"fit()"},{"location":"api/ctfidf.html#bertopic._ctfidf.ClassTFIDF.transform","text":"Transform a count-based matrix to c-TF-IDF Parameters: Name Type Description Default X csr_matrix A matrix of term/token counts. required Returns: Type Description X (sparse matrix) A c-TF-IDF matrix Source code in bertopic\\_ctfidf.py def transform ( self , X : sp . csr_matrix , copy = True ): \"\"\"Transform a count-based matrix to c-TF-IDF Arguments: X (sparse matrix): A matrix of term/token counts. Returns: X (sparse matrix): A c-TF-IDF matrix \"\"\" if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) X = X * self . _idf_diag return X","title":"transform()"},{"location":"api/mmr.html","text":"Maximal Marginal Relevance \u00b6 Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Parameters: Name Type Description Default doc_embedding ndarray The document embeddings required word_embeddings ndarray The embeddings of the selected candidate keywords/phrases required words List[str] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return 5 diversity float How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. 0.8 Returns: Type Description List[str] List[str]: The selected keywords/keyphrases Source code in bertopic\\_mmr.py def mmr ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int = 5 , diversity : float = 0.8 ) -> List [ str ]: \"\"\" Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return diversity: How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. Returns: List[str]: The selected keywords/keyphrases \"\"\" # Extract similarity within words, and between words and the document word_doc_similarity = cosine_similarity ( word_embeddings , doc_embedding ) word_similarity = cosine_similarity ( word_embeddings ) # Initialize candidates and already choose best keyword/keyphras keywords_idx = [ np . argmax ( word_doc_similarity )] candidates_idx = [ i for i in range ( len ( words )) if i != keywords_idx [ 0 ]] for _ in range ( top_n - 1 ): # Extract similarities within candidates and # between candidates and selected keywords/phrases candidate_similarities = word_doc_similarity [ candidates_idx , :] target_similarities = np . max ( word_similarity [ candidates_idx ][:, keywords_idx ], axis = 1 ) # Calculate MMR mmr = ( 1 - diversity ) * candidate_similarities - diversity * target_similarities . reshape ( - 1 , 1 ) mmr_idx = candidates_idx [ np . argmax ( mmr )] # Update keywords & candidates keywords_idx . append ( mmr_idx ) candidates_idx . remove ( mmr_idx ) return [ words [ idx ] for idx in keywords_idx ]","title":"MMR"},{"location":"api/mmr.html#maximal-marginal-relevance","text":"Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Parameters: Name Type Description Default doc_embedding ndarray The document embeddings required word_embeddings ndarray The embeddings of the selected candidate keywords/phrases required words List[str] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return 5 diversity float How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. 0.8 Returns: Type Description List[str] List[str]: The selected keywords/keyphrases Source code in bertopic\\_mmr.py def mmr ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int = 5 , diversity : float = 0.8 ) -> List [ str ]: \"\"\" Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return diversity: How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. Returns: List[str]: The selected keywords/keyphrases \"\"\" # Extract similarity within words, and between words and the document word_doc_similarity = cosine_similarity ( word_embeddings , doc_embedding ) word_similarity = cosine_similarity ( word_embeddings ) # Initialize candidates and already choose best keyword/keyphras keywords_idx = [ np . argmax ( word_doc_similarity )] candidates_idx = [ i for i in range ( len ( words )) if i != keywords_idx [ 0 ]] for _ in range ( top_n - 1 ): # Extract similarities within candidates and # between candidates and selected keywords/phrases candidate_similarities = word_doc_similarity [ candidates_idx , :] target_similarities = np . max ( word_similarity [ candidates_idx ][:, keywords_idx ], axis = 1 ) # Calculate MMR mmr = ( 1 - diversity ) * candidate_similarities - diversity * target_similarities . reshape ( - 1 , 1 ) mmr_idx = candidates_idx [ np . argmax ( mmr )] # Update keywords & candidates keywords_idx . append ( mmr_idx ) candidates_idx . remove ( mmr_idx ) return [ words [ idx ] for idx in keywords_idx ]","title":"Maximal Marginal Relevance"},{"location":"api/backends/base.html","text":"BaseEmbedder \u00b6 The Base Embedder used for creating embedding models Parameters: Name Type Description Default embedding_model The main embedding model to be used for extracting document and word embedding required word_embedding_model The embedding model used for extracting word embeddings only. If this model is selected, then the embedding_model is purely used for creating document embeddings. required embed ( self , documents , verbose = False ) \u00b6 Embed a list of n documents/words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default documents List[str] A list of documents or words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description ndarray Document/words embeddings with shape (n, m) with n documents/words that each have an embeddings size of m Source code in bertopic\\backend\\_base.py def embed ( self , documents : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n documents/words into an n-dimensional matrix of embeddings Arguments: documents: A list of documents or words to be embedded verbose: Controls the verbosity of the process Returns: Document/words embeddings with shape (n, m) with `n` documents/words that each have an embeddings size of `m` \"\"\" pass embed_documents ( self , document , verbose = False ) \u00b6 Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default document List[str] A list of documents to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description ndarray Document embeddings with shape (n, m) with n documents that each have an embeddings size of m Source code in bertopic\\backend\\_base.py def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embed ( document , verbose ) embed_words ( self , words , verbose = False ) \u00b6 Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default words List[str] A list of words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description ndarray Word embeddings with shape (n, m) with n words that each have an embeddings size of m Source code in bertopic\\backend\\_base.py def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . embed ( words , verbose )","title":"Base"},{"location":"api/backends/base.html#baseembedder","text":"The Base Embedder used for creating embedding models Parameters: Name Type Description Default embedding_model The main embedding model to be used for extracting document and word embedding required word_embedding_model The embedding model used for extracting word embeddings only. If this model is selected, then the embedding_model is purely used for creating document embeddings. required","title":"BaseEmbedder"},{"location":"api/backends/base.html#bertopic.backend._base.BaseEmbedder.embed","text":"Embed a list of n documents/words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default documents List[str] A list of documents or words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description ndarray Document/words embeddings with shape (n, m) with n documents/words that each have an embeddings size of m Source code in bertopic\\backend\\_base.py def embed ( self , documents : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n documents/words into an n-dimensional matrix of embeddings Arguments: documents: A list of documents or words to be embedded verbose: Controls the verbosity of the process Returns: Document/words embeddings with shape (n, m) with `n` documents/words that each have an embeddings size of `m` \"\"\" pass","title":"embed()"},{"location":"api/backends/base.html#bertopic.backend._base.BaseEmbedder.embed_documents","text":"Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default document List[str] A list of documents to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description ndarray Document embeddings with shape (n, m) with n documents that each have an embeddings size of m Source code in bertopic\\backend\\_base.py def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embed ( document , verbose )","title":"embed_documents()"},{"location":"api/backends/base.html#bertopic.backend._base.BaseEmbedder.embed_words","text":"Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default words List[str] A list of words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description ndarray Word embeddings with shape (n, m) with n words that each have an embeddings size of m Source code in bertopic\\backend\\_base.py def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . embed ( words , verbose )","title":"embed_words()"},{"location":"api/backends/word_doc.html","text":"WordDocEmbedder \u00b6 Combine a document- and word-level embedder embed_documents ( self , document , verbose = False ) \u00b6 Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default document List[str] A list of documents to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description ndarray Document embeddings with shape (n, m) with n documents that each have an embeddings size of m Source code in bertopic\\backend\\_word_doc.py def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embedding_model . embed ( document , verbose ) embed_words ( self , words , verbose = False ) \u00b6 Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default words List[str] A list of words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description ndarray Word embeddings with shape (n, m) with n words that each have an embeddings size of m Source code in bertopic\\backend\\_word_doc.py def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . word_embedding_model . embed ( words , verbose )","title":"Word Doc"},{"location":"api/backends/word_doc.html#worddocembedder","text":"Combine a document- and word-level embedder","title":"WordDocEmbedder"},{"location":"api/backends/word_doc.html#bertopic.backend._word_doc.WordDocEmbedder.embed_documents","text":"Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default document List[str] A list of documents to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description ndarray Document embeddings with shape (n, m) with n documents that each have an embeddings size of m Source code in bertopic\\backend\\_word_doc.py def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embedding_model . embed ( document , verbose )","title":"embed_documents()"},{"location":"api/backends/word_doc.html#bertopic.backend._word_doc.WordDocEmbedder.embed_words","text":"Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default words List[str] A list of words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description ndarray Word embeddings with shape (n, m) with n words that each have an embeddings size of m Source code in bertopic\\backend\\_word_doc.py def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . word_embedding_model . embed ( words , verbose )","title":"embed_words()"},{"location":"api/plotting/barchart.html","text":"Barchart \u00b6 Visualize a barchart of selected topics Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List[int] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. 6 n_words int Number of words to show in a topic 5 width int The width of the figure. 800 height int The height of the figure. 600 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the barchart of selected topics simply run: topic_model . visualize_barchart () Or if you want to save the resulting figure: fig = topic_model . visualize_barchart () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_barchart.py def visualize_barchart ( topic_model , topics : List [ int ] = None , top_n_topics : int = 6 , n_words : int = 5 , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize a barchart of selected topics Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_words: Number of words to show in a topic width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the barchart of selected topics simply run: ```python topic_model.visualize_barchart() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_barchart() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/bar_chart.html\" style=\"width:1100px; height: 660px; border: 0px;\"\"></iframe> \"\"\" # Select topics based on top_n and topics args if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = topic_model . get_topic_freq () . Topic . to_list ()[ 1 : top_n_topics + 1 ] else : topics = topic_model . get_topic_freq () . Topic . to_list ()[ 1 : 7 ] # Initialize figure subplot_titles = [ f \"Topic { topic } \" for topic in topics ] columns = 3 rows = int ( np . ceil ( len ( topics ) / columns )) fig = make_subplots ( rows = rows , cols = columns , shared_xaxes = True , horizontal_spacing =. 15 , vertical_spacing =. 15 , subplot_titles = subplot_titles ) # Add barchart for each topic row = 1 column = 1 for topic in topics : words = [ word + \" \" for word , _ in topic_model . get_topic ( topic )][: n_words ][:: - 1 ] scores = [ score for _ , score in topic_model . get_topic ( topic )][: n_words ][:: - 1 ] fig . add_trace ( go . Bar ( x = scores , y = words , orientation = 'h' ), row = row , col = column ) if column == columns : column = 1 row += 1 else : column += 1 # Stylize graph fig . update_layout ( template = \"plotly_white\" , showlegend = False , title = { 'text' : \"<b>Topic Word Scores\" , 'y' : . 95 , 'x' : . 15 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) return fig","title":"Barchart"},{"location":"api/plotting/barchart.html#barchart","text":"Visualize a barchart of selected topics Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List[int] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. 6 n_words int Number of words to show in a topic 5 width int The width of the figure. 800 height int The height of the figure. 600 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the barchart of selected topics simply run: topic_model . visualize_barchart () Or if you want to save the resulting figure: fig = topic_model . visualize_barchart () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_barchart.py def visualize_barchart ( topic_model , topics : List [ int ] = None , top_n_topics : int = 6 , n_words : int = 5 , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize a barchart of selected topics Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_words: Number of words to show in a topic width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the barchart of selected topics simply run: ```python topic_model.visualize_barchart() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_barchart() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/bar_chart.html\" style=\"width:1100px; height: 660px; border: 0px;\"\"></iframe> \"\"\" # Select topics based on top_n and topics args if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = topic_model . get_topic_freq () . Topic . to_list ()[ 1 : top_n_topics + 1 ] else : topics = topic_model . get_topic_freq () . Topic . to_list ()[ 1 : 7 ] # Initialize figure subplot_titles = [ f \"Topic { topic } \" for topic in topics ] columns = 3 rows = int ( np . ceil ( len ( topics ) / columns )) fig = make_subplots ( rows = rows , cols = columns , shared_xaxes = True , horizontal_spacing =. 15 , vertical_spacing =. 15 , subplot_titles = subplot_titles ) # Add barchart for each topic row = 1 column = 1 for topic in topics : words = [ word + \" \" for word , _ in topic_model . get_topic ( topic )][: n_words ][:: - 1 ] scores = [ score for _ , score in topic_model . get_topic ( topic )][: n_words ][:: - 1 ] fig . add_trace ( go . Bar ( x = scores , y = words , orientation = 'h' ), row = row , col = column ) if column == columns : column = 1 row += 1 else : column += 1 # Stylize graph fig . update_layout ( template = \"plotly_white\" , showlegend = False , title = { 'text' : \"<b>Topic Word Scores\" , 'y' : . 95 , 'x' : . 15 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) return fig","title":"Barchart"},{"location":"api/plotting/distribution.html","text":"Distribution \u00b6 Visualize the distribution of topic probabilities Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required probabilities ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 width int The width of the figure. 800 height int The height of the figure. 600 Usage: Make sure to fit the model before and only input the probabilities of a single document: topic_model . visualize_distribution ( probabilities [ 0 ]) Or if you want to save the resulting figure: fig = topic_model . visualize_distribution ( probabilities [ 0 ]) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_distribution.py def visualize_distribution ( topic_model , probabilities : np . ndarray , min_probability : float = 0.015 , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize the distribution of topic probabilities Arguments: topic_model: A fitted BERTopic instance. probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. width: The width of the figure. height: The height of the figure. Usage: Make sure to fit the model before and only input the probabilities of a single document: ```python topic_model.visualize_distribution(probabilities[0]) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_distribution(probabilities[0]) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/probabilities.html\" style=\"width:1000px; height: 500px; border: 0px;\"\"></iframe> \"\"\" if len ( probabilities . shape ) != 2 : raise ValueError ( \"This visualization cannot be used if you have set `calculate_probabilities` to False \" \"as it uses the topic probabilities of all topics. \" ) if len ( probabilities [ probabilities > min_probability ]) == 0 : raise ValueError ( \"There are no values where `min_probability` is higher than the \" \"probabilities that were supplied. Lower `min_probability` to prevent this error.\" ) if not topic_model . calculate_probabilities : raise ValueError ( \"This visualization cannot be used if you have set `calculate_probabilities` to False \" \"as it uses the topic probabilities. \" ) # Get values and indices equal or exceed the minimum probability labels_idx = np . argwhere ( probabilities >= min_probability ) . flatten () vals = probabilities [ labels_idx ] . tolist () # Create labels labels = [] for idx in labels_idx : words = topic_model . get_topic ( idx ) if words : label = [ word [ 0 ] for word in words [: 5 ]] label = f \"<b>Topic { idx } </b>: { '_' . join ( label ) } \" label = label [: 40 ] + \"...\" if len ( label ) > 40 else label labels . append ( label ) else : vals . remove ( probabilities [ idx ]) # Create Figure fig = go . Figure ( go . Bar ( x = vals , y = labels , marker = dict ( color = '#C8D2D7' , line = dict ( color = '#6E8484' , width = 1 ), ), orientation = 'h' ) ) fig . update_layout ( xaxis_title = \"Probability\" , title = { 'text' : \"<b>Topic Probability Distribution\" , 'y' : . 95 , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) return fig","title":"Distribution"},{"location":"api/plotting/distribution.html#distribution","text":"Visualize the distribution of topic probabilities Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required probabilities ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 width int The width of the figure. 800 height int The height of the figure. 600 Usage: Make sure to fit the model before and only input the probabilities of a single document: topic_model . visualize_distribution ( probabilities [ 0 ]) Or if you want to save the resulting figure: fig = topic_model . visualize_distribution ( probabilities [ 0 ]) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_distribution.py def visualize_distribution ( topic_model , probabilities : np . ndarray , min_probability : float = 0.015 , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize the distribution of topic probabilities Arguments: topic_model: A fitted BERTopic instance. probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. width: The width of the figure. height: The height of the figure. Usage: Make sure to fit the model before and only input the probabilities of a single document: ```python topic_model.visualize_distribution(probabilities[0]) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_distribution(probabilities[0]) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/probabilities.html\" style=\"width:1000px; height: 500px; border: 0px;\"\"></iframe> \"\"\" if len ( probabilities . shape ) != 2 : raise ValueError ( \"This visualization cannot be used if you have set `calculate_probabilities` to False \" \"as it uses the topic probabilities of all topics. \" ) if len ( probabilities [ probabilities > min_probability ]) == 0 : raise ValueError ( \"There are no values where `min_probability` is higher than the \" \"probabilities that were supplied. Lower `min_probability` to prevent this error.\" ) if not topic_model . calculate_probabilities : raise ValueError ( \"This visualization cannot be used if you have set `calculate_probabilities` to False \" \"as it uses the topic probabilities. \" ) # Get values and indices equal or exceed the minimum probability labels_idx = np . argwhere ( probabilities >= min_probability ) . flatten () vals = probabilities [ labels_idx ] . tolist () # Create labels labels = [] for idx in labels_idx : words = topic_model . get_topic ( idx ) if words : label = [ word [ 0 ] for word in words [: 5 ]] label = f \"<b>Topic { idx } </b>: { '_' . join ( label ) } \" label = label [: 40 ] + \"...\" if len ( label ) > 40 else label labels . append ( label ) else : vals . remove ( probabilities [ idx ]) # Create Figure fig = go . Figure ( go . Bar ( x = vals , y = labels , marker = dict ( color = '#C8D2D7' , line = dict ( color = '#6E8484' , width = 1 ), ), orientation = 'h' ) ) fig . update_layout ( xaxis_title = \"Probability\" , title = { 'text' : \"<b>Topic Probability Distribution\" , 'y' : . 95 , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) return fig","title":"Distribution"},{"location":"api/plotting/dtm.html","text":"DTM \u00b6 Visualize topics over time Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics_over_time DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all None topics List[int] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False width int The width of the figure. 1250 height int The height of the figure. 450 Returns: Type Description Figure A plotly.graph_objects.Figure including all traces Usage: To visualize the topics over time, simply run: topics_over_time = topic_model . topics_over_time ( docs , topics , timestamps ) topic_model . visualize_topics_over_time ( topics_over_time ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_over_time ( topics_over_time ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics_over_time.py def visualize_topics_over_time ( topic_model , topics_over_time : pd . DataFrame , top_n_topics : int = None , topics : List [ int ] = None , normalize_frequency : bool = False , width : int = 1250 , height : int = 450 ) -> go . Figure : \"\"\" Visualize topics over time Arguments: topic_model: A fitted BERTopic instance. topics_over_time: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Usage: To visualize the topics over time, simply run: ```python topics_over_time = topic_model.topics_over_time(docs, topics, timestamps) topic_model.visualize_topics_over_time(topics_over_time) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_over_time(topics_over_time) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/trump.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" colors = [ \"#E69F00\" , \"#56B4E9\" , \"#009E73\" , \"#F0E442\" , \"#D55E00\" , \"#0072B2\" , \"#CC79A7\" ] # Select topics if topics : selected_topics = topics elif top_n_topics : selected_topics = topic_model . get_topic_freq () . head ( top_n_topics + 1 )[ 1 :] . Topic . values else : selected_topics = topic_model . get_topic_freq () . Topic . values # Prepare data topic_names = { key : value [: 40 ] + \"...\" if len ( value ) > 40 else value for key , value in topic_model . topic_names . items ()} topics_over_time [ \"Name\" ] = topics_over_time . Topic . map ( topic_names ) data = topics_over_time . loc [ topics_over_time . Topic . isin ( selected_topics ), :] # Add traces fig = go . Figure () for index , topic in enumerate ( data . Topic . unique ()): trace_data = data . loc [ data . Topic == topic , :] topic_name = trace_data . Name . values [ 0 ] words = trace_data . Words . values if normalize_frequency : y = normalize ( trace_data . Frequency . values . reshape ( 1 , - 1 ))[ 0 ] else : y = trace_data . Frequency fig . add_trace ( go . Scatter ( x = trace_data . Timestamp , y = y , mode = 'lines' , marker_color = colors [ index % 7 ], hoverinfo = \"text\" , name = topic_name , hovertext = [ f '<b>Topic { topic } </b><br>Words: { word } ' for word in words ])) # Styling of the visualization fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) fig . update_layout ( yaxis_title = \"Normalized Frequency\" if normalize_frequency else \"Frequency\" , title = { 'text' : \"<b>Topics over Time\" , 'y' : . 95 , 'x' : 0.40 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), legend = dict ( title = \"<b>Global Topic Representation\" , ) ) return fig","title":"DTM"},{"location":"api/plotting/dtm.html#dtm","text":"Visualize topics over time Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics_over_time DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all None topics List[int] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False width int The width of the figure. 1250 height int The height of the figure. 450 Returns: Type Description Figure A plotly.graph_objects.Figure including all traces Usage: To visualize the topics over time, simply run: topics_over_time = topic_model . topics_over_time ( docs , topics , timestamps ) topic_model . visualize_topics_over_time ( topics_over_time ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_over_time ( topics_over_time ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics_over_time.py def visualize_topics_over_time ( topic_model , topics_over_time : pd . DataFrame , top_n_topics : int = None , topics : List [ int ] = None , normalize_frequency : bool = False , width : int = 1250 , height : int = 450 ) -> go . Figure : \"\"\" Visualize topics over time Arguments: topic_model: A fitted BERTopic instance. topics_over_time: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Usage: To visualize the topics over time, simply run: ```python topics_over_time = topic_model.topics_over_time(docs, topics, timestamps) topic_model.visualize_topics_over_time(topics_over_time) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_over_time(topics_over_time) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/trump.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" colors = [ \"#E69F00\" , \"#56B4E9\" , \"#009E73\" , \"#F0E442\" , \"#D55E00\" , \"#0072B2\" , \"#CC79A7\" ] # Select topics if topics : selected_topics = topics elif top_n_topics : selected_topics = topic_model . get_topic_freq () . head ( top_n_topics + 1 )[ 1 :] . Topic . values else : selected_topics = topic_model . get_topic_freq () . Topic . values # Prepare data topic_names = { key : value [: 40 ] + \"...\" if len ( value ) > 40 else value for key , value in topic_model . topic_names . items ()} topics_over_time [ \"Name\" ] = topics_over_time . Topic . map ( topic_names ) data = topics_over_time . loc [ topics_over_time . Topic . isin ( selected_topics ), :] # Add traces fig = go . Figure () for index , topic in enumerate ( data . Topic . unique ()): trace_data = data . loc [ data . Topic == topic , :] topic_name = trace_data . Name . values [ 0 ] words = trace_data . Words . values if normalize_frequency : y = normalize ( trace_data . Frequency . values . reshape ( 1 , - 1 ))[ 0 ] else : y = trace_data . Frequency fig . add_trace ( go . Scatter ( x = trace_data . Timestamp , y = y , mode = 'lines' , marker_color = colors [ index % 7 ], hoverinfo = \"text\" , name = topic_name , hovertext = [ f '<b>Topic { topic } </b><br>Words: { word } ' for word in words ])) # Styling of the visualization fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) fig . update_layout ( yaxis_title = \"Normalized Frequency\" if normalize_frequency else \"Frequency\" , title = { 'text' : \"<b>Topics over Time\" , 'y' : . 95 , 'x' : 0.40 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), legend = dict ( title = \"<b>Global Topic Representation\" , ) ) return fig","title":"DTM"},{"location":"api/plotting/heatmap.html","text":"Heatmap \u00b6 Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List[int] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. None n_clusters int Create n clusters and order the similarity matrix by those clusters. None width int The width of the figure. 800 height int The height of the figure. 800 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the similarity matrix of topics simply run: topic_model . visualize_heatmap () Or if you want to save the resulting figure: fig = topic_model . visualize_heatmap () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_heatmap.py def visualize_heatmap ( topic_model , topics : List [ int ] = None , top_n_topics : int = None , n_clusters : int = None , width : int = 800 , height : int = 800 ) -> go . Figure : \"\"\" Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_clusters: Create n clusters and order the similarity matrix by those clusters. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the similarity matrix of topics simply run: ```python topic_model.visualize_heatmap() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_heatmap() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/heatmap.html\" style=\"width:1000px; height: 720px; border: 0px;\"\"></iframe> \"\"\" # Select topic embeddings if topic_model . topic_embeddings is not None : embeddings = np . array ( topic_model . topic_embeddings ) else : embeddings = topic_model . c_tf_idf # Select topics based on top_n and topics args if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( topic_model . get_topic_freq () . Topic . to_list ()[ 1 : top_n_topics + 1 ]) else : topics = sorted ( list ( topic_model . get_topics () . keys ())) # Order heatmap by similar clusters of topics if n_clusters : if n_clusters >= len ( set ( topics )): raise ValueError ( \"Make sure to set `n_clusters` lower than \" \"the total number of unique topics.\" ) embeddings = embeddings [[ topic + 1 for topic in topics ]] distance_matrix = cosine_similarity ( embeddings ) Z = linkage ( distance_matrix , 'ward' ) clusters = fcluster ( Z , t = n_clusters , criterion = 'maxclust' ) # Extract new order of topics mapping = { cluster : [] for cluster in clusters } for topic , cluster in zip ( topics , clusters ): mapping [ cluster ] . append ( topic ) mapping = [ cluster for cluster in mapping . values ()] sorted_topics = [ topic for cluster in mapping for topic in cluster ] else : sorted_topics = topics # Select embeddings indices = np . array ([ topics . index ( topic ) for topic in sorted_topics ]) embeddings = embeddings [ indices ] distance_matrix = cosine_similarity ( embeddings ) # Create nicer labels new_labels = [[[ str ( topic ), None ]] + topic_model . get_topic ( topic ) for topic in sorted_topics ] new_labels = [ \"_\" . join ([ label [ 0 ] for label in labels [: 4 ]]) for labels in new_labels ] new_labels = [ label if len ( label ) < 30 else label [: 27 ] + \"...\" for label in new_labels ] fig = px . imshow ( distance_matrix , labels = dict ( color = \"Similarity Score\" ), x = new_labels , y = new_labels , color_continuous_scale = 'GnBu' ) fig . update_layout ( title = { 'text' : \"<b>Similarity Matrix\" , 'y' : . 95 , 'x' : 0.55 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_layout ( showlegend = True ) fig . update_layout ( legend_title_text = 'Trend' ) return fig","title":"Heatmap"},{"location":"api/plotting/heatmap.html#heatmap","text":"Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List[int] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. None n_clusters int Create n clusters and order the similarity matrix by those clusters. None width int The width of the figure. 800 height int The height of the figure. 800 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the similarity matrix of topics simply run: topic_model . visualize_heatmap () Or if you want to save the resulting figure: fig = topic_model . visualize_heatmap () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_heatmap.py def visualize_heatmap ( topic_model , topics : List [ int ] = None , top_n_topics : int = None , n_clusters : int = None , width : int = 800 , height : int = 800 ) -> go . Figure : \"\"\" Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_clusters: Create n clusters and order the similarity matrix by those clusters. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the similarity matrix of topics simply run: ```python topic_model.visualize_heatmap() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_heatmap() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/heatmap.html\" style=\"width:1000px; height: 720px; border: 0px;\"\"></iframe> \"\"\" # Select topic embeddings if topic_model . topic_embeddings is not None : embeddings = np . array ( topic_model . topic_embeddings ) else : embeddings = topic_model . c_tf_idf # Select topics based on top_n and topics args if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( topic_model . get_topic_freq () . Topic . to_list ()[ 1 : top_n_topics + 1 ]) else : topics = sorted ( list ( topic_model . get_topics () . keys ())) # Order heatmap by similar clusters of topics if n_clusters : if n_clusters >= len ( set ( topics )): raise ValueError ( \"Make sure to set `n_clusters` lower than \" \"the total number of unique topics.\" ) embeddings = embeddings [[ topic + 1 for topic in topics ]] distance_matrix = cosine_similarity ( embeddings ) Z = linkage ( distance_matrix , 'ward' ) clusters = fcluster ( Z , t = n_clusters , criterion = 'maxclust' ) # Extract new order of topics mapping = { cluster : [] for cluster in clusters } for topic , cluster in zip ( topics , clusters ): mapping [ cluster ] . append ( topic ) mapping = [ cluster for cluster in mapping . values ()] sorted_topics = [ topic for cluster in mapping for topic in cluster ] else : sorted_topics = topics # Select embeddings indices = np . array ([ topics . index ( topic ) for topic in sorted_topics ]) embeddings = embeddings [ indices ] distance_matrix = cosine_similarity ( embeddings ) # Create nicer labels new_labels = [[[ str ( topic ), None ]] + topic_model . get_topic ( topic ) for topic in sorted_topics ] new_labels = [ \"_\" . join ([ label [ 0 ] for label in labels [: 4 ]]) for labels in new_labels ] new_labels = [ label if len ( label ) < 30 else label [: 27 ] + \"...\" for label in new_labels ] fig = px . imshow ( distance_matrix , labels = dict ( color = \"Similarity Score\" ), x = new_labels , y = new_labels , color_continuous_scale = 'GnBu' ) fig . update_layout ( title = { 'text' : \"<b>Similarity Matrix\" , 'y' : . 95 , 'x' : 0.55 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_layout ( showlegend = True ) fig . update_layout ( legend_title_text = 'Trend' ) return fig","title":"Heatmap"},{"location":"api/plotting/hierarchy.html","text":"Hierarchy \u00b6 Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required orientation str The orientation of the figure. Either 'left' or 'bottom' 'left' topics List[int] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 1000 height int The height of the figure. 600 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the hierarchical structure of topics simply run: topic_model . visualize_hierarchy () Or if you want to save the resulting figure: fig = topic_model . visualize_hierarchy () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_hierarchy.py def visualize_hierarchy ( topic_model , orientation : str = \"left\" , topics : List [ int ] = None , top_n_topics : int = None , width : int = 1000 , height : int = 600 ) -> go . Figure : \"\"\" Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Arguments: topic_model: A fitted BERTopic instance. orientation: The orientation of the figure. Either 'left' or 'bottom' topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the hierarchical structure of topics simply run: ```python topic_model.visualize_hierarchy() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchy() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/hierarchy.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" # Select topic embeddings if topic_model . topic_embeddings is not None : embeddings = np . array ( topic_model . topic_embeddings ) else : embeddings = topic_model . c_tf_idf # Select topics based on top_n and topics args if topics is not None : topics = sorted ( list ( topics )) elif top_n_topics is not None : topics = sorted ( topic_model . get_topic_freq () . Topic . to_list ()[ 1 : top_n_topics + 1 ]) else : topics = sorted ( list ( topic_model . get_topics () . keys ())) # Select embeddings all_topics = sorted ( list ( topic_model . get_topics () . keys ())) indices = np . array ([ all_topics . index ( topic ) for topic in topics ]) embeddings = embeddings [ indices ] # Create dendogram distance_matrix = 1 - cosine_similarity ( embeddings ) fig = ff . create_dendrogram ( distance_matrix , orientation = orientation , linkagefun = lambda x : linkage ( x , \"ward\" ), color_threshold = 1 ) # Create nicer labels axis = \"yaxis\" if orientation == \"left\" else \"xaxis\" new_labels = [[[ str ( topics [ int ( x )]), None ]] + topic_model . get_topic ( topics [ int ( x )]) for x in fig . layout [ axis ][ \"ticktext\" ]] new_labels = [ \"_\" . join ([ label [ 0 ] for label in labels [: 4 ]]) for labels in new_labels ] new_labels = [ label if len ( label ) < 30 else label [: 27 ] + \"...\" for label in new_labels ] # Stylize layout fig . update_layout ( plot_bgcolor = '#ECEFF1' , template = \"plotly_white\" , title = { 'text' : \"<b>Hierarchical Clustering\" , 'y' : . 95 , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) # Stylize orientation if orientation == \"left\" : fig . update_layout ( yaxis = dict ( tickmode = \"array\" , ticktext = new_labels )) else : fig . update_layout ( xaxis = dict ( tickmode = \"array\" , ticktext = new_labels )) return fig","title":"Hierarchy"},{"location":"api/plotting/hierarchy.html#hierarchy","text":"Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required orientation str The orientation of the figure. Either 'left' or 'bottom' 'left' topics List[int] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 1000 height int The height of the figure. 600 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the hierarchical structure of topics simply run: topic_model . visualize_hierarchy () Or if you want to save the resulting figure: fig = topic_model . visualize_hierarchy () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_hierarchy.py def visualize_hierarchy ( topic_model , orientation : str = \"left\" , topics : List [ int ] = None , top_n_topics : int = None , width : int = 1000 , height : int = 600 ) -> go . Figure : \"\"\" Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Arguments: topic_model: A fitted BERTopic instance. orientation: The orientation of the figure. Either 'left' or 'bottom' topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the hierarchical structure of topics simply run: ```python topic_model.visualize_hierarchy() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchy() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/hierarchy.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" # Select topic embeddings if topic_model . topic_embeddings is not None : embeddings = np . array ( topic_model . topic_embeddings ) else : embeddings = topic_model . c_tf_idf # Select topics based on top_n and topics args if topics is not None : topics = sorted ( list ( topics )) elif top_n_topics is not None : topics = sorted ( topic_model . get_topic_freq () . Topic . to_list ()[ 1 : top_n_topics + 1 ]) else : topics = sorted ( list ( topic_model . get_topics () . keys ())) # Select embeddings all_topics = sorted ( list ( topic_model . get_topics () . keys ())) indices = np . array ([ all_topics . index ( topic ) for topic in topics ]) embeddings = embeddings [ indices ] # Create dendogram distance_matrix = 1 - cosine_similarity ( embeddings ) fig = ff . create_dendrogram ( distance_matrix , orientation = orientation , linkagefun = lambda x : linkage ( x , \"ward\" ), color_threshold = 1 ) # Create nicer labels axis = \"yaxis\" if orientation == \"left\" else \"xaxis\" new_labels = [[[ str ( topics [ int ( x )]), None ]] + topic_model . get_topic ( topics [ int ( x )]) for x in fig . layout [ axis ][ \"ticktext\" ]] new_labels = [ \"_\" . join ([ label [ 0 ] for label in labels [: 4 ]]) for labels in new_labels ] new_labels = [ label if len ( label ) < 30 else label [: 27 ] + \"...\" for label in new_labels ] # Stylize layout fig . update_layout ( plot_bgcolor = '#ECEFF1' , template = \"plotly_white\" , title = { 'text' : \"<b>Hierarchical Clustering\" , 'y' : . 95 , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) # Stylize orientation if orientation == \"left\" : fig . update_layout ( yaxis = dict ( tickmode = \"array\" , ticktext = new_labels )) else : fig . update_layout ( xaxis = dict ( tickmode = \"array\" , ticktext = new_labels )) return fig","title":"Hierarchy"},{"location":"api/plotting/term.html","text":"Term Score Decline \u00b6 Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List[int] A selection of topics to visualize. These will be colored red where all others will be colored black. None log_scale bool Whether to represent the ranking on a log scale False width int The width of the figure. 800 height int The height of the figure. 500 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the ranks of all words across all topics simply run: topic_model . visualize_word_rank () Or if you want to save the resulting figure: fig = topic_model . visualize_word_rank () fig . write_html ( \"path/to/file.html\" ) Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here . Source code in bertopic\\plotting\\_term_rank.py def visualize_term_rank ( topic_model , topics : List [ int ] = None , log_scale : bool = False , width : int = 800 , height : int = 500 ) -> go . Figure : \"\"\" Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. These will be colored red where all others will be colored black. log_scale: Whether to represent the ranking on a log scale width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the ranks of all words across all topics simply run: ```python topic_model.visualize_word_rank() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_word_rank() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/term_rank.html\" style=\"width:1000px; height: 530px; border: 0px;\"\"></iframe> <iframe src=\"../../tutorial/visualization/term_rank_log.html\" style=\"width:1000px; height: 530px; border: 0px;\"\"></iframe> Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/). Reference to that specific analysis can be found [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html). \"\"\" topics = [] if topics is None else topics topic_ids = topic_model . get_topic_info () . Topic . unique () . tolist () topic_words = [ topic_model . get_topic ( topic ) for topic in topic_ids ] values = np . array ([[ value [ 1 ] for value in values ] for values in topic_words ]) indices = np . array ([[ value + 1 for value in range ( len ( values ))] for values in topic_words ]) # Create figure lines = [] for topic , x , y in zip ( topic_ids , indices , values ): if not any ( y > 1.5 ): # labels label = f \"<b>Topic { topic } </b>:\" + \"_\" . join ([ word [ 0 ] for word in topic_model . get_topic ( topic )]) label = label [: 50 ] # line parameters color = \"red\" if topic in topics else \"black\" opacity = 1 if topic in topics else . 1 if any ( y == 0 ): y [ y == 0 ] = min ( values [ values > 0 ]) y = np . log10 ( y , out = y , where = y > 0 ) if log_scale else y line = go . Scatter ( x = x , y = y , name = \"\" , hovertext = label , mode = \"lines+lines\" , opacity = opacity , line = dict ( color = color , width = 1.5 )) lines . append ( line ) fig = go . Figure ( data = lines ) # Stylize layout fig . update_xaxes ( range = [ 0 , len ( indices [ 0 ])], tick0 = 1 , dtick = 2 ) fig . update_layout ( showlegend = False , template = \"plotly_white\" , title = { 'text' : \"<b>Term score decline per Topic</b>\" , 'y' : . 9 , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_xaxes ( title_text = 'Term Rank' ) if log_scale : fig . update_yaxes ( title_text = 'c-TF-IDF score (log scale)' ) else : fig . update_yaxes ( title_text = 'c-TF-IDF score' ) return fig","title":"Term Scores"},{"location":"api/plotting/term.html#term-score-decline","text":"Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List[int] A selection of topics to visualize. These will be colored red where all others will be colored black. None log_scale bool Whether to represent the ranking on a log scale False width int The width of the figure. 800 height int The height of the figure. 500 Returns: Type Description Figure fig: A plotly figure Usage: To visualize the ranks of all words across all topics simply run: topic_model . visualize_word_rank () Or if you want to save the resulting figure: fig = topic_model . visualize_word_rank () fig . write_html ( \"path/to/file.html\" ) Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here . Source code in bertopic\\plotting\\_term_rank.py def visualize_term_rank ( topic_model , topics : List [ int ] = None , log_scale : bool = False , width : int = 800 , height : int = 500 ) -> go . Figure : \"\"\" Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. These will be colored red where all others will be colored black. log_scale: Whether to represent the ranking on a log scale width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Usage: To visualize the ranks of all words across all topics simply run: ```python topic_model.visualize_word_rank() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_word_rank() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/term_rank.html\" style=\"width:1000px; height: 530px; border: 0px;\"\"></iframe> <iframe src=\"../../tutorial/visualization/term_rank_log.html\" style=\"width:1000px; height: 530px; border: 0px;\"\"></iframe> Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/). Reference to that specific analysis can be found [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html). \"\"\" topics = [] if topics is None else topics topic_ids = topic_model . get_topic_info () . Topic . unique () . tolist () topic_words = [ topic_model . get_topic ( topic ) for topic in topic_ids ] values = np . array ([[ value [ 1 ] for value in values ] for values in topic_words ]) indices = np . array ([[ value + 1 for value in range ( len ( values ))] for values in topic_words ]) # Create figure lines = [] for topic , x , y in zip ( topic_ids , indices , values ): if not any ( y > 1.5 ): # labels label = f \"<b>Topic { topic } </b>:\" + \"_\" . join ([ word [ 0 ] for word in topic_model . get_topic ( topic )]) label = label [: 50 ] # line parameters color = \"red\" if topic in topics else \"black\" opacity = 1 if topic in topics else . 1 if any ( y == 0 ): y [ y == 0 ] = min ( values [ values > 0 ]) y = np . log10 ( y , out = y , where = y > 0 ) if log_scale else y line = go . Scatter ( x = x , y = y , name = \"\" , hovertext = label , mode = \"lines+lines\" , opacity = opacity , line = dict ( color = color , width = 1.5 )) lines . append ( line ) fig = go . Figure ( data = lines ) # Stylize layout fig . update_xaxes ( range = [ 0 , len ( indices [ 0 ])], tick0 = 1 , dtick = 2 ) fig . update_layout ( showlegend = False , template = \"plotly_white\" , title = { 'text' : \"<b>Term score decline per Topic</b>\" , 'y' : . 9 , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_xaxes ( title_text = 'Term Rank' ) if log_scale : fig . update_yaxes ( title_text = 'c-TF-IDF score (log scale)' ) else : fig . update_yaxes ( title_text = 'c-TF-IDF score' ) return fig","title":"Term Score Decline"},{"location":"api/plotting/topics.html","text":"Topics \u00b6 Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List[int] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 650 height int The height of the figure. 650 Usage: To visualize the topics simply run: topic_model . visualize_topics () Or if you want to save the resulting figure: fig = topic_model . visualize_topics () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics.py def visualize_topics ( topic_model , topics : List [ int ] = None , top_n_topics : int = None , width : int = 650 , height : int = 650 ) -> go . Figure : \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Usage: To visualize the topics simply run: ```python topic_model.visualize_topics() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/viz.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" # Select topics based on top_n and topics args if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( topic_model . get_topic_freq () . Topic . to_list ()[ 1 : top_n_topics + 1 ]) else : topics = sorted ( list ( topic_model . get_topics () . keys ())) # Extract topic words and their frequencies topic_list = sorted ( topics ) frequencies = [ topic_model . topic_sizes [ topic ] for topic in topic_list ] words = [ \" | \" . join ([ word [ 0 ] for word in topic_model . get_topic ( topic )[: 5 ]]) for topic in topic_list ] # Embed c-TF-IDF into 2D all_topics = sorted ( list ( topic_model . get_topics () . keys ())) indices = np . array ([ all_topics . index ( topic ) for topic in topics ]) embeddings = topic_model . c_tf_idf . toarray ()[ indices ] embeddings = MinMaxScaler () . fit_transform ( embeddings ) embeddings = UMAP ( n_neighbors = 2 , n_components = 2 , metric = 'hellinger' ) . fit_transform ( embeddings ) # Visualize with plotly df = pd . DataFrame ({ \"x\" : embeddings [ 1 :, 0 ], \"y\" : embeddings [ 1 :, 1 ], \"Topic\" : topic_list [ 1 :], \"Words\" : words [ 1 :], \"Size\" : frequencies [ 1 :]}) return _plotly_topic_visualization ( df , topic_list , width , height )","title":"Topics"},{"location":"api/plotting/topics.html#topics","text":"Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List[int] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 650 height int The height of the figure. 650 Usage: To visualize the topics simply run: topic_model . visualize_topics () Or if you want to save the resulting figure: fig = topic_model . visualize_topics () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics.py def visualize_topics ( topic_model , topics : List [ int ] = None , top_n_topics : int = None , width : int = 650 , height : int = 650 ) -> go . Figure : \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Usage: To visualize the topics simply run: ```python topic_model.visualize_topics() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/viz.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" # Select topics based on top_n and topics args if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( topic_model . get_topic_freq () . Topic . to_list ()[ 1 : top_n_topics + 1 ]) else : topics = sorted ( list ( topic_model . get_topics () . keys ())) # Extract topic words and their frequencies topic_list = sorted ( topics ) frequencies = [ topic_model . topic_sizes [ topic ] for topic in topic_list ] words = [ \" | \" . join ([ word [ 0 ] for word in topic_model . get_topic ( topic )[: 5 ]]) for topic in topic_list ] # Embed c-TF-IDF into 2D all_topics = sorted ( list ( topic_model . get_topics () . keys ())) indices = np . array ([ all_topics . index ( topic ) for topic in topics ]) embeddings = topic_model . c_tf_idf . toarray ()[ indices ] embeddings = MinMaxScaler () . fit_transform ( embeddings ) embeddings = UMAP ( n_neighbors = 2 , n_components = 2 , metric = 'hellinger' ) . fit_transform ( embeddings ) # Visualize with plotly df = pd . DataFrame ({ \"x\" : embeddings [ 1 :, 0 ], \"y\" : embeddings [ 1 :, 1 ], \"Topic\" : topic_list [ 1 :], \"Words\" : words [ 1 :], \"Size\" : frequencies [ 1 :]}) return _plotly_topic_visualization ( df , topic_list , width , height )","title":"Topics"},{"location":"api/plotting/topics_per_class.html","text":"Topics per Class \u00b6 Visualize topics per class Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics_per_class DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all 10 topics List[int] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False width int The width of the figure. 1250 height int The height of the figure. 900 Returns: Type Description Figure A plotly.graph_objects.Figure including all traces Usage: To visualize the topics per class, simply run: topics_per_class = topic_model . topics_per_class ( docs , topics , classes ) topic_model . visualize_topics_per_class ( topics_per_class ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_per_class ( topics_per_class ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics_per_class.py def visualize_topics_per_class ( topic_model , topics_per_class : pd . DataFrame , top_n_topics : int = 10 , topics : List [ int ] = None , normalize_frequency : bool = False , width : int = 1250 , height : int = 900 ) -> go . Figure : \"\"\" Visualize topics per class Arguments: topic_model: A fitted BERTopic instance. topics_per_class: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Usage: To visualize the topics per class, simply run: ```python topics_per_class = topic_model.topics_per_class(docs, topics, classes) topic_model.visualize_topics_per_class(topics_per_class) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_per_class(topics_per_class) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/topics_per_class.html\" style=\"width:1400px; height: 1000px; border: 0px;\"\"></iframe> \"\"\" colors = [ \"#E69F00\" , \"#56B4E9\" , \"#009E73\" , \"#F0E442\" , \"#D55E00\" , \"#0072B2\" , \"#CC79A7\" ] # Select topics if topics : selected_topics = topics elif top_n_topics : selected_topics = topic_model . get_topic_freq () . head ( top_n_topics + 1 )[ 1 :] . Topic . values else : selected_topics = topic_model . get_topic_freq () . Topic . values # Prepare data topic_names = { key : value [: 40 ] + \"...\" if len ( value ) > 40 else value for key , value in topic_model . topic_names . items ()} topics_per_class [ \"Name\" ] = topics_per_class . Topic . map ( topic_names ) data = topics_per_class . loc [ topics_per_class . Topic . isin ( selected_topics ), :] # Add traces fig = go . Figure () for index , topic in enumerate ( selected_topics ): if index == 0 : visible = True else : visible = \"legendonly\" trace_data = data . loc [ data . Topic == topic , :] topic_name = trace_data . Name . values [ 0 ] words = trace_data . Words . values if normalize_frequency : x = normalize ( trace_data . Frequency . values . reshape ( 1 , - 1 ))[ 0 ] else : x = trace_data . Frequency fig . add_trace ( go . Bar ( y = trace_data . Class , x = x , visible = visible , marker_color = colors [ index % 7 ], hoverinfo = \"text\" , name = topic_name , orientation = \"h\" , hovertext = [ f '<b>Topic { topic } </b><br>Words: { word } ' for word in words ])) # Styling of the visualization fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) fig . update_layout ( xaxis_title = \"Normalized Frequency\" if normalize_frequency else \"Frequency\" , yaxis_title = \"Class\" , title = { 'text' : \"<b>Topics per Class\" , 'y' : . 95 , 'x' : 0.40 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), legend = dict ( title = \"<b>Global Topic Representation\" , ) ) return fig","title":"Topics per Class"},{"location":"api/plotting/topics_per_class.html#topics-per-class","text":"Visualize topics per class Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics_per_class DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all 10 topics List[int] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False width int The width of the figure. 1250 height int The height of the figure. 900 Returns: Type Description Figure A plotly.graph_objects.Figure including all traces Usage: To visualize the topics per class, simply run: topics_per_class = topic_model . topics_per_class ( docs , topics , classes ) topic_model . visualize_topics_per_class ( topics_per_class ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_per_class ( topics_per_class ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics_per_class.py def visualize_topics_per_class ( topic_model , topics_per_class : pd . DataFrame , top_n_topics : int = 10 , topics : List [ int ] = None , normalize_frequency : bool = False , width : int = 1250 , height : int = 900 ) -> go . Figure : \"\"\" Visualize topics per class Arguments: topic_model: A fitted BERTopic instance. topics_per_class: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Usage: To visualize the topics per class, simply run: ```python topics_per_class = topic_model.topics_per_class(docs, topics, classes) topic_model.visualize_topics_per_class(topics_per_class) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_per_class(topics_per_class) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../tutorial/visualization/topics_per_class.html\" style=\"width:1400px; height: 1000px; border: 0px;\"\"></iframe> \"\"\" colors = [ \"#E69F00\" , \"#56B4E9\" , \"#009E73\" , \"#F0E442\" , \"#D55E00\" , \"#0072B2\" , \"#CC79A7\" ] # Select topics if topics : selected_topics = topics elif top_n_topics : selected_topics = topic_model . get_topic_freq () . head ( top_n_topics + 1 )[ 1 :] . Topic . values else : selected_topics = topic_model . get_topic_freq () . Topic . values # Prepare data topic_names = { key : value [: 40 ] + \"...\" if len ( value ) > 40 else value for key , value in topic_model . topic_names . items ()} topics_per_class [ \"Name\" ] = topics_per_class . Topic . map ( topic_names ) data = topics_per_class . loc [ topics_per_class . Topic . isin ( selected_topics ), :] # Add traces fig = go . Figure () for index , topic in enumerate ( selected_topics ): if index == 0 : visible = True else : visible = \"legendonly\" trace_data = data . loc [ data . Topic == topic , :] topic_name = trace_data . Name . values [ 0 ] words = trace_data . Words . values if normalize_frequency : x = normalize ( trace_data . Frequency . values . reshape ( 1 , - 1 ))[ 0 ] else : x = trace_data . Frequency fig . add_trace ( go . Bar ( y = trace_data . Class , x = x , visible = visible , marker_color = colors [ index % 7 ], hoverinfo = \"text\" , name = topic_name , orientation = \"h\" , hovertext = [ f '<b>Topic { topic } </b><br>Words: { word } ' for word in words ])) # Styling of the visualization fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) fig . update_layout ( xaxis_title = \"Normalized Frequency\" if normalize_frequency else \"Frequency\" , yaxis_title = \"Class\" , title = { 'text' : \"<b>Topics per Class\" , 'y' : . 95 , 'x' : 0.40 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), legend = dict ( title = \"<b>Global Topic Representation\" , ) ) return fig","title":"Topics per Class"},{"location":"tutorial/algorithm/algorithm.html","text":"The algorithm contains, roughly, 3 stages: Embed documents Extract document embeddings with BERT or any other embedding technique Cluster Documents UMAP to reduce the dimensionality of embeddings HDBSCAN to cluster reduced embeddings and create clusters of semantically similar documents Create topic representation Extract and reduce topics with c-TF-IDF Improve coherence of words with Maximal Marginal Relevance Embed documents \u00b6 We start by creating document embeddings from a set of documents using sentence-transformers . These models are pre-trained for many language and are great for creating either document- or sentence-embeddings. In BERTopic, you can choose any sentence transformers model but there are two models that are set as defaults: \"paraphrase-MiniLM-L6-v2\" \"paraphrase-multilingual-MiniLM-L12-v2\" The first is an English BERT-based model trained specifically for semantic similarity tasks which work quite well for most use-cases. The second model is very similar to the first with one major difference is that the xlm models work for 50+ languages. This model is quite a bit larger than the first and is only selected if you select any language other than English. Cluster Documents \u00b6 Next, in order to cluster the documents using a clustering algorithm such as HDBSCAN we first need to reduce its dimensionality as HDBCAN is prone to the curse of dimensionality. Thus, we first lower dimensionality with UMAP as it preserves local structure well after which we can use HDBSCAN to cluster similar documents. Create topic representation \u00b6 What we want to know from the clusters that we generated, is what makes one cluster, based on its content, different from another? To solve this, we can modify TF-IDF such that it allows for interesting words per topic instead of per document. When you apply TF-IDF as usual on a set of documents, what you are doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of topics ! This model is called class-based TF-IDF Each cluster is converted to a single document instead of a set of documents. Then, the frequency of word t are extracted for each class i and divided by the total number of words w . This action can now be seen as a form of regularization of frequent words in the class. Next, the average number of words per class m is divided by the total frequency of word t across all classes n . Topic Coherence \u00b6 This step is executed if you do not use custom embeddings but generate the document embeddings within BERTopic itself. The embedding model provided by BERTopic will be used to improve the coherence of words within a topic. After having generated the c-TF-IDF representations, we have a set of words that describe a collection of documents. Technically, this does not mean that this collection of words describes a coherent topic. In practice, we will see that many of the words do describe a similar topic but some words will, in a way, overfit the documents. For example, if you have a set of documents that are written by the same person whose signature will be in the topic description. To improve the coherence of words, Maximal Marginal Relevance was used to find the most coherent words without having too much overlap between the words themselves. This results in the removal of words that do not contribute to a topic.","title":"The Algorithm"},{"location":"tutorial/algorithm/algorithm.html#embed-documents","text":"We start by creating document embeddings from a set of documents using sentence-transformers . These models are pre-trained for many language and are great for creating either document- or sentence-embeddings. In BERTopic, you can choose any sentence transformers model but there are two models that are set as defaults: \"paraphrase-MiniLM-L6-v2\" \"paraphrase-multilingual-MiniLM-L12-v2\" The first is an English BERT-based model trained specifically for semantic similarity tasks which work quite well for most use-cases. The second model is very similar to the first with one major difference is that the xlm models work for 50+ languages. This model is quite a bit larger than the first and is only selected if you select any language other than English.","title":"Embed documents"},{"location":"tutorial/algorithm/algorithm.html#cluster-documents","text":"Next, in order to cluster the documents using a clustering algorithm such as HDBSCAN we first need to reduce its dimensionality as HDBCAN is prone to the curse of dimensionality. Thus, we first lower dimensionality with UMAP as it preserves local structure well after which we can use HDBSCAN to cluster similar documents.","title":"Cluster Documents"},{"location":"tutorial/algorithm/algorithm.html#create-topic-representation","text":"What we want to know from the clusters that we generated, is what makes one cluster, based on its content, different from another? To solve this, we can modify TF-IDF such that it allows for interesting words per topic instead of per document. When you apply TF-IDF as usual on a set of documents, what you are doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of topics ! This model is called class-based TF-IDF Each cluster is converted to a single document instead of a set of documents. Then, the frequency of word t are extracted for each class i and divided by the total number of words w . This action can now be seen as a form of regularization of frequent words in the class. Next, the average number of words per class m is divided by the total frequency of word t across all classes n .","title":"Create topic representation"},{"location":"tutorial/algorithm/algorithm.html#topic-coherence","text":"This step is executed if you do not use custom embeddings but generate the document embeddings within BERTopic itself. The embedding model provided by BERTopic will be used to improve the coherence of words within a topic. After having generated the c-TF-IDF representations, we have a set of words that describe a collection of documents. Technically, this does not mean that this collection of words describes a coherent topic. In practice, we will see that many of the words do describe a similar topic but some words will, in a way, overfit the documents. For example, if you have a set of documents that are written by the same person whose signature will be in the topic description. To improve the coherence of words, Maximal Marginal Relevance was used to find the most coherent words without having too much overlap between the words themselves. This results in the removal of words that do not contribute to a topic.","title":"Topic Coherence"},{"location":"tutorial/embeddings/embeddings.html","text":"Embedding Models \u00b6 In this tutorial, we will be going through the embedding models that can be used in BERTopic. Having the option to choose embedding models allows you to leverage pre-trained embeddings that suit your use case. Moreover, it helps to create a topic when you have little data available. Sentence Transformers \u00b6 You can select any model from sentence-transformers here and pass it through BERTopic with embedding_model : from bertopic import BERTopic topic_model = BERTopic ( embedding_model = \"paraphrase-MiniLM-L6-v2\" ) Or select a SentenceTransformer model with your parameters: from sentence_transformers import SentenceTransformer sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) topic_model = BERTopic ( embedding_model = sentence_model ) Flair \u00b6 Flair allows you to choose almost any embedding model that is publicly available. Flair can be used as follows: from flair.embeddings import TransformerDocumentEmbeddings roberta = TransformerDocumentEmbeddings ( 'roberta-base' ) topic_model = BERTopic ( embedding_model = roberta ) You can select any \ud83e\udd17 transformers model here . Moreover, you can also use Flair to use word embeddings and pool them to create document embeddings. Under the hood, Flair simply averages all word embeddings in a document. Then, we can easily pass it to BERTopic in order to use those word embeddings as document embeddings: from flair.embeddings import WordEmbeddings , DocumentPoolEmbeddings glove_embedding = WordEmbeddings ( 'crawl' ) document_glove_embeddings = DocumentPoolEmbeddings ([ glove_embedding ]) topic_model = BERTopic ( embedding_model = document_glove_embeddings ) Spacy \u00b6 Spacy is an amazing framework for processing text. There are many models available across many languages for modeling text. allows you to choose almost any embedding model that is publicly available. Flair can be used as follows: To use Spacy's non-transformer models in BERTopic: import spacy nlp = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) topic_model = BERTopic ( embedding_model = nlp ) Using spacy-transformer models: import spacy spacy . prefer_gpu () nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) topic_model = BERTopic ( embedding_model = nlp ) If you run into memory issues with spacy-transformer models, try: import spacy from thinc.api import set_gpu_allocator , require_gpu nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) set_gpu_allocator ( \"pytorch\" ) require_gpu ( 0 ) topic_model = BERTopic ( embedding_model = nlp ) Universal Sentence Encoder (USE) \u00b6 The Universal Sentence Encoder encodes text into high-dimensional vectors that are used here for embedding the documents. The model is trained and optimized for greater-than-word length text, such as sentences, phrases, or short paragraphs. Using USE in BERTopic is rather straightforward: import tensorflow_hub embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) topic_model = BERTopic ( embedding_model = embedding_model ) Gensim \u00b6 For Gensim, BERTopic supports its gensim.downloader module. Here, we can download any model word embedding model to be used in BERTopic. Note that Gensim is primarily used for Word Embedding models. This works typically best for short documents since the word embeddings are pooled. import gensim.downloader as api ft = api . load ( 'fasttext-wiki-news-subwords-300' ) topic_model = BERTopic ( embedding_model = ft ) Customization \u00b6 Over the last years, many new embedding models have been released that could be interesting to use as a backend in BERTopic. It is not always feasible to implement them all as there are simply too many to follow. In order to still allow to use those embeddings, BERTopic knows several ways to add these embeddings while still allowing for full functionality of BERTopic. Moreover, there are several customization options that allow for a bit more control over which embedding to use when. Word + Document Embeddings \u00b6 You might want to be using different language models for creating document- and word-embeddings. For example, while SentenceTransformers might be great in embedding sentences and documents, you might prefer to use FastText to create the word embeddings. from bertopic.backend import WordDocEmbedder import gensim.downloader as api from sentence_transformers import SentenceTransformer # Word embedding model ft = api . load ( 'fasttext-wiki-news-subwords-300' ) # Document embedding model embedding_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) # Create a model that uses both language models and pass it through BERTopic word_doc_embedder = WordDocEmbedder ( embedding_model = embedding_model , word_embedding_model = ft ) topic_model = BERTopic ( embedding_model = word_doc_embedder ) Custom Backend \u00b6 If your backend or model cannot be found in the ones currently available, you can use the bertopic.backend.BaseEmbedder class to create your backend. Below, you will find an example of creating a SentenceTransformer backend for BERTopic: from bertopic.backend import BaseEmbedder from sentence_transformers import SentenceTransformer class CustomEmbedder ( BaseEmbedder ): def __init__ ( self , embedding_model ): super () . __init__ () self . embedding_model = embedding_model def embed ( self , documents , verbose = False ): embeddings = self . embedding_model . encode ( documents , show_progress_bar = verbose ) return embeddings # Create custom backend embedding_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) custom_embedder = CustomEmbedder ( embedding_model = embedding_model ) # Pass custom backend to bertopic topic_model = BERTopic ( embedding_model = custom_embedder ) Custom Embeddings \u00b6 The base models in BERTopic are BERT-based models that work well with document similarity tasks. Your documents, however, might be too specific for a general pre-trained model to be used. Fortunately, you can use embedding model in BERTopic to create document features. You only need to prepare the document embeddings yourself and pass them through fit_transform of BERTopic: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Create topic model and use the custom embeddings topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs , embeddings ) As you can see above, we used a SentenceTransformer model to create the embedding. You could also have used \ud83e\udd17 transformers , Doc2Vec , or any other embedding method. TF-IDF \u00b6 As mentioned above, any embedding technique can be used. However, when running umap, the typical distance metric is cosine which does not work quite well for a TF-IDF matrix. Instead, BERTopic will recognize that a sparse matrix is passed and use hellinger instead which works quite well for the similarity between probability distributions. We simply create a TF-IDF matrix and use them as embeddings in our fit_transform method: from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer # Create TF-IDF sparse matrix docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] vectorizer = TfidfVectorizer ( min_df = 5 ) embeddings = vectorizer . fit_transform ( docs ) # topic_model = BERTopic ( stop_words = \"english\" ) topics , probs = topic_model . fit_transform ( docs , embeddings ) Here, you will probably notice that creating the embeddings is quite fast whereas fit_transform is quite slow. This is to be expected as reducing the dimensionality of a large sparse matrix takes some time. The inverse of using transformer embeddings is true: creating the embeddings is slow whereas fit_transform is quite fast. You can play around with different models until you find the best suiting model for you.","title":"Embedding Models"},{"location":"tutorial/embeddings/embeddings.html#embedding-models","text":"In this tutorial, we will be going through the embedding models that can be used in BERTopic. Having the option to choose embedding models allows you to leverage pre-trained embeddings that suit your use case. Moreover, it helps to create a topic when you have little data available.","title":"Embedding Models"},{"location":"tutorial/embeddings/embeddings.html#sentence-transformers","text":"You can select any model from sentence-transformers here and pass it through BERTopic with embedding_model : from bertopic import BERTopic topic_model = BERTopic ( embedding_model = \"paraphrase-MiniLM-L6-v2\" ) Or select a SentenceTransformer model with your parameters: from sentence_transformers import SentenceTransformer sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) topic_model = BERTopic ( embedding_model = sentence_model )","title":"Sentence Transformers"},{"location":"tutorial/embeddings/embeddings.html#flair","text":"Flair allows you to choose almost any embedding model that is publicly available. Flair can be used as follows: from flair.embeddings import TransformerDocumentEmbeddings roberta = TransformerDocumentEmbeddings ( 'roberta-base' ) topic_model = BERTopic ( embedding_model = roberta ) You can select any \ud83e\udd17 transformers model here . Moreover, you can also use Flair to use word embeddings and pool them to create document embeddings. Under the hood, Flair simply averages all word embeddings in a document. Then, we can easily pass it to BERTopic in order to use those word embeddings as document embeddings: from flair.embeddings import WordEmbeddings , DocumentPoolEmbeddings glove_embedding = WordEmbeddings ( 'crawl' ) document_glove_embeddings = DocumentPoolEmbeddings ([ glove_embedding ]) topic_model = BERTopic ( embedding_model = document_glove_embeddings )","title":"Flair"},{"location":"tutorial/embeddings/embeddings.html#spacy","text":"Spacy is an amazing framework for processing text. There are many models available across many languages for modeling text. allows you to choose almost any embedding model that is publicly available. Flair can be used as follows: To use Spacy's non-transformer models in BERTopic: import spacy nlp = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) topic_model = BERTopic ( embedding_model = nlp ) Using spacy-transformer models: import spacy spacy . prefer_gpu () nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) topic_model = BERTopic ( embedding_model = nlp ) If you run into memory issues with spacy-transformer models, try: import spacy from thinc.api import set_gpu_allocator , require_gpu nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) set_gpu_allocator ( \"pytorch\" ) require_gpu ( 0 ) topic_model = BERTopic ( embedding_model = nlp )","title":"Spacy"},{"location":"tutorial/embeddings/embeddings.html#universal-sentence-encoder-use","text":"The Universal Sentence Encoder encodes text into high-dimensional vectors that are used here for embedding the documents. The model is trained and optimized for greater-than-word length text, such as sentences, phrases, or short paragraphs. Using USE in BERTopic is rather straightforward: import tensorflow_hub embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) topic_model = BERTopic ( embedding_model = embedding_model )","title":"Universal Sentence Encoder (USE)"},{"location":"tutorial/embeddings/embeddings.html#gensim","text":"For Gensim, BERTopic supports its gensim.downloader module. Here, we can download any model word embedding model to be used in BERTopic. Note that Gensim is primarily used for Word Embedding models. This works typically best for short documents since the word embeddings are pooled. import gensim.downloader as api ft = api . load ( 'fasttext-wiki-news-subwords-300' ) topic_model = BERTopic ( embedding_model = ft )","title":"Gensim"},{"location":"tutorial/embeddings/embeddings.html#customization","text":"Over the last years, many new embedding models have been released that could be interesting to use as a backend in BERTopic. It is not always feasible to implement them all as there are simply too many to follow. In order to still allow to use those embeddings, BERTopic knows several ways to add these embeddings while still allowing for full functionality of BERTopic. Moreover, there are several customization options that allow for a bit more control over which embedding to use when.","title":"Customization"},{"location":"tutorial/embeddings/embeddings.html#word-document-embeddings","text":"You might want to be using different language models for creating document- and word-embeddings. For example, while SentenceTransformers might be great in embedding sentences and documents, you might prefer to use FastText to create the word embeddings. from bertopic.backend import WordDocEmbedder import gensim.downloader as api from sentence_transformers import SentenceTransformer # Word embedding model ft = api . load ( 'fasttext-wiki-news-subwords-300' ) # Document embedding model embedding_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) # Create a model that uses both language models and pass it through BERTopic word_doc_embedder = WordDocEmbedder ( embedding_model = embedding_model , word_embedding_model = ft ) topic_model = BERTopic ( embedding_model = word_doc_embedder )","title":"Word + Document Embeddings"},{"location":"tutorial/embeddings/embeddings.html#custom-backend","text":"If your backend or model cannot be found in the ones currently available, you can use the bertopic.backend.BaseEmbedder class to create your backend. Below, you will find an example of creating a SentenceTransformer backend for BERTopic: from bertopic.backend import BaseEmbedder from sentence_transformers import SentenceTransformer class CustomEmbedder ( BaseEmbedder ): def __init__ ( self , embedding_model ): super () . __init__ () self . embedding_model = embedding_model def embed ( self , documents , verbose = False ): embeddings = self . embedding_model . encode ( documents , show_progress_bar = verbose ) return embeddings # Create custom backend embedding_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) custom_embedder = CustomEmbedder ( embedding_model = embedding_model ) # Pass custom backend to bertopic topic_model = BERTopic ( embedding_model = custom_embedder )","title":"Custom Backend"},{"location":"tutorial/embeddings/embeddings.html#custom-embeddings","text":"The base models in BERTopic are BERT-based models that work well with document similarity tasks. Your documents, however, might be too specific for a general pre-trained model to be used. Fortunately, you can use embedding model in BERTopic to create document features. You only need to prepare the document embeddings yourself and pass them through fit_transform of BERTopic: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Create topic model and use the custom embeddings topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs , embeddings ) As you can see above, we used a SentenceTransformer model to create the embedding. You could also have used \ud83e\udd17 transformers , Doc2Vec , or any other embedding method.","title":"Custom Embeddings"},{"location":"tutorial/embeddings/embeddings.html#tf-idf","text":"As mentioned above, any embedding technique can be used. However, when running umap, the typical distance metric is cosine which does not work quite well for a TF-IDF matrix. Instead, BERTopic will recognize that a sparse matrix is passed and use hellinger instead which works quite well for the similarity between probability distributions. We simply create a TF-IDF matrix and use them as embeddings in our fit_transform method: from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer # Create TF-IDF sparse matrix docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] vectorizer = TfidfVectorizer ( min_df = 5 ) embeddings = vectorizer . fit_transform ( docs ) # topic_model = BERTopic ( stop_words = \"english\" ) topics , probs = topic_model . fit_transform ( docs , embeddings ) Here, you will probably notice that creating the embeddings is quite fast whereas fit_transform is quite slow. This is to be expected as reducing the dimensionality of a large sparse matrix takes some time. The inverse of using transformer embeddings is true: creating the embeddings is slow whereas fit_transform is quite fast. You can play around with different models until you find the best suiting model for you.","title":"TF-IDF"},{"location":"tutorial/guided/guided.html","text":"Guided Topic Modeling \u00b6 Guided Topic Modeling or Seeded Topic Modeling is a collection of techniques that guides the topic modeling approach by setting a number of seed topics in which the model will converge to. These techniques allow the user to set a pre-defined number of topic representations that are sure to be in documents. For example, take an IT-business that has a ticket system for the software their clients use. Those tickets may typically contain information about a specific bug regarding login issues that the IT-business is aware off. To model that bug, we can create a seed topic representation containing the words bug , login , password , and username . By defining those words, a Guided Topic Modeling approach will try to converge at least one topic to those words. Guided BERTopic has two main steps: First, we create embeddings for each seeded topics by joining them and passing them through the document embedder. These embeddings will be compared with the existing document embeddings through cosine similarity and assigned a label. If the document is most similar to a seeded topic, then it will get that topic's label. If it is most similar to the average document embedding, it will get the -1 label. These labels are then passed through UMAP to create a semi-supervised approach that should nudge the topic creation to the seeded topics. Second, we take all words in seed_topic_list and assign them a multiplier larger than 1. Those multipliers will be used to increase the IDF values of the words across all topics thereby increasing the likelihood that a seeded topic word will appear in a topic. This does, however, also increase the chance of an irrelevant topic having unrelated words. In practice, this should not be an issue since the IDF value is likely to remain low regardless of the multiplier. The multiplier is now a fixed value but may change to something more elegant, like taking the distribution of IDF values and its position into account when defining the multiplier. Example \u00b6 To demonstrate Guided BERTopic, we use the 20 Newsgroups dataset as our example. We have frequently used this dataset in BERTopic examples and we sometimes see a topic generated about health with words as drug and cancer being important. However, due to the stocastisch nature of UMAP this topic is not always found. In order to guide BERTopic to that topic, we create a seed topic list that we pass through our model. However, there may be several other topics that we know should be in the documents. Let's also initialize those: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] seed_topic_list = [[ \"drug\" , \"cancer\" , \"drugs\" , \"doctor\" ], [ \"windows\" , \"drive\" , \"dos\" , \"file\" ], [ \"space\" , \"launch\" , \"orbit\" , \"lunar\" ]] topic_model = BERTopic ( seed_topic_list = seed_topic_list ) topics , probs = topic_model . fit_transform ( docs ) AS you can see above, the seed_topic_list contains a list of topic representations. By defining the above topics BERTopic is more likely to model the defined seeded topics. However, BERTopic is merely nudged towards creating those topics. In practice, if the seeded topics do not exist or might be divided into smaller topics, then they will not be modeled. Thus, seed topics need to be accurate in order to accurately converge towards them.","title":"Guided Topic Modeling"},{"location":"tutorial/guided/guided.html#guided-topic-modeling","text":"Guided Topic Modeling or Seeded Topic Modeling is a collection of techniques that guides the topic modeling approach by setting a number of seed topics in which the model will converge to. These techniques allow the user to set a pre-defined number of topic representations that are sure to be in documents. For example, take an IT-business that has a ticket system for the software their clients use. Those tickets may typically contain information about a specific bug regarding login issues that the IT-business is aware off. To model that bug, we can create a seed topic representation containing the words bug , login , password , and username . By defining those words, a Guided Topic Modeling approach will try to converge at least one topic to those words. Guided BERTopic has two main steps: First, we create embeddings for each seeded topics by joining them and passing them through the document embedder. These embeddings will be compared with the existing document embeddings through cosine similarity and assigned a label. If the document is most similar to a seeded topic, then it will get that topic's label. If it is most similar to the average document embedding, it will get the -1 label. These labels are then passed through UMAP to create a semi-supervised approach that should nudge the topic creation to the seeded topics. Second, we take all words in seed_topic_list and assign them a multiplier larger than 1. Those multipliers will be used to increase the IDF values of the words across all topics thereby increasing the likelihood that a seeded topic word will appear in a topic. This does, however, also increase the chance of an irrelevant topic having unrelated words. In practice, this should not be an issue since the IDF value is likely to remain low regardless of the multiplier. The multiplier is now a fixed value but may change to something more elegant, like taking the distribution of IDF values and its position into account when defining the multiplier.","title":"Guided Topic Modeling"},{"location":"tutorial/guided/guided.html#example","text":"To demonstrate Guided BERTopic, we use the 20 Newsgroups dataset as our example. We have frequently used this dataset in BERTopic examples and we sometimes see a topic generated about health with words as drug and cancer being important. However, due to the stocastisch nature of UMAP this topic is not always found. In order to guide BERTopic to that topic, we create a seed topic list that we pass through our model. However, there may be several other topics that we know should be in the documents. Let's also initialize those: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] seed_topic_list = [[ \"drug\" , \"cancer\" , \"drugs\" , \"doctor\" ], [ \"windows\" , \"drive\" , \"dos\" , \"file\" ], [ \"space\" , \"launch\" , \"orbit\" , \"lunar\" ]] topic_model = BERTopic ( seed_topic_list = seed_topic_list ) topics , probs = topic_model . fit_transform ( docs ) AS you can see above, the seed_topic_list contains a list of topic representations. By defining the above topics BERTopic is more likely to model the defined seeded topics. However, BERTopic is merely nudged towards creating those topics. In practice, if the seeded topics do not exist or might be divided into smaller topics, then they will not be modeled. Thus, seed topics need to be accurate in order to accurately converge towards them.","title":"Example"},{"location":"tutorial/models/models.html","text":"There are three models underpinning BERTopic that are most important in creating the topics, namely UMAP, HDBSCAN, and CountVectorizer. The parameters of these models have been carefully selected to give the best results. However, there is no one-size-fits-all solution using these default parameters. Therefore, BERTopic allows you to pass in any custom UMAP, HDBSCAN, and/or CountVectorizer with the parameters that best suit your use case. For example, you might want to change the minimum document frequency in CountVectorizer or use a different distance metric in HDBSCAN or UMAP. To do this, simply create the instances of these models and initialize BERTopic with them: from bertopic import BERTopic from umap import UMAP from hdbscan import HDBSCAN from sklearn.feature_extraction.text import CountVectorizer # Prepare custom models hdbscan_model = HDBSCAN ( min_cluster_size = 10 , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) umap_model = UMAP ( n_neighbors = 15 , n_components = 10 , min_dist = 0.0 , metric = 'cosine' ) vectorizer_model = CountVectorizer ( ngram_range = ( 1 , 2 ), stop_words = \"english\" ) # Pass the custom models to BERTopic topic_model = BERTopic ( umap_model = umap_model , hdbscan_model = hdbscan_model , vectorizer_model = vectorizer_model )","title":"Custom Sub-Models"},{"location":"tutorial/quickstart/quickstart.html","text":"Installation \u00b6 Installation, with sentence-transformers, can be done using pypi : pip install bertopic You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install bertopic [ flair ] pip install bertopic [ gensim ] pip install bertopic [ spacy ] pip install bertopic [ use ] To install all backends: pip install bertopic [ all ] Quick Start \u00b6 We start by extracting topics from the well-known 20 newsgroups dataset which is comprised of English documents: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) After generating topics, we can access the frequent topics that were generated: >>> topic_model . get_topic_info () Topic Count Name - 1 4630 - 1 _can_your_will_any 0 693 49 _windows_drive_dos_file 1 466 32 _jesus_bible_christian_faith 2 441 2 _space_launch_orbit_lunar 3 381 22 _key_encryption_keys_encrypted -1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated, topic 0: >>> topic_model . get_topic ( 0 ) [( 'windows' , 0.006152228076250982 ), ( 'drive' , 0.004982897610645755 ), ( 'dos' , 0.004845038866360651 ), ( 'file' , 0.004140142872194834 ), ( 'disk' , 0.004131678774810884 ), ( 'mac' , 0.003624848635985097 ), ( 'memory' , 0.0034840976976789903 ), ( 'software' , 0.0034415334250699077 ), ( 'email' , 0.0034239554442333257 ), ( 'pc' , 0.003047105930670237 )] NOTE : Use BERTopic(language=\"multilingual\") to select a model that supports 50+ languages. Visualize Topics \u00b6 After having trained our BERTopic model, we can iteratively go through perhaps a hundred topic to get a good understanding of the topics that were extracted. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis : topic_model . visualize_topics () Save/Load BERTopic model \u00b6 We can easily save a trained BERTopic model by calling save : from bertopic import BERTopic topic_model = BERTopic () topic_model . save ( \"my_model\" ) Then, we can load the model in one line: topic_model = BERTopic . load ( \"my_model\" ) If you do not want to save the embedding model because it is loaded from the cloud, simply run model.save(\"my_model\", save_embedding_model=False) instead. Then, you can load in the model with BERTopic.load(\"my_model\", embedding_model=\"whatever_model_you_used\") .","title":"Getting Started"},{"location":"tutorial/quickstart/quickstart.html#installation","text":"Installation, with sentence-transformers, can be done using pypi : pip install bertopic You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install bertopic [ flair ] pip install bertopic [ gensim ] pip install bertopic [ spacy ] pip install bertopic [ use ] To install all backends: pip install bertopic [ all ]","title":"Installation"},{"location":"tutorial/quickstart/quickstart.html#quick-start","text":"We start by extracting topics from the well-known 20 newsgroups dataset which is comprised of English documents: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) After generating topics, we can access the frequent topics that were generated: >>> topic_model . get_topic_info () Topic Count Name - 1 4630 - 1 _can_your_will_any 0 693 49 _windows_drive_dos_file 1 466 32 _jesus_bible_christian_faith 2 441 2 _space_launch_orbit_lunar 3 381 22 _key_encryption_keys_encrypted -1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated, topic 0: >>> topic_model . get_topic ( 0 ) [( 'windows' , 0.006152228076250982 ), ( 'drive' , 0.004982897610645755 ), ( 'dos' , 0.004845038866360651 ), ( 'file' , 0.004140142872194834 ), ( 'disk' , 0.004131678774810884 ), ( 'mac' , 0.003624848635985097 ), ( 'memory' , 0.0034840976976789903 ), ( 'software' , 0.0034415334250699077 ), ( 'email' , 0.0034239554442333257 ), ( 'pc' , 0.003047105930670237 )] NOTE : Use BERTopic(language=\"multilingual\") to select a model that supports 50+ languages.","title":"Quick Start"},{"location":"tutorial/quickstart/quickstart.html#visualize-topics","text":"After having trained our BERTopic model, we can iteratively go through perhaps a hundred topic to get a good understanding of the topics that were extracted. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis : topic_model . visualize_topics ()","title":"Visualize Topics"},{"location":"tutorial/quickstart/quickstart.html#saveload-bertopic-model","text":"We can easily save a trained BERTopic model by calling save : from bertopic import BERTopic topic_model = BERTopic () topic_model . save ( \"my_model\" ) Then, we can load the model in one line: topic_model = BERTopic . load ( \"my_model\" ) If you do not want to save the embedding model because it is loaded from the cloud, simply run model.save(\"my_model\", save_embedding_model=False) instead. Then, you can load in the model with BERTopic.load(\"my_model\", embedding_model=\"whatever_model_you_used\") .","title":"Save/Load BERTopic model"},{"location":"tutorial/search/search.html","text":"After having created a BERTopic model, you might end up with over a hundred topics. Searching through those can be quite cumbersome especially if you are searching for a specific topic. Fortunately, BERTopic allows you to search for topics using search terms. First, let's create and train a BERTopic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) After having trained our model, we can use find_topics to search for topics that are similar to an input search_term. Here, we are going to be searching for topics that closely relate the search term \"motor\". Then, we extract the most similar topic and check the results: >>> similar_topics , similarity = topic_model . find_topics ( \"motor\" , top_n = 5 ) >>> topic_model . get_topic ( similar_topics [ 0 ]) [( 'bike' , 0.02275997701645559 ), ( 'motorcycle' , 0.011391202866080292 ), ( 'bikes' , 0.00981187573649205 ), ( 'dod' , 0.009614623748226669 ), ( 'honda' , 0.008247663662558535 ), ( 'ride' , 0.0064683227888861945 ), ( 'harley' , 0.006355502638631013 ), ( 'riding' , 0.005766601561614182 ), ( 'motorcycles' , 0.005596372493714447 ), ( 'advice' , 0.005534544418830091 )] It definitely seems that a topic was found that closely matches with \"motor\". The topic seems to be motorcycle related and therefore matches with our \"motor\" input. You can use the similarity variable to see how similar the extracted topics are to the search term.","title":"Search Topics"},{"location":"tutorial/supervised/supervised.html","text":"In this tutorial, we will be looking at a new feature of BERTopic, namely (semi)-supervised topic modeling! This allows us to steer the dimensionality reduction of the embeddings into a space that closely follows any labels you might already have. In other words, we use a semi-supervised UMAP instance to reduce the dimensionality of embeddings before clustering the documents with HDBSCAN. First, let us prepare the data needed for our topic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups data = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' )) docs = data [ \"data\" ] categories = data [ \"target\" ] category_names = data [ \"target_names\" ] We are using the popular 20 Newsgroups dataset which contains roughly 18000 newsgroups posts that each is assigned to one of 20 categories. Using this dataset we can try to extract its corresponding topic model whilst taking its underlying categories into account. These categories are here the variable targets . Each document can be put into one of the following categories: >>> category_names [ 'alt.atheism' , 'comp.graphics' , 'comp.os.ms-windows.misc' , 'comp.sys.ibm.pc.hardware' , 'comp.sys.mac.hardware' , 'comp.windows.x' , 'misc.forsale' , 'rec.autos' , 'rec.motorcycles' , 'rec.sport.baseball' , 'rec.sport.hockey' , 'sci.crypt' , 'sci.electronics' , 'sci.med' , 'sci.space' , 'soc.religion.christian' , 'talk.politics.guns' , 'talk.politics.mideast' , 'talk.politics.misc' , 'talk.religion.misc' ] Semi-supervised Topic Modeling \u00b6 In semi-supervised topic modeling, we only have some labels for our documents. The documents for which we do have labels are used to somewhat guide BERTopic to the extraction of topics for those labels. The documents for which we do not have labels are assigned a -1. For this example, imagine we only the labels of categories that are related to computers and we want to create a topic model using semi-supervised modeling: labels_to_add = [ 'comp.graphics' , 'comp.os.ms-windows.misc' , 'comp.sys.ibm.pc.hardware' , 'comp.sys.mac.hardware' , 'comp.windows.x' ,] indices = [ category_names . index ( label ) for label in labels_to_add ] y = [ label if label in indices else - 1 for label in categories ] The y variable contains many -1 values since we do not know all the categories. Next, we use those newly constructed labels to again BERTopic semi-supervised: topic_model = BERTopic ( verbose = True ) . fit ( docs , y = y ) And that is it! By defining certain classes for our documents, we can steer the topic modeling towards modeling the pre-defined categories. Supervised Topic Modeling \u00b6 In supervised topic modeling, we have labels for all our documents. This can be pre-defined topics or simply documents that you feel belong together regardless of their content. BERTopic will nudge the creation of topics towards these categories using the pre-defined labels. To perform supervised topic modeling, we simply use all categories: topic_model = BERTopic ( verbose = True ) . fit ( docs , y = categories ) The topic model will be much more attuned to the categories that were defined previously. However, this does not mean that only topics for these categories will be found. BERTopic is likely to find more specific topics in those you have already defined. This allows you to discover previously unknown topics!","title":"(semi)-Supervised Topic Modeling"},{"location":"tutorial/supervised/supervised.html#semi-supervised-topic-modeling","text":"In semi-supervised topic modeling, we only have some labels for our documents. The documents for which we do have labels are used to somewhat guide BERTopic to the extraction of topics for those labels. The documents for which we do not have labels are assigned a -1. For this example, imagine we only the labels of categories that are related to computers and we want to create a topic model using semi-supervised modeling: labels_to_add = [ 'comp.graphics' , 'comp.os.ms-windows.misc' , 'comp.sys.ibm.pc.hardware' , 'comp.sys.mac.hardware' , 'comp.windows.x' ,] indices = [ category_names . index ( label ) for label in labels_to_add ] y = [ label if label in indices else - 1 for label in categories ] The y variable contains many -1 values since we do not know all the categories. Next, we use those newly constructed labels to again BERTopic semi-supervised: topic_model = BERTopic ( verbose = True ) . fit ( docs , y = y ) And that is it! By defining certain classes for our documents, we can steer the topic modeling towards modeling the pre-defined categories.","title":"Semi-supervised Topic Modeling"},{"location":"tutorial/supervised/supervised.html#supervised-topic-modeling","text":"In supervised topic modeling, we have labels for all our documents. This can be pre-defined topics or simply documents that you feel belong together regardless of their content. BERTopic will nudge the creation of topics towards these categories using the pre-defined labels. To perform supervised topic modeling, we simply use all categories: topic_model = BERTopic ( verbose = True ) . fit ( docs , y = categories ) The topic model will be much more attuned to the categories that were defined previously. However, this does not mean that only topics for these categories will be found. BERTopic is likely to find more specific topics in those you have already defined. This allows you to discover previously unknown topics!","title":"Supervised Topic Modeling"},{"location":"tutorial/topicreduction/topicreduction.html","text":"In BERTopic , there are several arguments that might be helpful if you tend to end up with too many or too few topics. Topic Parameters \u00b6 The arguments discussed here all relate to the clustering step of BERTopic. Minimum topic size \u00b6 The min_topic_size parameter is actually used in HDBSCAN . It tells HDBSCAN what the minimum size of a cluster should be before it is accepted as a cluster. When you set this parameter very high, you will get very few clusters as they all need to be high. In contrast, if you set this too low you might end with too many extremely specific clusters. from bertopic import BERTopic topic_model = BERTopic ( min_topic_size = 10 ) You can increase this value if you have more data available or if you expect clusters to be quite large. Hierarchical Topic Reduction \u00b6 HDBSCAN can't specify the number of clusters you would want. To a certain extent, this is an advantage, as we can trust HDBSCAN to be better in finding the number of clusters than we are. Instead, we can try to reduce the number of topics that have been created. Below, you will find three methods of doing so. Manual Topic Reduction \u00b6 Each resulting topic has its own feature vector constructed from c-TF-IDF. Using those feature vectors, we can find the most similar topics and merge them. If we do this iteratively, starting from the least frequent topic, we can reduce the number of topics quite easily. We do this until we reach the value of nr_topics : from bertopic import BERTopic topic_model = BERTopic ( nr_topics = 20 ) Automatic Topic Reduction \u00b6 One issue with the approach above is that it will merge topics regardless of whether they are very similar. They are simply the most similar out of all options. This can be resolved by reducing the number of topics automatically. It will reduce the number of topics, starting from the least frequent topic, as long as it exceeds a minimum similarity of 0.915. To use this option, we simply set nr_topics to \"auto\" : from bertopic import BERTopic topic_model = BERTopic ( nr_topics = \"auto\" ) Topic Reduction after Training \u00b6 Finally, we can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, is that you can decide the number of topics after knowing how many are created. It is difficult to predict before training your model how many topics that are in your documents and how many will be extracted. Instead, we can decide afterward how many topics seem realistic: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics -> Typically over 50 topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic ( calculate_probabilities = True ) topics , probs = topic_model . fit_transform ( docs ) # Further reduce topics new_topics , new_probs = topic_model . reduce_topics ( docs , topics , probabilities = probs , nr_topics = 30 ) The reasoning for putting docs and topics (and optionally probabilities ) as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it is very inefficient to save those in BERTopic instead of a dedicated database.","title":"Topic Reduction"},{"location":"tutorial/topicreduction/topicreduction.html#topic-parameters","text":"The arguments discussed here all relate to the clustering step of BERTopic.","title":"Topic Parameters"},{"location":"tutorial/topicreduction/topicreduction.html#minimum-topic-size","text":"The min_topic_size parameter is actually used in HDBSCAN . It tells HDBSCAN what the minimum size of a cluster should be before it is accepted as a cluster. When you set this parameter very high, you will get very few clusters as they all need to be high. In contrast, if you set this too low you might end with too many extremely specific clusters. from bertopic import BERTopic topic_model = BERTopic ( min_topic_size = 10 ) You can increase this value if you have more data available or if you expect clusters to be quite large.","title":"Minimum topic size"},{"location":"tutorial/topicreduction/topicreduction.html#hierarchical-topic-reduction","text":"HDBSCAN can't specify the number of clusters you would want. To a certain extent, this is an advantage, as we can trust HDBSCAN to be better in finding the number of clusters than we are. Instead, we can try to reduce the number of topics that have been created. Below, you will find three methods of doing so.","title":"Hierarchical Topic Reduction"},{"location":"tutorial/topicreduction/topicreduction.html#manual-topic-reduction","text":"Each resulting topic has its own feature vector constructed from c-TF-IDF. Using those feature vectors, we can find the most similar topics and merge them. If we do this iteratively, starting from the least frequent topic, we can reduce the number of topics quite easily. We do this until we reach the value of nr_topics : from bertopic import BERTopic topic_model = BERTopic ( nr_topics = 20 )","title":"Manual Topic Reduction"},{"location":"tutorial/topicreduction/topicreduction.html#automatic-topic-reduction","text":"One issue with the approach above is that it will merge topics regardless of whether they are very similar. They are simply the most similar out of all options. This can be resolved by reducing the number of topics automatically. It will reduce the number of topics, starting from the least frequent topic, as long as it exceeds a minimum similarity of 0.915. To use this option, we simply set nr_topics to \"auto\" : from bertopic import BERTopic topic_model = BERTopic ( nr_topics = \"auto\" )","title":"Automatic Topic Reduction"},{"location":"tutorial/topicreduction/topicreduction.html#topic-reduction-after-training","text":"Finally, we can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, is that you can decide the number of topics after knowing how many are created. It is difficult to predict before training your model how many topics that are in your documents and how many will be extracted. Instead, we can decide afterward how many topics seem realistic: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics -> Typically over 50 topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic ( calculate_probabilities = True ) topics , probs = topic_model . fit_transform ( docs ) # Further reduce topics new_topics , new_probs = topic_model . reduce_topics ( docs , topics , probabilities = probs , nr_topics = 30 ) The reasoning for putting docs and topics (and optionally probabilities ) as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it is very inefficient to save those in BERTopic instead of a dedicated database.","title":"Topic Reduction after Training"},{"location":"tutorial/topicrepresentation/topicrepresentation.html","text":"The topics that are extracted from BERTopic are represented by words. These words are extracted from the documents occupying their topics using a class-based TF-IDF. This allows us to extract words that are interesting to a topic but less so to another. Update Topic Representation after Training \u00b6 When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. We can use the function update_topics to update the topic representation with new parameters for c-TF-IDF : from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic ( n_gram_range = ( 2 , 3 )) topics , probs = topic_model . fit_transform ( docs ) From the model created above, one of the most frequent topics is the following: >>> topic_model . get_topic ( 31 )[: 10 ] [( 'clipper chip' , 0.007240771542316232 ), ( 'key escrow' , 0.004601603973377443 ), ( 'law enforcement' , 0.004277247929596332 ), ( 'intercon com' , 0.0035961920238955824 ), ( 'amanda walker' , 0.003474856425297157 ), ( 'serial number' , 0.0029876119137150358 ), ( 'com amanda' , 0.002789303096817983 ), ( 'intercon com amanda' , 0.0027386688593327084 ), ( 'amanda intercon' , 0.002585262048515583 ), ( 'amanda intercon com' , 0.002585262048515583 )] Although there does seem to be some relation between words, it is difficult, at least for me, to intuitively understand what the topic is about. Instead, let's simplify the topic representation by setting n_gram_range to (1, 3) to also allow for single words. >>> topic_model . update_topics ( docs , topics , n_gram_range = ( 1 , 3 )) >>> topic_model . get_topic ( 31 )[: 10 ] [( 'encryption' , 0.008021846079148017 ), ( 'clipper' , 0.00789642647602742 ), ( 'chip' , 0.00637127942464045 ), ( 'key' , 0.006363124787175884 ), ( 'escrow' , 0.005030980365244285 ), ( 'clipper chip' , 0.0048271268437973395 ), ( 'keys' , 0.0043245812747907545 ), ( 'crypto' , 0.004311198708675516 ), ( 'intercon' , 0.0038772934659295076 ), ( 'amanda' , 0.003516026493904586 )] To me, the combination of the words above seem a bit more intuitive than the words we previously had! You can play around with n_gram_range or use your own custom sklearn.feature_extraction.text.CountVectorizer and pass that instead: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( stop_words = \"English\" , ngram_range = ( 1 , 5 )) topic_model . update_topics ( docs , topics , vectorizer_model = vectorizer_model )","title":"Topic Representation"},{"location":"tutorial/topicrepresentation/topicrepresentation.html#update-topic-representation-after-training","text":"When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. We can use the function update_topics to update the topic representation with new parameters for c-TF-IDF : from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic ( n_gram_range = ( 2 , 3 )) topics , probs = topic_model . fit_transform ( docs ) From the model created above, one of the most frequent topics is the following: >>> topic_model . get_topic ( 31 )[: 10 ] [( 'clipper chip' , 0.007240771542316232 ), ( 'key escrow' , 0.004601603973377443 ), ( 'law enforcement' , 0.004277247929596332 ), ( 'intercon com' , 0.0035961920238955824 ), ( 'amanda walker' , 0.003474856425297157 ), ( 'serial number' , 0.0029876119137150358 ), ( 'com amanda' , 0.002789303096817983 ), ( 'intercon com amanda' , 0.0027386688593327084 ), ( 'amanda intercon' , 0.002585262048515583 ), ( 'amanda intercon com' , 0.002585262048515583 )] Although there does seem to be some relation between words, it is difficult, at least for me, to intuitively understand what the topic is about. Instead, let's simplify the topic representation by setting n_gram_range to (1, 3) to also allow for single words. >>> topic_model . update_topics ( docs , topics , n_gram_range = ( 1 , 3 )) >>> topic_model . get_topic ( 31 )[: 10 ] [( 'encryption' , 0.008021846079148017 ), ( 'clipper' , 0.00789642647602742 ), ( 'chip' , 0.00637127942464045 ), ( 'key' , 0.006363124787175884 ), ( 'escrow' , 0.005030980365244285 ), ( 'clipper chip' , 0.0048271268437973395 ), ( 'keys' , 0.0043245812747907545 ), ( 'crypto' , 0.004311198708675516 ), ( 'intercon' , 0.0038772934659295076 ), ( 'amanda' , 0.003516026493904586 )] To me, the combination of the words above seem a bit more intuitive than the words we previously had! You can play around with n_gram_range or use your own custom sklearn.feature_extraction.text.CountVectorizer and pass that instead: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( stop_words = \"English\" , ngram_range = ( 1 , 5 )) topic_model . update_topics ( docs , topics , vectorizer_model = vectorizer_model )","title":"Update Topic Representation after Training"},{"location":"tutorial/topicsovertime/topicsovertime.html","text":"Dynamic Topic Modeling \u00b6 Dynamic topic modeling (DTM) is a collection of techniques aimed at analyzing the evolution of topics over time. These methods allow you to understand how a topic is represented across different times. For example, in 1995 people may talk differently about environmental awareness than those in 2015. Although the topic itself remains the same, environmental awareness, the exact representation of that topic might differ. BERTopic allows for DTM by calculating the topic representation at each timestep without the need to run the entire model several times. To do this, we first need to fit BERTopic as if there were no temporal aspect in the data. Thus, a general topic model will be created. We use the global representation as to the main topics that can be found at, most likely, different timesteps. For each topic and timestep, we calculate the c-TF-IDF representation. This will result in a specific topic representation at each timestep without the need to create clusters from embeddings as they were already created. Next, there are two main ways to further fine-tune these specific topic representations, namely globally and evolutionary . A topic representation at timestep t can fine-tuned globally by averaging its c-TF-IDF representation with that of the global representation. This allows each topic representation to move slightly towards the global representation whilst still keeping some its specific words. A topic representation at timestep t can be fine-tuned evolutionary by averaging its c-TF-IDF representation with that of the c-TF-IDF representation at timestep t-1 . This is done for each topic representation allowing for the representations to evolve over time. Both fine-tuning methods are set to True as a default and allow for interesting representations to be created. Example \u00b6 To demonstrate DTM in BERTopic, we first need to prepare our data. A good example of where DTM is useful is topic modeling on Twitter data. We can analyze how certain people have talked about certain topics in the years they have been on Twitter. Due to the controversial nature of his tweets, we are going to be using all tweets by Donald Trump. First, we need to load in the data and do some very basic cleaning. For example, I am not interested in his re-tweets for this use-case: import re import pandas as pd # Prepare data trump = pd . read_csv ( 'https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6' ) trump . text = trump . apply ( lambda row : re . sub ( r \"http\\S+\" , \"\" , row . text ) . lower (), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( filter ( lambda x : x [ 0 ] != \"@\" , row . text . split ())), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( re . sub ( \"[^a-zA-Z]+\" , \" \" , row . text ) . split ()), 1 ) trump = trump . loc [( trump . isRetweet == \"f\" ) & ( trump . text != \"\" ), :] timestamps = trump . date . to_list () tweets = trump . text . to_list () Then, we need to extract the global topic representations by simply creating and training a BERTopic model: from bertopic import BERTopic topic_model = BERTopic ( verbose = True ) topics , probs = topic_model . fit_transform ( tweets ) From these topics, we are going to generate the topic representations at each timestamp for each topic. We do this by simply calling topics_over_time and pass in his tweets, the corresponding timestamps, and the related topics: topics_over_time = model . topics_over_time ( tweets , topics , timestamps ) And that is it! Aside from what you always need for BERTopic, you now only need to add topics and timestamps to quickly calculate the topics over time. Parameters \u00b6 There are a few parameters that are of interest which will be discussed below. Tuning \u00b6 Both global_tuning and evolutionary_tuning are set to True as a default, but can easily be changed. Perhaps you do not want the representations to be influenced by the global representation and merely see how they evolved over time: topics_over_time = topic_model . topics_over_time ( tweets , topics , timestamps , global_tuning = True , evolution_tuning = True ) Bins \u00b6 If you have more than 100 unique timestamps, then there will be topic representations created for each of those timestamps which can negatively affect the topic representations. It is advised to keep the number of unique timestamps below 50. To do this, you can simply set the number of bins that are created when calculating the topic representations. The timestamps will be taken and put into equal-sized bins: topics_over_time = topic_model . topics_over_time ( tweets , topics , timestamps , nr_bins = 20 ) Datetime format \u00b6 If you are passing strings (dates) instead of integers, then BERTopic will try to automatically detect which datetime format your strings have. Unfortunately, this will not always work if they are in an unexpected format. We can use datetime_format to pass the format the timestamps have: topics_over_time = topic_model . topics_over_time ( tweets , topics , timestamps , datetime_format = \"%b%M\" ) Visualization \u00b6 To me, DTM becomes truly interesting when you have a good way of visualizing how topics have changed over time. A nice way of doing so is leveraging the interactive abilities of Plotly. Plotly allows us to show the frequency of topics over time whilst giving the option of hovering over the points to show the time-specific topic representations. Simply call visualize_topics_over_time with the newly created topics over time: topic_model . visualize_topics_over_time ( topics_over_time , top_n_topics = 20 ) I used top_n_topics to only show the top 20 most frequent topics. If I were to visualize all topics, which is possible by leaving top_n_topics empty, there is a chance that hundreds of lines will fill the plot. You can also use topics to show specific topics: topic_model . visualize_topics_over_time ( topics_over_time , topics = [ 9 , 10 , 72 , 83 , 87 , 91 ])","title":"Dynamic Topic Modeling"},{"location":"tutorial/topicsovertime/topicsovertime.html#dynamic-topic-modeling","text":"Dynamic topic modeling (DTM) is a collection of techniques aimed at analyzing the evolution of topics over time. These methods allow you to understand how a topic is represented across different times. For example, in 1995 people may talk differently about environmental awareness than those in 2015. Although the topic itself remains the same, environmental awareness, the exact representation of that topic might differ. BERTopic allows for DTM by calculating the topic representation at each timestep without the need to run the entire model several times. To do this, we first need to fit BERTopic as if there were no temporal aspect in the data. Thus, a general topic model will be created. We use the global representation as to the main topics that can be found at, most likely, different timesteps. For each topic and timestep, we calculate the c-TF-IDF representation. This will result in a specific topic representation at each timestep without the need to create clusters from embeddings as they were already created. Next, there are two main ways to further fine-tune these specific topic representations, namely globally and evolutionary . A topic representation at timestep t can fine-tuned globally by averaging its c-TF-IDF representation with that of the global representation. This allows each topic representation to move slightly towards the global representation whilst still keeping some its specific words. A topic representation at timestep t can be fine-tuned evolutionary by averaging its c-TF-IDF representation with that of the c-TF-IDF representation at timestep t-1 . This is done for each topic representation allowing for the representations to evolve over time. Both fine-tuning methods are set to True as a default and allow for interesting representations to be created.","title":"Dynamic Topic Modeling"},{"location":"tutorial/topicsovertime/topicsovertime.html#example","text":"To demonstrate DTM in BERTopic, we first need to prepare our data. A good example of where DTM is useful is topic modeling on Twitter data. We can analyze how certain people have talked about certain topics in the years they have been on Twitter. Due to the controversial nature of his tweets, we are going to be using all tweets by Donald Trump. First, we need to load in the data and do some very basic cleaning. For example, I am not interested in his re-tweets for this use-case: import re import pandas as pd # Prepare data trump = pd . read_csv ( 'https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6' ) trump . text = trump . apply ( lambda row : re . sub ( r \"http\\S+\" , \"\" , row . text ) . lower (), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( filter ( lambda x : x [ 0 ] != \"@\" , row . text . split ())), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( re . sub ( \"[^a-zA-Z]+\" , \" \" , row . text ) . split ()), 1 ) trump = trump . loc [( trump . isRetweet == \"f\" ) & ( trump . text != \"\" ), :] timestamps = trump . date . to_list () tweets = trump . text . to_list () Then, we need to extract the global topic representations by simply creating and training a BERTopic model: from bertopic import BERTopic topic_model = BERTopic ( verbose = True ) topics , probs = topic_model . fit_transform ( tweets ) From these topics, we are going to generate the topic representations at each timestamp for each topic. We do this by simply calling topics_over_time and pass in his tweets, the corresponding timestamps, and the related topics: topics_over_time = model . topics_over_time ( tweets , topics , timestamps ) And that is it! Aside from what you always need for BERTopic, you now only need to add topics and timestamps to quickly calculate the topics over time.","title":"Example"},{"location":"tutorial/topicsovertime/topicsovertime.html#parameters","text":"There are a few parameters that are of interest which will be discussed below.","title":"Parameters"},{"location":"tutorial/topicsovertime/topicsovertime.html#tuning","text":"Both global_tuning and evolutionary_tuning are set to True as a default, but can easily be changed. Perhaps you do not want the representations to be influenced by the global representation and merely see how they evolved over time: topics_over_time = topic_model . topics_over_time ( tweets , topics , timestamps , global_tuning = True , evolution_tuning = True )","title":"Tuning"},{"location":"tutorial/topicsovertime/topicsovertime.html#bins","text":"If you have more than 100 unique timestamps, then there will be topic representations created for each of those timestamps which can negatively affect the topic representations. It is advised to keep the number of unique timestamps below 50. To do this, you can simply set the number of bins that are created when calculating the topic representations. The timestamps will be taken and put into equal-sized bins: topics_over_time = topic_model . topics_over_time ( tweets , topics , timestamps , nr_bins = 20 )","title":"Bins"},{"location":"tutorial/topicsovertime/topicsovertime.html#datetime-format","text":"If you are passing strings (dates) instead of integers, then BERTopic will try to automatically detect which datetime format your strings have. Unfortunately, this will not always work if they are in an unexpected format. We can use datetime_format to pass the format the timestamps have: topics_over_time = topic_model . topics_over_time ( tweets , topics , timestamps , datetime_format = \"%b%M\" )","title":"Datetime format"},{"location":"tutorial/topicsovertime/topicsovertime.html#visualization","text":"To me, DTM becomes truly interesting when you have a good way of visualizing how topics have changed over time. A nice way of doing so is leveraging the interactive abilities of Plotly. Plotly allows us to show the frequency of topics over time whilst giving the option of hovering over the points to show the time-specific topic representations. Simply call visualize_topics_over_time with the newly created topics over time: topic_model . visualize_topics_over_time ( topics_over_time , top_n_topics = 20 ) I used top_n_topics to only show the top 20 most frequent topics. If I were to visualize all topics, which is possible by leaving top_n_topics empty, there is a chance that hundreds of lines will fill the plot. You can also use topics to show specific topics: topic_model . visualize_topics_over_time ( topics_over_time , topics = [ 9 , 10 , 72 , 83 , 87 , 91 ])","title":"Visualization"},{"location":"tutorial/topicsperclass/topicsperclass.html","text":"In some cases, you might be interested in how certain topics are represented over certain categories. Perhaps there are specific groups of users for which you want to see how they talk about certain topics. Instead of running the topic model per class, we can simply create a topic model and then extract, for each topic, its representation per class. This allows you to see how certain topics, calculated over all documents, are represented for certain subgroups. To do so, we use the 20 Newsgroups dataset to see how the topics that we uncover are represented in the 20 categories of documents. First, let's prepare the data: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups data = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' )) docs = data [ \"data\" ] targets = data [ \"target\" ] target_names = data [ \"target_names\" ] classes = [ data [ \"target_names\" ][ i ] for i in data [ \"target\" ]] Next, we want to extract the topics across all documents without taking the categories into account: topic_model = BERTopic ( verbose = True ) topics , probs = topic_model . fit_transform ( docs ) Now that we have created our global topic model, let us calculate the topic representations across each category: topics_per_class = topic_model . topics_per_class ( docs , topics , classes = classes ) The classes variable contains the class for each document. Then, we simply visualize these topics per class: topic_model . visualize_topics_per_class ( topics_per_class , top_n = 10 ) You can hover over the bars to see the topic representation per class. As you can see in the visualization above, the topics 93_homosexual_homosexuality_sex and 58_bike_bikes_motorcycle are somewhat distributed over all classes. You can see that the topic representation between rec.motorcycles and rec.autos in 58_bike_bikes_motorcycle clearly differs from one another. It seems that BERTopic has tried to combine those two categories into a single topic. However, since they do contain two separate topics, the topic representation in those two categories differs. We see something similar for 93_homosexual_homosexuality_sex , where the topic is distributed among several categories and is represented slightly differently. Thus, you can see that although in certain categories the topic is similar, the way the topic is represented can differ.","title":"Topics per Class"},{"location":"tutorial/visualization/visualization.html","text":"Visualize Topics \u00b6 After having trained our BERTopic model, we can iteratively go through hundreds of topics to get a good understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis . We embed our c-TF-IDF representation of the topics in 2D using Umap and then visualize the two dimensions using plotly such that we can create an interactive view. First, we need to train our model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) Then, we simply call topic_model.visualize_topics() in order to visualize our topics. The resulting graph is a plotly interactive graph which can be converted to HTML. Thus, you can play around with the results below: You can use the slider to select the topic which then lights up red. If you hover over a topic, then general information is given about the topic, including the size of the topic and its corresponding words. Visualize Topic Hierarchy \u00b6 The topics that were created can be hierarchically reduced. In order to understand the potential hierarchical structure of the topics, we can use scipy.cluster.hierarchy to create clusters and visualize how they relate to one another. This might help selecting an appropriate nr_topics when reducing the number of topics that you have created. To visualize this hierarchy, simply call topic_model.visualize_hierarchy() : Do note that this is not the actual procedure of .reduce_topics() when nr_topics is set to auto since HDBSCAN is used to automatically extract topics. The visualization above closely resembles the actual procedure of .reduce_topics() when any number of nr_topics is selected. Visualize Terms \u00b6 We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics. Moreover, you can easily compare topic representations to each other. To visualize this hierarchy, simply call topic_model.visualize_barchart() : Visualize Topic Similarity \u00b6 Having generated topic embeddings, through both c-TF-IDF and embeddings, we can create a similarity matrix by simply applying cosine similarities through those topic embeddings. The result will be a matrix indicating how similar certain topics are to each other. To visualize the heatmap, simply call topic_model.visualize_heatmap() : Note that you can set n_clusters in visualize_heatmap to order the topics by their similarity. This will result in blocks being formed in the heatmap indicating which clusters of topics are similar to each other. This step is very much recommended as it will make reading the heatmap easier. Visualize Term Score Decline \u00b6 Topics are represented by a number of words starting with the best representative word. Each word is represented by a c-TF-IDF score. The higher the score, the more representative a word to the topic is. Since the topic words are sorted by their c-TF-IDF score, the scores slowly decline with each word that is added. At some point adding words to the topic representation only marginally increases the total c-TF-IDF score and would not be beneficial for its representation. To visualize this effect, we can plot the c-TF-IDF scores for each topic by the term rank of each word. In other words, the position of the words (term rank), where the words with the highest c-TF-IDF score will have a rank of 1, will be put on the x-axis. Whereas the y-axis will be populated by the c-TF-IDF scores. The result is a visualization that shows you the decline of c-TF-IDF score when adding words to the topic representation. It allows you, using the elbow method, the select the best number of words in a topic. To visualize the c-TF-IDF score decline, simply call topic_model.visualize_term_rank() : To enable the log scale on the y-axis for a better view of individual topics, simply call topic_model.visualize_term_rank(log_scale=True) : This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here . Visualize Topics over Time \u00b6 After creating topics over time with Dynamic Topic Modeling, we can visualize these topics by leveraging the interactive abilities of Plotly. Plotly allows us to show the frequency of topics over time whilst giving the option of hovering over the points to show the time-specific topic representations. Simply call visualize_topics_over_time with the newly created topics over time: import re import pandas as pd from bertopic import BERTopic # Prepare data trump = pd . read_csv ( 'https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6' ) trump . text = trump . apply ( lambda row : re . sub ( r \"http\\S+\" , \"\" , row . text ) . lower (), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( filter ( lambda x : x [ 0 ] != \"@\" , row . text . split ())), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( re . sub ( \"[^a-zA-Z]+\" , \" \" , row . text ) . split ()), 1 ) trump = trump . loc [( trump . isRetweet == \"f\" ) & ( trump . text != \"\" ), :] timestamps = trump . date . to_list () tweets = trump . text . to_list () # Create topics over time model = BERTopic ( verbose = True ) topics , probs = model . fit_transform ( tweets ) topics_over_time = model . topics_over_time ( tweets , topics , timestamps ) Then, we visualize some interesting topics: model . visualize_topics_over_time ( topics_over_time , topics = [ 9 , 10 , 72 , 83 , 87 , 91 ]) Visualize Topics per Class \u00b6 You might want to extract and visualize the topic representation per class. For example, if you have specific groups of users that might approach topics differently, then extracting them would help understanding how these users talk about certain topics. In other words, this is simply creating a topic representation for certain classes that you might have in your data. First, we need to train our model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Prepare data and classes data = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' )) docs = data [ \"data\" ] classes = [ data [ \"target_names\" ][ i ] for i in data [ \"target\" ]] # Create topic model and calculate topics per class topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_per_class = topic_model . topics_per_class ( docs , topics , classes = classes ) Then, we visualize the topic representation of major topics per class: topic_model . visualize_topics_per_class ( topics_per_class ) Visualize Probablities \u00b6 We can also calculate the probabilities of topics found in a document. In order to do so, we have to set calculate_probabilities to True as calculating them can be quite computationally expensive. Then, we use the variable probabilities that is returned from transform() or fit_transform() to understand how confident BERTopic is that certain topics can be found in a document: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic ( calculate_probabilities = True ) topics , probabilities = topic_model . fit_transform ( docs ) To visualize the distributions, we simply call: topic_model . visualize_distribution ( probabilities [ 0 ]) NOTE : The distribution of the probabilities does not give an indication to the distribution of the frequencies of topics across a document. It merely shows how confident BERTopic is that certain topics can be found in a document.","title":"Topic Visualization"},{"location":"tutorial/visualization/visualization.html#visualize-topics","text":"After having trained our BERTopic model, we can iteratively go through hundreds of topics to get a good understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis . We embed our c-TF-IDF representation of the topics in 2D using Umap and then visualize the two dimensions using plotly such that we can create an interactive view. First, we need to train our model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) Then, we simply call topic_model.visualize_topics() in order to visualize our topics. The resulting graph is a plotly interactive graph which can be converted to HTML. Thus, you can play around with the results below: You can use the slider to select the topic which then lights up red. If you hover over a topic, then general information is given about the topic, including the size of the topic and its corresponding words.","title":"Visualize Topics"},{"location":"tutorial/visualization/visualization.html#visualize-topic-hierarchy","text":"The topics that were created can be hierarchically reduced. In order to understand the potential hierarchical structure of the topics, we can use scipy.cluster.hierarchy to create clusters and visualize how they relate to one another. This might help selecting an appropriate nr_topics when reducing the number of topics that you have created. To visualize this hierarchy, simply call topic_model.visualize_hierarchy() : Do note that this is not the actual procedure of .reduce_topics() when nr_topics is set to auto since HDBSCAN is used to automatically extract topics. The visualization above closely resembles the actual procedure of .reduce_topics() when any number of nr_topics is selected.","title":"Visualize Topic Hierarchy"},{"location":"tutorial/visualization/visualization.html#visualize-terms","text":"We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics. Moreover, you can easily compare topic representations to each other. To visualize this hierarchy, simply call topic_model.visualize_barchart() :","title":"Visualize Terms"},{"location":"tutorial/visualization/visualization.html#visualize-topic-similarity","text":"Having generated topic embeddings, through both c-TF-IDF and embeddings, we can create a similarity matrix by simply applying cosine similarities through those topic embeddings. The result will be a matrix indicating how similar certain topics are to each other. To visualize the heatmap, simply call topic_model.visualize_heatmap() : Note that you can set n_clusters in visualize_heatmap to order the topics by their similarity. This will result in blocks being formed in the heatmap indicating which clusters of topics are similar to each other. This step is very much recommended as it will make reading the heatmap easier.","title":"Visualize Topic Similarity"},{"location":"tutorial/visualization/visualization.html#visualize-term-score-decline","text":"Topics are represented by a number of words starting with the best representative word. Each word is represented by a c-TF-IDF score. The higher the score, the more representative a word to the topic is. Since the topic words are sorted by their c-TF-IDF score, the scores slowly decline with each word that is added. At some point adding words to the topic representation only marginally increases the total c-TF-IDF score and would not be beneficial for its representation. To visualize this effect, we can plot the c-TF-IDF scores for each topic by the term rank of each word. In other words, the position of the words (term rank), where the words with the highest c-TF-IDF score will have a rank of 1, will be put on the x-axis. Whereas the y-axis will be populated by the c-TF-IDF scores. The result is a visualization that shows you the decline of c-TF-IDF score when adding words to the topic representation. It allows you, using the elbow method, the select the best number of words in a topic. To visualize the c-TF-IDF score decline, simply call topic_model.visualize_term_rank() : To enable the log scale on the y-axis for a better view of individual topics, simply call topic_model.visualize_term_rank(log_scale=True) : This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here .","title":"Visualize Term Score Decline"},{"location":"tutorial/visualization/visualization.html#visualize-topics-over-time","text":"After creating topics over time with Dynamic Topic Modeling, we can visualize these topics by leveraging the interactive abilities of Plotly. Plotly allows us to show the frequency of topics over time whilst giving the option of hovering over the points to show the time-specific topic representations. Simply call visualize_topics_over_time with the newly created topics over time: import re import pandas as pd from bertopic import BERTopic # Prepare data trump = pd . read_csv ( 'https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6' ) trump . text = trump . apply ( lambda row : re . sub ( r \"http\\S+\" , \"\" , row . text ) . lower (), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( filter ( lambda x : x [ 0 ] != \"@\" , row . text . split ())), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( re . sub ( \"[^a-zA-Z]+\" , \" \" , row . text ) . split ()), 1 ) trump = trump . loc [( trump . isRetweet == \"f\" ) & ( trump . text != \"\" ), :] timestamps = trump . date . to_list () tweets = trump . text . to_list () # Create topics over time model = BERTopic ( verbose = True ) topics , probs = model . fit_transform ( tweets ) topics_over_time = model . topics_over_time ( tweets , topics , timestamps ) Then, we visualize some interesting topics: model . visualize_topics_over_time ( topics_over_time , topics = [ 9 , 10 , 72 , 83 , 87 , 91 ])","title":"Visualize Topics over Time"},{"location":"tutorial/visualization/visualization.html#visualize-topics-per-class","text":"You might want to extract and visualize the topic representation per class. For example, if you have specific groups of users that might approach topics differently, then extracting them would help understanding how these users talk about certain topics. In other words, this is simply creating a topic representation for certain classes that you might have in your data. First, we need to train our model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Prepare data and classes data = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' )) docs = data [ \"data\" ] classes = [ data [ \"target_names\" ][ i ] for i in data [ \"target\" ]] # Create topic model and calculate topics per class topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_per_class = topic_model . topics_per_class ( docs , topics , classes = classes ) Then, we visualize the topic representation of major topics per class: topic_model . visualize_topics_per_class ( topics_per_class )","title":"Visualize Topics per Class"},{"location":"tutorial/visualization/visualization.html#visualize-probablities","text":"We can also calculate the probabilities of topics found in a document. In order to do so, we have to set calculate_probabilities to True as calculating them can be quite computationally expensive. Then, we use the variable probabilities that is returned from transform() or fit_transform() to understand how confident BERTopic is that certain topics can be found in a document: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic ( calculate_probabilities = True ) topics , probabilities = topic_model . fit_transform ( docs ) To visualize the distributions, we simply call: topic_model . visualize_distribution ( probabilities [ 0 ]) NOTE : The distribution of the probabilities does not give an indication to the distribution of the frequencies of topics across a document. It merely shows how confident BERTopic is that certain topics can be found in a document.","title":"Visualize Probablities"}]}