{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"BERTopic \u00b6 BERTopic is a topic modeling technique that leverages \ud83e\udd17 transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. It even supports visualizations similar to LDAvis! Corresponding medium post can be found here . Installation \u00b6 Installation can be done using pypi : pip install bertopic To use the visualization options, install BERTopic as follows: pip install bertopic [ visualization ] Usage \u00b6 Below is an example of how to use the model. The example uses the 20 newsgroups dataset. You can also follow along with the Google Colab notebook here . from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probabilities = model . fit_transform ( docs ) After generating topics and their probabilities, we can access the frequent topics that were generated: >>> model . get_topic_freq () . head () Topic Count - 1 7288 49 3992 30 701 27 684 11 568 -1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated, topic 49 : >>> model . get_topic ( 49 ) [( 'windows' , 0.006152228076250982 ), ( 'drive' , 0.004982897610645755 ), ( 'dos' , 0.004845038866360651 ), ( 'file' , 0.004140142872194834 ), ( 'disk' , 0.004131678774810884 ), ( 'mac' , 0.003624848635985097 ), ( 'memory' , 0.0034840976976789903 ), ( 'software' , 0.0034415334250699077 ), ( 'email' , 0.0034239554442333257 ), ( 'pc' , 0.003047105930670237 )] Supported Languages Use \"multilingual\" to select a model that supports 50+ languages. Moreover, the following languages are supported: Afrikaans, Albanian, Amharic, Arabic, Armenian, Assamese, Azerbaijani, Basque, Belarusian, Bengali, Bengali Romanize, Bosnian, Breton, Bulgarian, Burmese, Burmese zawgyi font, Catalan, Chinese (Simplified), Chinese (Traditional), Croatian, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Hausa, Hebrew, Hindi, Hindi Romanize, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish (Kurmanji), Kyrgyz, Lao, Latin, Latvian, Lithuanian, Macedonian, Malagasy, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian, Oriya, Oromo, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Sanskrit, Scottish Gaelic, Serbian, Sindhi, Sinhala, Slovak, Slovenian, Somali, Spanish, Sundanese, Swahili, Swedish, Tamil, Tamil Romanize, Telugu, Telugu Romanize, Thai, Turkish, Ukrainian, Urdu, Urdu Romanize, Uyghur, Uzbek, Vietnamese, Welsh, Western Frisian, Xhosa, Yiddish Overview \u00b6 Methods Code Fit the model model.fit(docs]) Fit the model and predict documents model.fit_transform(docs]) Predict new documents model.transform([new_doc]) Access single topic model.get_topic(12) Access all topics model.get_topics() Get topic freq model.get_topic_freq() Visualize Topics model.visualize_topics() Visualize Topic Probability Distribution model.visualize_distribution(probabilities[0]) Update topic representation model.update_topics(docs, topics, n_gram_range=(1, 3)) Reduce nr of topics model.reduce_topics(docs, topics, probabilities, nr_topics=30) Find topics model.find_topics(\"vehicle\") Save model model.save(\"my_model\") Load model BERTopic.load(\"my_model\")","title":"Home"},{"location":"index.html#bertopic","text":"BERTopic is a topic modeling technique that leverages \ud83e\udd17 transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. It even supports visualizations similar to LDAvis! Corresponding medium post can be found here .","title":"BERTopic"},{"location":"index.html#installation","text":"Installation can be done using pypi : pip install bertopic To use the visualization options, install BERTopic as follows: pip install bertopic [ visualization ]","title":"Installation"},{"location":"index.html#usage","text":"Below is an example of how to use the model. The example uses the 20 newsgroups dataset. You can also follow along with the Google Colab notebook here . from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probabilities = model . fit_transform ( docs ) After generating topics and their probabilities, we can access the frequent topics that were generated: >>> model . get_topic_freq () . head () Topic Count - 1 7288 49 3992 30 701 27 684 11 568 -1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated, topic 49 : >>> model . get_topic ( 49 ) [( 'windows' , 0.006152228076250982 ), ( 'drive' , 0.004982897610645755 ), ( 'dos' , 0.004845038866360651 ), ( 'file' , 0.004140142872194834 ), ( 'disk' , 0.004131678774810884 ), ( 'mac' , 0.003624848635985097 ), ( 'memory' , 0.0034840976976789903 ), ( 'software' , 0.0034415334250699077 ), ( 'email' , 0.0034239554442333257 ), ( 'pc' , 0.003047105930670237 )] Supported Languages Use \"multilingual\" to select a model that supports 50+ languages. Moreover, the following languages are supported: Afrikaans, Albanian, Amharic, Arabic, Armenian, Assamese, Azerbaijani, Basque, Belarusian, Bengali, Bengali Romanize, Bosnian, Breton, Bulgarian, Burmese, Burmese zawgyi font, Catalan, Chinese (Simplified), Chinese (Traditional), Croatian, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Hausa, Hebrew, Hindi, Hindi Romanize, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish (Kurmanji), Kyrgyz, Lao, Latin, Latvian, Lithuanian, Macedonian, Malagasy, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian, Oriya, Oromo, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Sanskrit, Scottish Gaelic, Serbian, Sindhi, Sinhala, Slovak, Slovenian, Somali, Spanish, Sundanese, Swahili, Swedish, Tamil, Tamil Romanize, Telugu, Telugu Romanize, Thai, Turkish, Ukrainian, Urdu, Urdu Romanize, Uyghur, Uzbek, Vietnamese, Welsh, Western Frisian, Xhosa, Yiddish","title":"Usage"},{"location":"index.html#overview","text":"Methods Code Fit the model model.fit(docs]) Fit the model and predict documents model.fit_transform(docs]) Predict new documents model.transform([new_doc]) Access single topic model.get_topic(12) Access all topics model.get_topics() Get topic freq model.get_topic_freq() Visualize Topics model.visualize_topics() Visualize Topic Probability Distribution model.visualize_distribution(probabilities[0]) Update topic representation model.update_topics(docs, topics, n_gram_range=(1, 3)) Reduce nr of topics model.reduce_topics(docs, topics, probabilities, nr_topics=30) Find topics model.find_topics(\"vehicle\") Save model model.save(\"my_model\") Load model BERTopic.load(\"my_model\")","title":"Overview"},{"location":"changelog.html","text":"Version 0.4.0 \u00b6 Release date: 21 December, 2020 Highlights : Visualize Topics similar to LDAvis Added option to reduce topics after training Added option to update topic representation after training Added option to search topics using a search term Significantly improved the stability of generating clusters Finetune the topic words by selecting the most coherent words with the highest c-TF-IDF values More extensive tutorials in the documentation Notable Changes : Option to select language instead of sentence-transformers models to minimize the complexity of using BERTopic Improved logging (remove duplicates) Check if BERTopic is fitted Added TF-IDF as an embedder instead of transformer models (see tutorial) Numpy for Python 3.6 will be dropped and was therefore removed from the workflow. Preprocess text before passing it through c-TF-IDF Merged get_topics_freq() with get_topic_freq() Fixes : Fix error handling topic probabilities Version 0.3.2 \u00b6 Release date: 16 November, 2020 Highlights : Fixed a bug with the topic reduction method that seems to reduce the number of topics but not to the nr_topics as defined in the class. Since this was, to a certain extend, breaking the topic reduction method a new release was necessary. Version 0.3.1 \u00b6 Release date: 4 November, 2020 Highlights : Adding the option to use custom embeddings or embeddings that you generated beforehand with whatever package you'd like to use. This allows users to further customize BERTopic to their liking. Version 0.3.0 \u00b6 Release date: 29 October, 2020 Highlights : transform() and fit_transform() now also return the topic probability distributions Added visualize_distribution() which visualizes the topic probability distribution for a single document Version 0.2.2 \u00b6 Release date: 17 October, 2020 Highlights : Fixed n_gram_range not being used Added option for using stopwords Version 0.2.1 \u00b6 Release date: 11 October, 2020 Highlights : Improved the calculation of the class-based TF-IDF procedure by limiting the calculation to sparse matrices. This prevents out-of-memory problems when faced with large datasets. Version 0.2.0 \u00b6 Release date: 11 October, 2020 Highlights : Changed c-TF-IDF procedure such that it implements a version of scikit-learns procedure. This should also speed up the calculation of the sparse matrix and prevent memory errors. Added automated unit tests Version 0.1.2 \u00b6 Release date: 1 October, 2020 Highlights : When transforming new documents, self.mapped_topics seemed to be missing. Added to the init. Version 0.1.1 \u00b6 Release date: 24 September, 2020 Highlights : Fixed requirements --> Issue with pytorch Update documentation Version 0.1.0 \u00b6 Release date: 24 September, 2020 Highlights : First release of BERTopic Added parameters for UMAP and HDBSCAN Option to choose sentence-transformer model Method for transforming unseen documents Save and load trained models (UMAP and HDBSCAN) Extract topics and their sizes Notable Changes : Optimized c-TF-IDF Improved documentation Improved topic reduction","title":"Changelog"},{"location":"changelog.html#version-040","text":"Release date: 21 December, 2020 Highlights : Visualize Topics similar to LDAvis Added option to reduce topics after training Added option to update topic representation after training Added option to search topics using a search term Significantly improved the stability of generating clusters Finetune the topic words by selecting the most coherent words with the highest c-TF-IDF values More extensive tutorials in the documentation Notable Changes : Option to select language instead of sentence-transformers models to minimize the complexity of using BERTopic Improved logging (remove duplicates) Check if BERTopic is fitted Added TF-IDF as an embedder instead of transformer models (see tutorial) Numpy for Python 3.6 will be dropped and was therefore removed from the workflow. Preprocess text before passing it through c-TF-IDF Merged get_topics_freq() with get_topic_freq() Fixes : Fix error handling topic probabilities","title":"Version 0.4.0"},{"location":"changelog.html#version-032","text":"Release date: 16 November, 2020 Highlights : Fixed a bug with the topic reduction method that seems to reduce the number of topics but not to the nr_topics as defined in the class. Since this was, to a certain extend, breaking the topic reduction method a new release was necessary.","title":"Version 0.3.2"},{"location":"changelog.html#version-031","text":"Release date: 4 November, 2020 Highlights : Adding the option to use custom embeddings or embeddings that you generated beforehand with whatever package you'd like to use. This allows users to further customize BERTopic to their liking.","title":"Version 0.3.1"},{"location":"changelog.html#version-030","text":"Release date: 29 October, 2020 Highlights : transform() and fit_transform() now also return the topic probability distributions Added visualize_distribution() which visualizes the topic probability distribution for a single document","title":"Version 0.3.0"},{"location":"changelog.html#version-022","text":"Release date: 17 October, 2020 Highlights : Fixed n_gram_range not being used Added option for using stopwords","title":"Version 0.2.2"},{"location":"changelog.html#version-021","text":"Release date: 11 October, 2020 Highlights : Improved the calculation of the class-based TF-IDF procedure by limiting the calculation to sparse matrices. This prevents out-of-memory problems when faced with large datasets.","title":"Version 0.2.1"},{"location":"changelog.html#version-020","text":"Release date: 11 October, 2020 Highlights : Changed c-TF-IDF procedure such that it implements a version of scikit-learns procedure. This should also speed up the calculation of the sparse matrix and prevent memory errors. Added automated unit tests","title":"Version 0.2.0"},{"location":"changelog.html#version-012","text":"Release date: 1 October, 2020 Highlights : When transforming new documents, self.mapped_topics seemed to be missing. Added to the init.","title":"Version 0.1.2"},{"location":"changelog.html#version-011","text":"Release date: 24 September, 2020 Highlights : Fixed requirements --> Issue with pytorch Update documentation","title":"Version 0.1.1"},{"location":"changelog.html#version-010","text":"Release date: 24 September, 2020 Highlights : First release of BERTopic Added parameters for UMAP and HDBSCAN Option to choose sentence-transformer model Method for transforming unseen documents Save and load trained models (UMAP and HDBSCAN) Extract topics and their sizes Notable Changes : Optimized c-TF-IDF Improved documentation Improved topic reduction","title":"Version 0.1.0"},{"location":"api/bertopic.html","text":"BERTopic \u00b6 \u00b6 BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics = model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( verbose = True ) topics = model . fit_transform ( docs , embeddings ) Due to the stochastisch nature of UMAP, the results from BERTopic might differ and the quality can degrade. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best. __init__ ( self , language = 'english' , embedding_model = None , top_n_words = 10 , nr_topics = None , n_gram_range = ( 1 , 1 ), min_topic_size = 10 , n_neighbors = 15 , n_components = 5 , stop_words = None , verbose = False , vectorizer = None , calculate_probabilities = True , allow_st_model = True ) special \u00b6 BERTopic initialization Parameters: Name Type Description Default language str The main language used in your documents. For a full overview of supported languages see bertopic.embeddings.languages. Select \"multilingual\" to load in a model that support 50+ languages. 'english' embedding_model str Model to use. Overview of options can be found here https://www.sbert.net/docs/pretrained_models.html None top_n_words int The number of words per topic to extract 10 nr_topics Union[int, str] Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. IF this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that have a similarity of at least 0.9, do not maps all others. None n_gram_range Tuple[int, int] The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. Note that this will not be used if you pass in your own CountVectorizer. (1, 1) min_topic_size int The minimum size of the topic. 10 n_neighbors int The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation (UMAP). 15 n_components int The dimension of the space to embed into when reducing dimensionality with UMAP. 5 stop_words Union[str, List[str]] Stopwords that can be used as either a list of strings, or the name of the language as a string. For example: 'english' or ['the', 'and', 'I']. Note that this will not be used if you pass in your own CountVectorizer. None verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False vectorizer CountVectorizer Pass in your own CountVectorizer from scikit-learn None calculate_probabilities bool Whether to calculate the topic probabilities. This could slow down extraction of topics if you have many documents (>100_000). If so, set this to False to increase speed. True allow_st_model bool This allows BERTopic to use a multi-lingual version of SentenceTransformer to be used to fine-tune the topic words extracted from the c-TF-IDF representation. Moreover, it will allow you to search for topics based on search queries. True Usage: from bertopic import BERTopic model = BERTopic ( language = \"english\" , embedding_model = None , top_n_words = 10 , nr_topics = 30 , n_gram_range = ( 1 , 1 ), min_topic_size = 10 , n_neighbors = 15 , n_components = 5 , stop_words = None , verbose = True , vectorizer = None , allow_st_model = True ) Source code in bertopic\\_bertopic.py def __init__ ( self , language : str = \"english\" , embedding_model : str = None , top_n_words : int = 10 , nr_topics : Union [ int , str ] = None , n_gram_range : Tuple [ int , int ] = ( 1 , 1 ), min_topic_size : int = 10 , n_neighbors : int = 15 , n_components : int = 5 , stop_words : Union [ str , List [ str ]] = None , verbose : bool = False , vectorizer : CountVectorizer = None , calculate_probabilities : bool = True , allow_st_model : bool = True ): \"\"\"BERTopic initialization Args: language: The main language used in your documents. For a full overview of supported languages see bertopic.embeddings.languages. Select \"multilingual\" to load in a model that support 50+ languages. embedding_model: Model to use. Overview of options can be found here https://www.sbert.net/docs/pretrained_models.html top_n_words: The number of words per topic to extract nr_topics: Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. IF this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that have a similarity of at least 0.9, do not maps all others. n_gram_range: The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. Note that this will not be used if you pass in your own CountVectorizer. min_topic_size: The minimum size of the topic. n_neighbors: The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation (UMAP). n_components: The dimension of the space to embed into when reducing dimensionality with UMAP. stop_words: Stopwords that can be used as either a list of strings, or the name of the language as a string. For example: 'english' or ['the', 'and', 'I']. Note that this will not be used if you pass in your own CountVectorizer. verbose: Changes the verbosity of the model, Set to True if you want to track the stages of the model. vectorizer: Pass in your own CountVectorizer from scikit-learn calculate_probabilities: Whether to calculate the topic probabilities. This could slow down extraction of topics if you have many documents (>100_000). If so, set this to False to increase speed. allow_st_model: This allows BERTopic to use a multi-lingual version of SentenceTransformer to be used to fine-tune the topic words extracted from the c-TF-IDF representation. Moreover, it will allow you to search for topics based on search queries. Usage: ```python from bertopic import BERTopic model = BERTopic(language = \"english\", embedding_model = None, top_n_words = 10, nr_topics = 30, n_gram_range = (1, 1), min_topic_size = 10, n_neighbors = 15, n_components = 5, stop_words = None, verbose = True, vectorizer = None, allow_st_model = True) ``` \"\"\" # Embedding model self . language = language self . embedding_model = embedding_model self . allow_st_model = allow_st_model # Topic-based parameters if top_n_words > 30 : raise ValueError ( \"top_n_words should be lower or equal to 30. The preferred value is 10.\" ) self . top_n_words = top_n_words self . nr_topics = nr_topics self . min_topic_size = min_topic_size self . calculate_probabilities = calculate_probabilities # Umap parameters self . n_neighbors = n_neighbors self . n_components = n_components # Vectorizer parameters self . stop_words = stop_words self . n_gram_range = n_gram_range self . vectorizer = vectorizer or CountVectorizer ( ngram_range = self . n_gram_range , stop_words = self . stop_words ) self . umap_model = None self . cluster_model = None self . topics = None self . topic_sizes = None self . reduced_topics_mapped = None self . mapped_topics = None self . topic_embeddings = None self . topic_sim_matrix = None self . custom_embeddings = False if verbose : logger . set_level ( \"DEBUG\" ) find_topics ( self , search_term , top_n = 5 ) \u00b6 Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Parameters: Name Type Description Default search_term str the term you want to use to search for topics required top_n int the number of topics to return 5 Returns: Type Description Tuple[List[int], List[float]] similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low Source code in bertopic\\_bertopic.py def find_topics ( self , search_term : str , top_n : int = 5 ) -> Tuple [ List [ int ], List [ float ]]: \"\"\" Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Args: search_term: the term you want to use to search for topics top_n: the number of topics to return Returns: similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low \"\"\" if self . custom_embeddings and not self . allow_st_model : raise Exception ( \"This method can only be used if you set `allow_st_model` to True when \" \"using custom embeddings.\" ) topic_list = list ( self . topics . keys ()) topic_list . sort () # Extract search_term embeddings and compare with topic embeddings search_embedding = self . _extract_embeddings ([ search_term ]) . flatten () sims = cosine_similarity ( search_embedding . reshape ( 1 , - 1 ), self . topic_embeddings ) . flatten () # Extract topics most similar to search_term ids = np . argsort ( sims )[ - top_n :] similarity = [ sims [ i ] for i in ids ][:: - 1 ] similar_topics = [ topic_list [ index ] for index in ids ][:: - 1 ] return similar_topics , similarity fit ( self , documents , embeddings = None ) \u00b6 Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Parameters: Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) . fit ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) . fit ( docs , embeddings ) Source code in bertopic\\_bertopic.py def fit ( self , documents : List [ str ], embeddings : np . ndarray = None ): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True).fit(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True).fit(docs, embeddings) ``` \"\"\" self . fit_transform ( documents , embeddings ) return self fit_transform ( self , documents , embeddings = None ) \u00b6 Fit the models on a collection of documents, generate topics, and return the docs with topics Parameters: Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None Returns: Type Description Tuple[List[int], Optional[numpy.ndarray]] predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics = model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) topics = model . fit_transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py def fit_transform ( self , documents : List [ str ], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], Union [ np . ndarray , None ]]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True) topics = model.fit_transform(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True) topics = model.fit_transform(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) check_embeddings_shape ( embeddings , documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if not any ([ isinstance ( embeddings , np . ndarray ), isinstance ( embeddings , csr_matrix )]): embeddings = self . _extract_embeddings ( documents . Document ) else : self . custom_embeddings = True # Reduce dimensionality with UMAP umap_embeddings = self . _reduce_dimensionality ( embeddings ) # Cluster UMAP embeddings with HDBSCAN documents , probabilities = self . _cluster_embeddings ( umap_embeddings , documents ) # Extract topics by calculating c-TF-IDF self . _extract_topics ( documents ) if self . nr_topics : documents = self . _reduce_topics ( documents ) probabilities = self . _map_probabilities ( probabilities ) predictions = documents . Topic . to_list () return predictions , probabilities get_topic ( self , topic ) \u00b6 Return top n words for a specific topic and their c-TF-IDF scores Usage: topic = model . get_topic ( 12 ) Source code in bertopic\\_bertopic.py def get_topic ( self , topic : int ) -> Union [ Dict [ str , Tuple [ str , float ]], bool ]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores Usage: ```python topic = model.get_topic(12) ``` \"\"\" check_is_fitted ( self ) if self . topics . get ( topic ): return self . topics [ topic ] else : return False get_topic_freq ( self , topic = None ) \u00b6 Return the the size of topics (descending order) Usage: To extract the frequency of all topics: frequency = model . get_topic_freq () To get the frequency of a single topic: frequency = model . get_topic_freq ( 12 ) Source code in bertopic\\_bertopic.py def get_topic_freq ( self , topic : int = None ) -> Union [ pd . DataFrame , int ]: \"\"\" Return the the size of topics (descending order) Usage: To extract the frequency of all topics: ```python frequency = model.get_topic_freq() ``` To get the frequency of a single topic: ```python frequency = model.get_topic_freq(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . topic_sizes [ topic ] else : return pd . DataFrame ( self . topic_sizes . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False ) get_topics ( self ) \u00b6 Return topics with top n words and their c-TF-IDF score Usage: all_topics = model . get_topics () Source code in bertopic\\_bertopic.py def get_topics ( self ) -> Dict [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score Usage: ```python all_topics = model.get_topics() ``` \"\"\" check_is_fitted ( self ) return self . topics load ( path ) classmethod \u00b6 Loads the model from the specified path Parameters: Name Type Description Default path str the location and name of the BERTopic file you want to load required Usage: BERTopic . load ( \"my_model\" ) Source code in bertopic\\_bertopic.py @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the BERTopic file you want to load Usage: ```python BERTopic.load(\"my_model\") ``` \"\"\" with open ( path , 'rb' ) as file : return joblib . load ( file ) reduce_topics ( self , docs , topics , probabilities = None , nr_topics = 20 ) \u00b6 Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. The reasoning for putting docs , topics , and probs as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic instead of a dedicated database. Parameters: Name Type Description Default docs List[str] The docs you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required nr_topics int The number of topics you want reduced to 20 probabilities ndarray The probabilities that were returned when calling either fit or fit_transform None Returns: Type Description Tuple[List[int], numpy.ndarray] new_topics: Updated topics new_probabilities: Updated probabilities Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics -> Typically over 50 topics docs = fetch_20newsgroups ( subset = 'train' )[ 'data' ] model = BERTopic () topics , probs = model . fit_transform ( docs ) # Further reduce topics new_topics , new_probs = model . reduce_topics ( docs , topics , probs , nr_topics = 30 ) Source code in bertopic\\_bertopic.py def reduce_topics ( self , docs : List [ str ], topics : List [ int ], probabilities : np . ndarray = None , nr_topics : int = 20 ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. The reasoning for putting `docs`, `topics`, and `probs` as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic instead of a dedicated database. Arguments: docs: The docs you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` nr_topics: The number of topics you want reduced to probabilities: The probabilities that were returned when calling either `fit` or `fit_transform` Returns: new_topics: Updated topics new_probabilities: Updated probabilities Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics -> Typically over 50 topics docs = fetch_20newsgroups(subset='train')['data'] model = BERTopic() topics, probs = model.fit_transform(docs) # Further reduce topics new_topics, new_probs = model.reduce_topics(docs, topics, probs, nr_topics=30) ``` \"\"\" check_is_fitted ( self ) self . nr_topics = nr_topics documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) # Reduce number of topics self . _extract_topics ( documents ) documents = self . _reduce_topics ( documents ) new_topics = documents . Topic . to_list () new_probabilities = self . _map_probabilities ( probabilities ) return new_topics , new_probabilities save ( self , path ) \u00b6 Saves the model to the specified path Parameters: Name Type Description Default path str the location and name of the file you want to save required Usage: model . save ( \"my_model\" ) Source code in bertopic\\_bertopic.py def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save Usage: ```python model.save(\"my_model\") ``` \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file ) transform ( self , documents , embeddings = None ) \u00b6 After having fit a model, use transform to predict new instances Parameters: Name Type Description Default documents Union[str, List[str]] A single document or a list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model. None Returns: Type Description Tuple[List[int], numpy.ndarray] predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) . fit ( docs ) topics = model . transform ( docs ) If you want to use your own embeddings: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) . fit ( docs , embeddings ) topics = model . transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py def transform ( self , documents : Union [ str , List [ str ]], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" After having fit a model, use transform to predict new instances Arguments: documents: A single document or a list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True).fit(docs) topics = model.transform(docs) ``` If you want to use your own embeddings: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True).fit(docs, embeddings) topics = model.transform(docs, embeddings) ``` \"\"\" check_is_fitted ( self ) check_embeddings_shape ( embeddings , documents ) if isinstance ( documents , str ): documents = [ documents ] if not isinstance ( embeddings , np . ndarray ): embeddings = self . _extract_embeddings ( documents ) umap_embeddings = self . umap_model . transform ( embeddings ) predictions , _ = hdbscan . approximate_predict ( self . cluster_model , umap_embeddings ) if self . calculate_probabilities : probabilities = hdbscan . membership_vector ( self . cluster_model , umap_embeddings ) if len ( documents ) == 1 : probabilities = probabilities . flatten () else : probabilities = None if self . mapped_topics : predictions = self . _map_predictions ( predictions ) probabilities = self . _map_probabilities ( probabilities ) return predictions , probabilities update_topics ( self , docs , topics , n_gram_range = None , stop_words = None , vectorizer = None ) \u00b6 Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Parameters: Name Type Description Default docs List[str] The docs you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required n_gram_range Tuple[int, int] The n-gram range for the CountVectorizer. None stop_words str Stopwords that can be used as either a list of strings, or the name of the language as a string. For example: 'english' or ['the', 'and', 'I']. Note that this will not be used if you pass in your own CountVectorizer. None vectorizer CountVectorizer Pass in your own CountVectorizer from scikit-learn None Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'train' )[ 'data' ] model = BERTopic ( n_gram_range = ( 1 , 1 ), stop_words = None ) topics , probs = model . fit_transform ( docs ) # Update topic representation model . update_topics ( docs , topics , n_gram_range = ( 2 , 3 ), stop_words = \"english\" ) Source code in bertopic\\_bertopic.py def update_topics ( self , docs : List [ str ], topics : List [ int ], n_gram_range : Tuple [ int , int ] = None , stop_words : str = None , vectorizer : CountVectorizer = None ): \"\"\" Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Args: docs: The docs you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` n_gram_range: The n-gram range for the CountVectorizer. stop_words: Stopwords that can be used as either a list of strings, or the name of the language as a string. For example: 'english' or ['the', 'and', 'I']. Note that this will not be used if you pass in your own CountVectorizer. vectorizer: Pass in your own CountVectorizer from scikit-learn Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups(subset='train')['data'] model = BERTopic(n_gram_range=(1, 1), stop_words=None) topics, probs = model.fit_transform(docs) # Update topic representation model.update_topics(docs, topics, n_gram_range=(2, 3), stop_words=\"english\") ``` \"\"\" check_is_fitted ( self ) if not n_gram_range : n_gram_range = self . n_gram_range if not stop_words : stop_words = self . stop_words self . vectorizer = vectorizer or CountVectorizer ( ngram_range = n_gram_range , stop_words = stop_words ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) self . _extract_topics ( documents ) visualize_distribution ( self , probabilities , min_probability = 0.015 , figsize = ( 10 , 5 ), save = False ) \u00b6 Visualize the distribution of topic probabilities Parameters: Name Type Description Default probabilities ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 figsize tuple The size of the figure (10, 5) save bool Whether to save the resulting graph to probility.png False Usage: Make sure to fit the model before and only input the probabilities of a single document: model . visualize_distribution ( probabilities [ 0 ]) Source code in bertopic\\_bertopic.py def visualize_distribution ( self , probabilities : np . ndarray , min_probability : float = 0.015 , figsize : tuple = ( 10 , 5 ), save : bool = False ): \"\"\" Visualize the distribution of topic probabilities Arguments: probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. figsize: The size of the figure save: Whether to save the resulting graph to probility.png Usage: Make sure to fit the model before and only input the probabilities of a single document: ```python model.visualize_distribution(probabilities[0]) ``` ![](../img/probabilities.png) \"\"\" check_is_fitted ( self ) if not _HAS_VIZ : raise ModuleNotFoundError ( f \"In order to use this function you'll need to install \" f \"additional dependencies; \\n pip install bertopic[visualization]\" ) if len ( probabilities [ probabilities > min_probability ]) == 0 : raise ValueError ( \"There are no values where `min_probability` is higher than the \" \"probabilities that were supplied. Lower `min_probability` to prevent this error.\" ) if not self . calculate_probabilities : raise ValueError ( \"This visualization cannot be used if you have set `calculate_probabilities` to False \" \"as it uses the topic probabilities. \" ) # Get values and indices equal or exceed the minimum probability labels_idx = np . argwhere ( probabilities >= min_probability ) . flatten () vals = probabilities [ labels_idx ] . tolist () # Create labels labels = [] for idx in labels_idx : label = [] words = self . get_topic ( idx ) if words : for word in words [: 5 ]: label . append ( word [ 0 ]) label = str ( r \"$\\bf{Topic }$ \" + r \"$\\bf{\" + str ( idx ) + \":}$ \" + \" \" . join ( label )) labels . append ( label ) else : print ( idx , probabilities [ idx ]) vals . remove ( probabilities [ idx ]) pos = range ( len ( vals )) # Create figure fig , ax = plt . subplots ( figsize = figsize ) plt . hlines ( y = pos , xmin = 0 , xmax = vals , color = '#333F4B' , alpha = 0.2 , linewidth = 15 ) plt . hlines ( y = np . argmax ( vals ), xmin = 0 , xmax = max ( vals ), color = '#333F4B' , alpha = 1 , linewidth = 15 ) # Set ticks and labels ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) ax . set_xlabel ( 'Probability' , fontsize = 15 , fontweight = 'black' , color = '#333F4B' ) ax . set_ylabel ( '' ) plt . yticks ( pos , labels ) fig . text ( 0 , 1 , 'Topic Probability Distribution' , fontsize = 15 , fontweight = 'black' , color = '#333F4B' ) # Update spine style ax . spines [ 'right' ] . set_visible ( False ) ax . spines [ 'top' ] . set_visible ( False ) ax . spines [ 'left' ] . set_bounds ( pos [ 0 ], pos [ - 1 ]) ax . spines [ 'bottom' ] . set_bounds ( 0 , max ( vals )) ax . spines [ 'bottom' ] . set_position (( 'axes' , - 0.02 )) ax . spines [ 'left' ] . set_position (( 'axes' , 0.02 )) fig . tight_layout () if save : fig . savefig ( \"probability.png\" , dpi = 300 , bbox_inches = 'tight' ) visualize_topics ( self ) \u00b6 Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Source code in bertopic\\_bertopic.py def visualize_topics ( self ): \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. \"\"\" check_is_fitted ( self ) if not _HAS_VIZ : raise ModuleNotFoundError ( f \"In order to use this function you'll need to install \" f \"additional dependencies; \\n pip install bertopic[visualization]\" ) # Extract topic words and their frequencies topic_list = sorted ( list ( self . topics . keys ())) frequencies = [ self . topic_sizes [ topic ] for topic in topic_list ] words = [ \" | \" . join ([ word [ 0 ] for word in self . get_topic ( topic )[: 5 ]]) for topic in topic_list ] # Embed c-TF-IDF into 2D embeddings = MinMaxScaler () . fit_transform ( self . c_tf_idf . toarray ()) embeddings = umap . UMAP ( n_neighbors = 2 , n_components = 2 , metric = 'hellinger' ) . fit_transform ( embeddings ) # Visualize with plotly df = pd . DataFrame ({ \"x\" : embeddings [ 1 :, 0 ], \"y\" : embeddings [ 1 :, 1 ], \"Topic\" : topic_list [ 1 :], \"Words\" : words [ 1 :], \"Size\" : frequencies [ 1 :]}) self . _plotly_topic_visualization ( df , topic_list )","title":"BERTopic"},{"location":"api/bertopic.html#bertopic","text":"","title":"BERTopic"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic","text":"BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics = model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( verbose = True ) topics = model . fit_transform ( docs , embeddings ) Due to the stochastisch nature of UMAP, the results from BERTopic might differ and the quality can degrade. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best.","title":"bertopic._bertopic.BERTopic"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.__init__","text":"BERTopic initialization Parameters: Name Type Description Default language str The main language used in your documents. For a full overview of supported languages see bertopic.embeddings.languages. Select \"multilingual\" to load in a model that support 50+ languages. 'english' embedding_model str Model to use. Overview of options can be found here https://www.sbert.net/docs/pretrained_models.html None top_n_words int The number of words per topic to extract 10 nr_topics Union[int, str] Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. IF this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that have a similarity of at least 0.9, do not maps all others. None n_gram_range Tuple[int, int] The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. Note that this will not be used if you pass in your own CountVectorizer. (1, 1) min_topic_size int The minimum size of the topic. 10 n_neighbors int The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation (UMAP). 15 n_components int The dimension of the space to embed into when reducing dimensionality with UMAP. 5 stop_words Union[str, List[str]] Stopwords that can be used as either a list of strings, or the name of the language as a string. For example: 'english' or ['the', 'and', 'I']. Note that this will not be used if you pass in your own CountVectorizer. None verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False vectorizer CountVectorizer Pass in your own CountVectorizer from scikit-learn None calculate_probabilities bool Whether to calculate the topic probabilities. This could slow down extraction of topics if you have many documents (>100_000). If so, set this to False to increase speed. True allow_st_model bool This allows BERTopic to use a multi-lingual version of SentenceTransformer to be used to fine-tune the topic words extracted from the c-TF-IDF representation. Moreover, it will allow you to search for topics based on search queries. True Usage: from bertopic import BERTopic model = BERTopic ( language = \"english\" , embedding_model = None , top_n_words = 10 , nr_topics = 30 , n_gram_range = ( 1 , 1 ), min_topic_size = 10 , n_neighbors = 15 , n_components = 5 , stop_words = None , verbose = True , vectorizer = None , allow_st_model = True ) Source code in bertopic\\_bertopic.py def __init__ ( self , language : str = \"english\" , embedding_model : str = None , top_n_words : int = 10 , nr_topics : Union [ int , str ] = None , n_gram_range : Tuple [ int , int ] = ( 1 , 1 ), min_topic_size : int = 10 , n_neighbors : int = 15 , n_components : int = 5 , stop_words : Union [ str , List [ str ]] = None , verbose : bool = False , vectorizer : CountVectorizer = None , calculate_probabilities : bool = True , allow_st_model : bool = True ): \"\"\"BERTopic initialization Args: language: The main language used in your documents. For a full overview of supported languages see bertopic.embeddings.languages. Select \"multilingual\" to load in a model that support 50+ languages. embedding_model: Model to use. Overview of options can be found here https://www.sbert.net/docs/pretrained_models.html top_n_words: The number of words per topic to extract nr_topics: Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. IF this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that have a similarity of at least 0.9, do not maps all others. n_gram_range: The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. Note that this will not be used if you pass in your own CountVectorizer. min_topic_size: The minimum size of the topic. n_neighbors: The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation (UMAP). n_components: The dimension of the space to embed into when reducing dimensionality with UMAP. stop_words: Stopwords that can be used as either a list of strings, or the name of the language as a string. For example: 'english' or ['the', 'and', 'I']. Note that this will not be used if you pass in your own CountVectorizer. verbose: Changes the verbosity of the model, Set to True if you want to track the stages of the model. vectorizer: Pass in your own CountVectorizer from scikit-learn calculate_probabilities: Whether to calculate the topic probabilities. This could slow down extraction of topics if you have many documents (>100_000). If so, set this to False to increase speed. allow_st_model: This allows BERTopic to use a multi-lingual version of SentenceTransformer to be used to fine-tune the topic words extracted from the c-TF-IDF representation. Moreover, it will allow you to search for topics based on search queries. Usage: ```python from bertopic import BERTopic model = BERTopic(language = \"english\", embedding_model = None, top_n_words = 10, nr_topics = 30, n_gram_range = (1, 1), min_topic_size = 10, n_neighbors = 15, n_components = 5, stop_words = None, verbose = True, vectorizer = None, allow_st_model = True) ``` \"\"\" # Embedding model self . language = language self . embedding_model = embedding_model self . allow_st_model = allow_st_model # Topic-based parameters if top_n_words > 30 : raise ValueError ( \"top_n_words should be lower or equal to 30. The preferred value is 10.\" ) self . top_n_words = top_n_words self . nr_topics = nr_topics self . min_topic_size = min_topic_size self . calculate_probabilities = calculate_probabilities # Umap parameters self . n_neighbors = n_neighbors self . n_components = n_components # Vectorizer parameters self . stop_words = stop_words self . n_gram_range = n_gram_range self . vectorizer = vectorizer or CountVectorizer ( ngram_range = self . n_gram_range , stop_words = self . stop_words ) self . umap_model = None self . cluster_model = None self . topics = None self . topic_sizes = None self . reduced_topics_mapped = None self . mapped_topics = None self . topic_embeddings = None self . topic_sim_matrix = None self . custom_embeddings = False if verbose : logger . set_level ( \"DEBUG\" )","title":"__init__()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.find_topics","text":"Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Parameters: Name Type Description Default search_term str the term you want to use to search for topics required top_n int the number of topics to return 5 Returns: Type Description Tuple[List[int], List[float]] similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low Source code in bertopic\\_bertopic.py def find_topics ( self , search_term : str , top_n : int = 5 ) -> Tuple [ List [ int ], List [ float ]]: \"\"\" Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Args: search_term: the term you want to use to search for topics top_n: the number of topics to return Returns: similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low \"\"\" if self . custom_embeddings and not self . allow_st_model : raise Exception ( \"This method can only be used if you set `allow_st_model` to True when \" \"using custom embeddings.\" ) topic_list = list ( self . topics . keys ()) topic_list . sort () # Extract search_term embeddings and compare with topic embeddings search_embedding = self . _extract_embeddings ([ search_term ]) . flatten () sims = cosine_similarity ( search_embedding . reshape ( 1 , - 1 ), self . topic_embeddings ) . flatten () # Extract topics most similar to search_term ids = np . argsort ( sims )[ - top_n :] similarity = [ sims [ i ] for i in ids ][:: - 1 ] similar_topics = [ topic_list [ index ] for index in ids ][:: - 1 ] return similar_topics , similarity","title":"find_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.fit","text":"Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Parameters: Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) . fit ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) . fit ( docs , embeddings ) Source code in bertopic\\_bertopic.py def fit ( self , documents : List [ str ], embeddings : np . ndarray = None ): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True).fit(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True).fit(docs, embeddings) ``` \"\"\" self . fit_transform ( documents , embeddings ) return self","title":"fit()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.fit_transform","text":"Fit the models on a collection of documents, generate topics, and return the docs with topics Parameters: Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None Returns: Type Description Tuple[List[int], Optional[numpy.ndarray]] predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics = model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) topics = model . fit_transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py def fit_transform ( self , documents : List [ str ], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], Union [ np . ndarray , None ]]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True) topics = model.fit_transform(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True) topics = model.fit_transform(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) check_embeddings_shape ( embeddings , documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if not any ([ isinstance ( embeddings , np . ndarray ), isinstance ( embeddings , csr_matrix )]): embeddings = self . _extract_embeddings ( documents . Document ) else : self . custom_embeddings = True # Reduce dimensionality with UMAP umap_embeddings = self . _reduce_dimensionality ( embeddings ) # Cluster UMAP embeddings with HDBSCAN documents , probabilities = self . _cluster_embeddings ( umap_embeddings , documents ) # Extract topics by calculating c-TF-IDF self . _extract_topics ( documents ) if self . nr_topics : documents = self . _reduce_topics ( documents ) probabilities = self . _map_probabilities ( probabilities ) predictions = documents . Topic . to_list () return predictions , probabilities","title":"fit_transform()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topic","text":"Return top n words for a specific topic and their c-TF-IDF scores Usage: topic = model . get_topic ( 12 ) Source code in bertopic\\_bertopic.py def get_topic ( self , topic : int ) -> Union [ Dict [ str , Tuple [ str , float ]], bool ]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores Usage: ```python topic = model.get_topic(12) ``` \"\"\" check_is_fitted ( self ) if self . topics . get ( topic ): return self . topics [ topic ] else : return False","title":"get_topic()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topic_freq","text":"Return the the size of topics (descending order) Usage: To extract the frequency of all topics: frequency = model . get_topic_freq () To get the frequency of a single topic: frequency = model . get_topic_freq ( 12 ) Source code in bertopic\\_bertopic.py def get_topic_freq ( self , topic : int = None ) -> Union [ pd . DataFrame , int ]: \"\"\" Return the the size of topics (descending order) Usage: To extract the frequency of all topics: ```python frequency = model.get_topic_freq() ``` To get the frequency of a single topic: ```python frequency = model.get_topic_freq(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . topic_sizes [ topic ] else : return pd . DataFrame ( self . topic_sizes . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False )","title":"get_topic_freq()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topics","text":"Return topics with top n words and their c-TF-IDF score Usage: all_topics = model . get_topics () Source code in bertopic\\_bertopic.py def get_topics ( self ) -> Dict [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score Usage: ```python all_topics = model.get_topics() ``` \"\"\" check_is_fitted ( self ) return self . topics","title":"get_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.load","text":"Loads the model from the specified path Parameters: Name Type Description Default path str the location and name of the BERTopic file you want to load required Usage: BERTopic . load ( \"my_model\" ) Source code in bertopic\\_bertopic.py @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the BERTopic file you want to load Usage: ```python BERTopic.load(\"my_model\") ``` \"\"\" with open ( path , 'rb' ) as file : return joblib . load ( file )","title":"load()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.reduce_topics","text":"Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. The reasoning for putting docs , topics , and probs as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic instead of a dedicated database. Parameters: Name Type Description Default docs List[str] The docs you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required nr_topics int The number of topics you want reduced to 20 probabilities ndarray The probabilities that were returned when calling either fit or fit_transform None Returns: Type Description Tuple[List[int], numpy.ndarray] new_topics: Updated topics new_probabilities: Updated probabilities Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics -> Typically over 50 topics docs = fetch_20newsgroups ( subset = 'train' )[ 'data' ] model = BERTopic () topics , probs = model . fit_transform ( docs ) # Further reduce topics new_topics , new_probs = model . reduce_topics ( docs , topics , probs , nr_topics = 30 ) Source code in bertopic\\_bertopic.py def reduce_topics ( self , docs : List [ str ], topics : List [ int ], probabilities : np . ndarray = None , nr_topics : int = 20 ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. The reasoning for putting `docs`, `topics`, and `probs` as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic instead of a dedicated database. Arguments: docs: The docs you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` nr_topics: The number of topics you want reduced to probabilities: The probabilities that were returned when calling either `fit` or `fit_transform` Returns: new_topics: Updated topics new_probabilities: Updated probabilities Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics -> Typically over 50 topics docs = fetch_20newsgroups(subset='train')['data'] model = BERTopic() topics, probs = model.fit_transform(docs) # Further reduce topics new_topics, new_probs = model.reduce_topics(docs, topics, probs, nr_topics=30) ``` \"\"\" check_is_fitted ( self ) self . nr_topics = nr_topics documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) # Reduce number of topics self . _extract_topics ( documents ) documents = self . _reduce_topics ( documents ) new_topics = documents . Topic . to_list () new_probabilities = self . _map_probabilities ( probabilities ) return new_topics , new_probabilities","title":"reduce_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.save","text":"Saves the model to the specified path Parameters: Name Type Description Default path str the location and name of the file you want to save required Usage: model . save ( \"my_model\" ) Source code in bertopic\\_bertopic.py def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save Usage: ```python model.save(\"my_model\") ``` \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file )","title":"save()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.transform","text":"After having fit a model, use transform to predict new instances Parameters: Name Type Description Default documents Union[str, List[str]] A single document or a list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model. None Returns: Type Description Tuple[List[int], numpy.ndarray] predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) . fit ( docs ) topics = model . transform ( docs ) If you want to use your own embeddings: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) . fit ( docs , embeddings ) topics = model . transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py def transform ( self , documents : Union [ str , List [ str ]], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" After having fit a model, use transform to predict new instances Arguments: documents: A single document or a list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True).fit(docs) topics = model.transform(docs) ``` If you want to use your own embeddings: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True).fit(docs, embeddings) topics = model.transform(docs, embeddings) ``` \"\"\" check_is_fitted ( self ) check_embeddings_shape ( embeddings , documents ) if isinstance ( documents , str ): documents = [ documents ] if not isinstance ( embeddings , np . ndarray ): embeddings = self . _extract_embeddings ( documents ) umap_embeddings = self . umap_model . transform ( embeddings ) predictions , _ = hdbscan . approximate_predict ( self . cluster_model , umap_embeddings ) if self . calculate_probabilities : probabilities = hdbscan . membership_vector ( self . cluster_model , umap_embeddings ) if len ( documents ) == 1 : probabilities = probabilities . flatten () else : probabilities = None if self . mapped_topics : predictions = self . _map_predictions ( predictions ) probabilities = self . _map_probabilities ( probabilities ) return predictions , probabilities","title":"transform()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.update_topics","text":"Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Parameters: Name Type Description Default docs List[str] The docs you used when calling either fit or fit_transform required topics List[int] The topics that were returned when calling either fit or fit_transform required n_gram_range Tuple[int, int] The n-gram range for the CountVectorizer. None stop_words str Stopwords that can be used as either a list of strings, or the name of the language as a string. For example: 'english' or ['the', 'and', 'I']. Note that this will not be used if you pass in your own CountVectorizer. None vectorizer CountVectorizer Pass in your own CountVectorizer from scikit-learn None Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'train' )[ 'data' ] model = BERTopic ( n_gram_range = ( 1 , 1 ), stop_words = None ) topics , probs = model . fit_transform ( docs ) # Update topic representation model . update_topics ( docs , topics , n_gram_range = ( 2 , 3 ), stop_words = \"english\" ) Source code in bertopic\\_bertopic.py def update_topics ( self , docs : List [ str ], topics : List [ int ], n_gram_range : Tuple [ int , int ] = None , stop_words : str = None , vectorizer : CountVectorizer = None ): \"\"\" Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Args: docs: The docs you used when calling either `fit` or `fit_transform` topics: The topics that were returned when calling either `fit` or `fit_transform` n_gram_range: The n-gram range for the CountVectorizer. stop_words: Stopwords that can be used as either a list of strings, or the name of the language as a string. For example: 'english' or ['the', 'and', 'I']. Note that this will not be used if you pass in your own CountVectorizer. vectorizer: Pass in your own CountVectorizer from scikit-learn Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups(subset='train')['data'] model = BERTopic(n_gram_range=(1, 1), stop_words=None) topics, probs = model.fit_transform(docs) # Update topic representation model.update_topics(docs, topics, n_gram_range=(2, 3), stop_words=\"english\") ``` \"\"\" check_is_fitted ( self ) if not n_gram_range : n_gram_range = self . n_gram_range if not stop_words : stop_words = self . stop_words self . vectorizer = vectorizer or CountVectorizer ( ngram_range = n_gram_range , stop_words = stop_words ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) self . _extract_topics ( documents )","title":"update_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_distribution","text":"Visualize the distribution of topic probabilities Parameters: Name Type Description Default probabilities ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 figsize tuple The size of the figure (10, 5) save bool Whether to save the resulting graph to probility.png False Usage: Make sure to fit the model before and only input the probabilities of a single document: model . visualize_distribution ( probabilities [ 0 ]) Source code in bertopic\\_bertopic.py def visualize_distribution ( self , probabilities : np . ndarray , min_probability : float = 0.015 , figsize : tuple = ( 10 , 5 ), save : bool = False ): \"\"\" Visualize the distribution of topic probabilities Arguments: probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. figsize: The size of the figure save: Whether to save the resulting graph to probility.png Usage: Make sure to fit the model before and only input the probabilities of a single document: ```python model.visualize_distribution(probabilities[0]) ``` ![](../img/probabilities.png) \"\"\" check_is_fitted ( self ) if not _HAS_VIZ : raise ModuleNotFoundError ( f \"In order to use this function you'll need to install \" f \"additional dependencies; \\n pip install bertopic[visualization]\" ) if len ( probabilities [ probabilities > min_probability ]) == 0 : raise ValueError ( \"There are no values where `min_probability` is higher than the \" \"probabilities that were supplied. Lower `min_probability` to prevent this error.\" ) if not self . calculate_probabilities : raise ValueError ( \"This visualization cannot be used if you have set `calculate_probabilities` to False \" \"as it uses the topic probabilities. \" ) # Get values and indices equal or exceed the minimum probability labels_idx = np . argwhere ( probabilities >= min_probability ) . flatten () vals = probabilities [ labels_idx ] . tolist () # Create labels labels = [] for idx in labels_idx : label = [] words = self . get_topic ( idx ) if words : for word in words [: 5 ]: label . append ( word [ 0 ]) label = str ( r \"$\\bf{Topic }$ \" + r \"$\\bf{\" + str ( idx ) + \":}$ \" + \" \" . join ( label )) labels . append ( label ) else : print ( idx , probabilities [ idx ]) vals . remove ( probabilities [ idx ]) pos = range ( len ( vals )) # Create figure fig , ax = plt . subplots ( figsize = figsize ) plt . hlines ( y = pos , xmin = 0 , xmax = vals , color = '#333F4B' , alpha = 0.2 , linewidth = 15 ) plt . hlines ( y = np . argmax ( vals ), xmin = 0 , xmax = max ( vals ), color = '#333F4B' , alpha = 1 , linewidth = 15 ) # Set ticks and labels ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) ax . set_xlabel ( 'Probability' , fontsize = 15 , fontweight = 'black' , color = '#333F4B' ) ax . set_ylabel ( '' ) plt . yticks ( pos , labels ) fig . text ( 0 , 1 , 'Topic Probability Distribution' , fontsize = 15 , fontweight = 'black' , color = '#333F4B' ) # Update spine style ax . spines [ 'right' ] . set_visible ( False ) ax . spines [ 'top' ] . set_visible ( False ) ax . spines [ 'left' ] . set_bounds ( pos [ 0 ], pos [ - 1 ]) ax . spines [ 'bottom' ] . set_bounds ( 0 , max ( vals )) ax . spines [ 'bottom' ] . set_position (( 'axes' , - 0.02 )) ax . spines [ 'left' ] . set_position (( 'axes' , 0.02 )) fig . tight_layout () if save : fig . savefig ( \"probability.png\" , dpi = 300 , bbox_inches = 'tight' )","title":"visualize_distribution()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_topics","text":"Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Source code in bertopic\\_bertopic.py def visualize_topics ( self ): \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. \"\"\" check_is_fitted ( self ) if not _HAS_VIZ : raise ModuleNotFoundError ( f \"In order to use this function you'll need to install \" f \"additional dependencies; \\n pip install bertopic[visualization]\" ) # Extract topic words and their frequencies topic_list = sorted ( list ( self . topics . keys ())) frequencies = [ self . topic_sizes [ topic ] for topic in topic_list ] words = [ \" | \" . join ([ word [ 0 ] for word in self . get_topic ( topic )[: 5 ]]) for topic in topic_list ] # Embed c-TF-IDF into 2D embeddings = MinMaxScaler () . fit_transform ( self . c_tf_idf . toarray ()) embeddings = umap . UMAP ( n_neighbors = 2 , n_components = 2 , metric = 'hellinger' ) . fit_transform ( embeddings ) # Visualize with plotly df = pd . DataFrame ({ \"x\" : embeddings [ 1 :, 0 ], \"y\" : embeddings [ 1 :, 1 ], \"Topic\" : topic_list [ 1 :], \"Words\" : words [ 1 :], \"Size\" : frequencies [ 1 :]}) self . _plotly_topic_visualization ( df , topic_list )","title":"visualize_topics()"},{"location":"api/ctfidf.html","text":"c-TF-IDF \u00b6 \u00b6 A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base. C-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes by joining all documents per class. Thus, each class is converted to a single document instead of set of documents. Then, the frequency of words t are extracted for each class i and divided by the total number of words w . Next, the total, unjoined, number of documents across all classes m is divided by the total sum of word i across all classes. fit ( self , X , n_samples ) \u00b6 Learn the idf vector (global term weights). Parameters: Name Type Description Default X csr_matrix A matrix of term/token counts. required n_samples int Number of total documents required Source code in bertopic\\_ctfidf.py def fit ( self , X : sp . csr_matrix , n_samples : int ): \"\"\"Learn the idf vector (global term weights). Arguments: X: A matrix of term/token counts. n_samples: Number of total documents \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape self . df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) idf = np . log ( n_samples / self . df ) self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self transform ( self , X , copy = True ) \u00b6 Transform a count-based matrix to c-TF-IDF Parameters: Name Type Description Default X csr_matrix A matrix of term/token counts. required Returns: Type Description X (sparse matrix) A c-TF-IDF matrix Source code in bertopic\\_ctfidf.py def transform ( self , X : sp . csr_matrix , copy = True ): \"\"\"Transform a count-based matrix to c-TF-IDF Arguments: X (sparse matrix): A matrix of term/token counts. Returns: X (sparse matrix): A c-TF-IDF matrix \"\"\" if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) X = X * self . _idf_diag return X","title":"cTFIDF"},{"location":"api/ctfidf.html#c-tf-idf","text":"","title":"c-TF-IDF"},{"location":"api/ctfidf.html#bertopic._ctfidf.ClassTFIDF","text":"A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base. C-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes by joining all documents per class. Thus, each class is converted to a single document instead of set of documents. Then, the frequency of words t are extracted for each class i and divided by the total number of words w . Next, the total, unjoined, number of documents across all classes m is divided by the total sum of word i across all classes.","title":"bertopic._ctfidf.ClassTFIDF"},{"location":"api/ctfidf.html#bertopic._ctfidf.ClassTFIDF.fit","text":"Learn the idf vector (global term weights). Parameters: Name Type Description Default X csr_matrix A matrix of term/token counts. required n_samples int Number of total documents required Source code in bertopic\\_ctfidf.py def fit ( self , X : sp . csr_matrix , n_samples : int ): \"\"\"Learn the idf vector (global term weights). Arguments: X: A matrix of term/token counts. n_samples: Number of total documents \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape self . df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) idf = np . log ( n_samples / self . df ) self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self","title":"fit()"},{"location":"api/ctfidf.html#bertopic._ctfidf.ClassTFIDF.transform","text":"Transform a count-based matrix to c-TF-IDF Parameters: Name Type Description Default X csr_matrix A matrix of term/token counts. required Returns: Type Description X (sparse matrix) A c-TF-IDF matrix Source code in bertopic\\_ctfidf.py def transform ( self , X : sp . csr_matrix , copy = True ): \"\"\"Transform a count-based matrix to c-TF-IDF Arguments: X (sparse matrix): A matrix of term/token counts. Returns: X (sparse matrix): A c-TF-IDF matrix \"\"\" if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) X = X * self . _idf_diag return X","title":"transform()"},{"location":"tutorial/algorithm/algorithm.html","text":"The algorithm contains, roughly, 3 stages: Extract document embeddings with Sentence Transformers or TF-IDF Cluster document embeddings to create groups of similar documents with UMAP and HDBSCAN Extract and reduce topics with c-TF-IDF Create Document Embeddings \u00b6 We start by creating document embeddings from a set of documents using sentence-transformers . These models are pre-trained for many language and are great for creating either document- or sentence-embeddings. In BERTopic, you can choose any sentence transformers model but there are two models that are set as defaults: * \"distilbert-base-nli-stsb-mean-tokens\" * \"xlm-r-bert-base-nli-stsb-mean-tokens\" The first is an English BERT-based model trained specifically for semantic similarity tasks which works quite well for most use-cases. The second model is very similar to the first with one major difference is that the xlm models work for 50+ languages. This model is quite a bit larger than the first and is only selected if you select any language other than English. Cluster Document Embeddings \u00b6 Next, in order to cluster the documents using a clustering algorithm such as HDBSCAN we first need to reduce its dimensionality as HDBCAN is prone to the curse of dimensionality. Thus, we first lower dimensionality with UMAP as it preserves local structure well after which we can use HDBSCAN to cluster similar documents. Extract Topics \u00b6 What we want to know from the clusters that we generated, is what makes one cluster, based on their content, different from another? To solve this, we can modify TF-IDF such that it allows for interesting words per topic instead of per document. When you apply TF-IDF as usual on a set of documents, what you are basically doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of topics ! This model is called class-based TF-IDF Each cluster is converted to a single document instead of a set of documents. Then, the frequency of word t are extracted for each class i and divided by the total number of words w . This action can now be seen as a form of regularization of frequent words in the class. Next, the total, unjoined, number of documents m is divided by the total frequency of word t across all classes n . Topic Coherence \u00b6 This fourth step is executed if you do not use custom embeddings but generate the document embeddings within BERTopic itself. The embedding model provided by BERTopic will be used to improve the coherence of words within a topic. After having generated the c-TF-IDF representations, we have a set of words that describe a collection of documents. Technically, this does not mean that this collection of words actually describes a coherent topic. In practice, we will see that many of the words do describe a similar topic but some words will, in a way, overfit to the documents. For example, if you have a set of documents that is written by the same person whose signature will be in the topic description. In order to improve coherence of words Maximal Marginal Relevance was used to find the words that were most coherent without having too much overlap between the words itself. This results in the removal of words that do not contribute to a topic.","title":"The Algorithm"},{"location":"tutorial/algorithm/algorithm.html#create-document-embeddings","text":"We start by creating document embeddings from a set of documents using sentence-transformers . These models are pre-trained for many language and are great for creating either document- or sentence-embeddings. In BERTopic, you can choose any sentence transformers model but there are two models that are set as defaults: * \"distilbert-base-nli-stsb-mean-tokens\" * \"xlm-r-bert-base-nli-stsb-mean-tokens\" The first is an English BERT-based model trained specifically for semantic similarity tasks which works quite well for most use-cases. The second model is very similar to the first with one major difference is that the xlm models work for 50+ languages. This model is quite a bit larger than the first and is only selected if you select any language other than English.","title":"Create Document Embeddings"},{"location":"tutorial/algorithm/algorithm.html#cluster-document-embeddings","text":"Next, in order to cluster the documents using a clustering algorithm such as HDBSCAN we first need to reduce its dimensionality as HDBCAN is prone to the curse of dimensionality. Thus, we first lower dimensionality with UMAP as it preserves local structure well after which we can use HDBSCAN to cluster similar documents.","title":"Cluster Document Embeddings"},{"location":"tutorial/algorithm/algorithm.html#extract-topics","text":"What we want to know from the clusters that we generated, is what makes one cluster, based on their content, different from another? To solve this, we can modify TF-IDF such that it allows for interesting words per topic instead of per document. When you apply TF-IDF as usual on a set of documents, what you are basically doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of topics ! This model is called class-based TF-IDF Each cluster is converted to a single document instead of a set of documents. Then, the frequency of word t are extracted for each class i and divided by the total number of words w . This action can now be seen as a form of regularization of frequent words in the class. Next, the total, unjoined, number of documents m is divided by the total frequency of word t across all classes n .","title":"Extract Topics"},{"location":"tutorial/algorithm/algorithm.html#topic-coherence","text":"This fourth step is executed if you do not use custom embeddings but generate the document embeddings within BERTopic itself. The embedding model provided by BERTopic will be used to improve the coherence of words within a topic. After having generated the c-TF-IDF representations, we have a set of words that describe a collection of documents. Technically, this does not mean that this collection of words actually describes a coherent topic. In practice, we will see that many of the words do describe a similar topic but some words will, in a way, overfit to the documents. For example, if you have a set of documents that is written by the same person whose signature will be in the topic description. In order to improve coherence of words Maximal Marginal Relevance was used to find the words that were most coherent without having too much overlap between the words itself. This results in the removal of words that do not contribute to a topic.","title":"Topic Coherence"},{"location":"tutorial/embeddings/embeddings.html","text":"Transformer Models \u00b6 The base models in BERTopic are both BERT-based models that work well with document similarity tasks. You documents, however, might be too specific for a general pre-trained model to be used. Fortunately, you can use embedding model in BERTopic in order to create document features. You only need to prepare the document embeddings yourself and pass them through fit_transform of BERTopic: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Create topic model model = BERTopic () topics , probabilities = model . fit_transform ( docs , embeddings ) As you can see above, we used a SentenceTransformer model to create the embedding. You could also have used \ud83e\udd17 transformers , Doc2Vec , or any other embedding method. Due to the stochastisch nature of UMAP, the results from BERTopic might differ even if you run the same code multiple times. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best. You only need to generate the embeddings itself once and run BERTopic several times with different parameters. TF-IDF \u00b6 As mentioned above, any embedding technique can be used. However, when running umap, the typical distance metric is cosine which does not work quite well for a TF-IDF matrix. Instead, BERTopic will recognize that a sparse matrix is passed and use hellinger instead which works quite well for the similarity between probability distributions. We simply create a TF-IDF matrix and use them as embeddings in our fit_transform method: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer # Create TF-IDF sparse matrix docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] vectorizer = TfidfVectorizer ( min_df = 5 ) embeddings = vectorizer . fit_transform ( docs ) # model = BERTopic ( stop_words = \"english\" ) topics , probabilities = model . fit_transform ( docs , embeddings ) Here, you will probably notice that creating the embeddings is quite fast whereas fit_transform is quite slow. This is to be expected as reducing dimensionality of a large sparse matrix takes some time. The inverse of using transformer embeddings is true: creating the embeddings is slow whereas fit_transform is quite fast. You can play around with different models until you find the best suiting model for you.","title":"Custom Embeddings"},{"location":"tutorial/embeddings/embeddings.html#transformer-models","text":"The base models in BERTopic are both BERT-based models that work well with document similarity tasks. You documents, however, might be too specific for a general pre-trained model to be used. Fortunately, you can use embedding model in BERTopic in order to create document features. You only need to prepare the document embeddings yourself and pass them through fit_transform of BERTopic: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Create topic model model = BERTopic () topics , probabilities = model . fit_transform ( docs , embeddings ) As you can see above, we used a SentenceTransformer model to create the embedding. You could also have used \ud83e\udd17 transformers , Doc2Vec , or any other embedding method. Due to the stochastisch nature of UMAP, the results from BERTopic might differ even if you run the same code multiple times. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best. You only need to generate the embeddings itself once and run BERTopic several times with different parameters.","title":"Transformer Models"},{"location":"tutorial/embeddings/embeddings.html#tf-idf","text":"As mentioned above, any embedding technique can be used. However, when running umap, the typical distance metric is cosine which does not work quite well for a TF-IDF matrix. Instead, BERTopic will recognize that a sparse matrix is passed and use hellinger instead which works quite well for the similarity between probability distributions. We simply create a TF-IDF matrix and use them as embeddings in our fit_transform method: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer # Create TF-IDF sparse matrix docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] vectorizer = TfidfVectorizer ( min_df = 5 ) embeddings = vectorizer . fit_transform ( docs ) # model = BERTopic ( stop_words = \"english\" ) topics , probabilities = model . fit_transform ( docs , embeddings ) Here, you will probably notice that creating the embeddings is quite fast whereas fit_transform is quite slow. This is to be expected as reducing dimensionality of a large sparse matrix takes some time. The inverse of using transformer embeddings is true: creating the embeddings is slow whereas fit_transform is quite fast. You can play around with different models until you find the best suiting model for you.","title":"TF-IDF"},{"location":"tutorial/quickstart/quickstart.html","text":"Installation \u00b6 PyTorch 1.2.0 or higher is recommended. If the install below gives an error, please install pytorch first here . Installation can be done using pypi : pip install bertopic Quick Start \u00b6 Below is an example of how to use the model. The example uses the 20 newsgroups dataset. from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probabilities = model . fit_transform ( docs ) The resulting topics can be accessed through model.get_topic(topic) : >>> model . get_topic ( 9 ) [( 'game' , 0.005251396890032802 ), ( 'team' , 0.00482651185323754 ), ( 'hockey' , 0.004335032060690186 ), ( 'players' , 0.0034782716706978963 ), ( 'games' , 0.0032873248432630227 ), ( 'season' , 0.003218987432255393 ), ( 'play' , 0.0031855141725669637 ), ( 'year' , 0.002962343114817677 ), ( 'nhl' , 0.0029577648449943144 ), ( 'baseball' , 0.0029245163154193524 )] NOTE : If you get less than 10 topics, it is advised to decrease the min_topic_size in BERTopic . This will allow clusters to be created more easily and will typically result in more clusters. Languages \u00b6 BERTopic is set to english but supports essentially any language for which a document embedding model exists. You can choose the language by simply setting the language parameter in BERTopic. from bertopic import BERTopic model = BERTopic ( language = \"Dutch\" ) For a list of supported languages, please select the link below. Supported Languages The following languages are supported: Afrikaans, Albanian, Amharic, Arabic, Armenian, Assamese, Azerbaijani, Basque, Belarusian, Bengali, Bengali Romanize, Bosnian, Breton, Bulgarian, Burmese, Burmese zawgyi font, Catalan, Chinese (Simplified), Chinese (Traditional), Croatian, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Hausa, Hebrew, Hindi, Hindi Romanize, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish (Kurmanji), Kyrgyz, Lao, Latin, Latvian, Lithuanian, Macedonian, Malagasy, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian, Oriya, Oromo, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Sanskrit, Scottish Gaelic, Serbian, Sindhi, Sinhala, Slovak, Slovenian, Somali, Spanish, Sundanese, Swahili, Swedish, Tamil, Tamil Romanize, Telugu, Telugu Romanize, Thai, Turkish, Ukrainian, Urdu, Urdu Romanize, Uyghur, Uzbek, Vietnamese, Welsh, Western Frisian, Xhosa, Yiddish Embedding model \u00b6 If you want to select any model from sentence-transformers you can simply select that model and pass it through BERTopic with embedding_model : from bertopic import BERTopic model = BERTopic ( embedding_model = \"xlm-r-bert-base-nli-stsb-mean-tokens\" ) Click here for a list of supported sentence transformers models. Visualize Topic Probabilities \u00b6 The variable probabilities that is returned from transform() or fit_transform() can be used to understand how confident BERTopic is that certain topics can be found in a document. To visualize the distributions, we simply call: # Make sure to input the probabilities of a single document! model . visualize_distribution ( probabilities [ 0 ]) NOTE : The distribution of the probabilities does not give an indication to the distribution of the frequencies of topics across a document. It merely shows how confident BERTopic is that certain topics can be found in a document. Save/Load BERTopic model \u00b6 We can easily save a trained BERTopic model by calling save : from bertopic import BERTopic model = BERTopic () model . save ( \"my_model\" ) Then, we can load the model in one line: loaded_model = BERTopic . load ( \"my_model\" )","title":"Getting Started"},{"location":"tutorial/quickstart/quickstart.html#installation","text":"PyTorch 1.2.0 or higher is recommended. If the install below gives an error, please install pytorch first here . Installation can be done using pypi : pip install bertopic","title":"Installation"},{"location":"tutorial/quickstart/quickstart.html#quick-start","text":"Below is an example of how to use the model. The example uses the 20 newsgroups dataset. from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probabilities = model . fit_transform ( docs ) The resulting topics can be accessed through model.get_topic(topic) : >>> model . get_topic ( 9 ) [( 'game' , 0.005251396890032802 ), ( 'team' , 0.00482651185323754 ), ( 'hockey' , 0.004335032060690186 ), ( 'players' , 0.0034782716706978963 ), ( 'games' , 0.0032873248432630227 ), ( 'season' , 0.003218987432255393 ), ( 'play' , 0.0031855141725669637 ), ( 'year' , 0.002962343114817677 ), ( 'nhl' , 0.0029577648449943144 ), ( 'baseball' , 0.0029245163154193524 )] NOTE : If you get less than 10 topics, it is advised to decrease the min_topic_size in BERTopic . This will allow clusters to be created more easily and will typically result in more clusters.","title":"Quick Start"},{"location":"tutorial/quickstart/quickstart.html#languages","text":"BERTopic is set to english but supports essentially any language for which a document embedding model exists. You can choose the language by simply setting the language parameter in BERTopic. from bertopic import BERTopic model = BERTopic ( language = \"Dutch\" ) For a list of supported languages, please select the link below. Supported Languages The following languages are supported: Afrikaans, Albanian, Amharic, Arabic, Armenian, Assamese, Azerbaijani, Basque, Belarusian, Bengali, Bengali Romanize, Bosnian, Breton, Bulgarian, Burmese, Burmese zawgyi font, Catalan, Chinese (Simplified), Chinese (Traditional), Croatian, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Hausa, Hebrew, Hindi, Hindi Romanize, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish (Kurmanji), Kyrgyz, Lao, Latin, Latvian, Lithuanian, Macedonian, Malagasy, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian, Oriya, Oromo, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Sanskrit, Scottish Gaelic, Serbian, Sindhi, Sinhala, Slovak, Slovenian, Somali, Spanish, Sundanese, Swahili, Swedish, Tamil, Tamil Romanize, Telugu, Telugu Romanize, Thai, Turkish, Ukrainian, Urdu, Urdu Romanize, Uyghur, Uzbek, Vietnamese, Welsh, Western Frisian, Xhosa, Yiddish","title":"Languages"},{"location":"tutorial/quickstart/quickstart.html#embedding-model","text":"If you want to select any model from sentence-transformers you can simply select that model and pass it through BERTopic with embedding_model : from bertopic import BERTopic model = BERTopic ( embedding_model = \"xlm-r-bert-base-nli-stsb-mean-tokens\" ) Click here for a list of supported sentence transformers models.","title":"Embedding model"},{"location":"tutorial/quickstart/quickstart.html#visualize-topic-probabilities","text":"The variable probabilities that is returned from transform() or fit_transform() can be used to understand how confident BERTopic is that certain topics can be found in a document. To visualize the distributions, we simply call: # Make sure to input the probabilities of a single document! model . visualize_distribution ( probabilities [ 0 ]) NOTE : The distribution of the probabilities does not give an indication to the distribution of the frequencies of topics across a document. It merely shows how confident BERTopic is that certain topics can be found in a document.","title":"Visualize Topic Probabilities"},{"location":"tutorial/quickstart/quickstart.html#saveload-bertopic-model","text":"We can easily save a trained BERTopic model by calling save : from bertopic import BERTopic model = BERTopic () model . save ( \"my_model\" ) Then, we can load the model in one line: loaded_model = BERTopic . load ( \"my_model\" )","title":"Save/Load BERTopic model"},{"location":"tutorial/search/search.html","text":"Search Topics \u00b6 After having created a BERTopic model, you might end up with over a hundred topics. Searching through those can be quite cumbersome especially if you are searching for a specific topic. Fortunately, BERTopic allows you to search for topics using search terms. First, let's create an train a BERTopic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probs = model . fit_transform ( docs ) After having trained our model, we can use find_topics to search for topics that are similar to an input search_term. Here, we are going to be searching for topics that closely relate the search term \"motor\". Then, we extract the most similar topic and check the results: >>> similar_topics , similarity = model . find_topics ( \"motor\" , top_n = 5 ) >>> model . get_topic ( similar_topics [ 0 ]) [( 'bike' , 0.02275997701645559 ), ( 'motorcycle' , 0.011391202866080292 ), ( 'bikes' , 0.00981187573649205 ), ( 'dod' , 0.009614623748226669 ), ( 'honda' , 0.008247663662558535 ), ( 'ride' , 0.0064683227888861945 ), ( 'harley' , 0.006355502638631013 ), ( 'riding' , 0.005766601561614182 ), ( 'motorcycles' , 0.005596372493714447 ), ( 'advice' , 0.005534544418830091 )] It definitely seems that a topic was found that closely matches with \"motor\". The topic seems to be motorcycle related and therefore matches with out \"motor\" input. You can use the similarity variable to see how similar the extracted topics are to the search term.","title":"Search Topics"},{"location":"tutorial/search/search.html#search-topics","text":"After having created a BERTopic model, you might end up with over a hundred topics. Searching through those can be quite cumbersome especially if you are searching for a specific topic. Fortunately, BERTopic allows you to search for topics using search terms. First, let's create an train a BERTopic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probs = model . fit_transform ( docs ) After having trained our model, we can use find_topics to search for topics that are similar to an input search_term. Here, we are going to be searching for topics that closely relate the search term \"motor\". Then, we extract the most similar topic and check the results: >>> similar_topics , similarity = model . find_topics ( \"motor\" , top_n = 5 ) >>> model . get_topic ( similar_topics [ 0 ]) [( 'bike' , 0.02275997701645559 ), ( 'motorcycle' , 0.011391202866080292 ), ( 'bikes' , 0.00981187573649205 ), ( 'dod' , 0.009614623748226669 ), ( 'honda' , 0.008247663662558535 ), ( 'ride' , 0.0064683227888861945 ), ( 'harley' , 0.006355502638631013 ), ( 'riding' , 0.005766601561614182 ), ( 'motorcycles' , 0.005596372493714447 ), ( 'advice' , 0.005534544418830091 )] It definitely seems that a topic was found that closely matches with \"motor\". The topic seems to be motorcycle related and therefore matches with out \"motor\" input. You can use the similarity variable to see how similar the extracted topics are to the search term.","title":"Search Topics"},{"location":"tutorial/topicreduction/topicreduction.html","text":"In BERTopic , there are several arguments that might be helpful if you tend to end up with too many or too few topics. Topic Parameters \u00b6 The arguments discussed here all relate to the cluster step of BERTopic. Minimum topic size \u00b6 The min_topic_size parameter is actually used in HDBSCAN . It tells HDBSCAN what the minimum size of a cluster should be before it is accepted as a cluster. When you set this parameter very high, you will get very little clusters as they all need to be high. In constrast, if you set this too low you might end with too many extremely specific clusters. from bertopic import BERTopic model = BERTopic ( min_topic_size = 10 ) You can increase this value if you have more data available or if you expect clusters to be quite large. Local Neighborhood \u00b6 The n_neighbors parameter is used in UMAP when reducing the dimensionality. It is the size of the local neighborhood used for manifold approximation. If we set this relatively high, we get a more global view of the data which might reduce the number of clusters. Smaller values result in more local data being preseverd which could result in more clusters. from bertopic import BERTopic model = BERTopic ( n_neighbors = 15 ) If you have more data, you can increase the value of this parameter as you are more likely to have more neighbors. Dimensionality \u00b6 The n_components refers to the dimension size we reduce the document embeddings to. This is necessary for HDBSCAN to properly find clusters. A higher value will preserve more local structure but makes clustering more complicated for HDBSCAN which can result in fewer clusters if set to high. A small value will preserve less of the local structure but makes clustering easier for HDBSCAN. Similarly, this can result in fewer clusters if set to low. from bertopic import BERTopic model = BERTopic ( n_components = 5 ) I would recommend a value between 3 and 10 dimensions. Hierarchical Topic Reduction \u00b6 It is not possible for HDBSCAN to specify the number of clusters you would want. To a certain extent, this is actually an advantage, as we can trust HDBSCAN to be better in finding the number of clusters than we are. Instead, we can try to reduce the number of topics that have been created. Below, you will find three methods of doing so. Manual Topic Reduction \u00b6 Each resulting topic has its own feature vector constructed from c-TF-IDF. Using those feature vectors, we can find the most similar topics and merge them. If we do this iteratively, starting from the least frequent topic, we can reduce the number of topics quite easily. We do this until we reach the value of nr_topics : from bertopic import BERTopic model = BERTopic ( nr_topics = 20 ) Automatic Topic Reduction \u00b6 One issue with the approach above is that it will merge topics regardless of whether they are actually very similar. They are simply the most similar out of all options. This can be resolved by reducing the number of topics automatically. It will reduce the number of topics, starting from the least frequent topic, as long as it exceeds a minimum similarity of 0.9. To use this option, we simply set nr_topics to \"auto\" : from bertopic import BERTopic model = BERTopic ( nr_topics = \"auto\" ) Topic Reduction after Training \u00b6 Finally, we can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, is that you can decide the number of topics after knowing how many are actually created. It is difficult to predict before training your model how many topics that are in your documents and how many will be extracted. Instead, we can decide afterwards how many topics seems realistic: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics -> Typically over 50 topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probs = model . fit_transform ( docs ) # Further reduce topics new_topics , new_probs = model . reduce_topics ( docs , topics , probs , nr_topics = 30 ) The reasoning for putting docs , topics , and probs as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic instead of a dedicated database.","title":"Topic Reduction"},{"location":"tutorial/topicreduction/topicreduction.html#topic-parameters","text":"The arguments discussed here all relate to the cluster step of BERTopic.","title":"Topic Parameters"},{"location":"tutorial/topicreduction/topicreduction.html#minimum-topic-size","text":"The min_topic_size parameter is actually used in HDBSCAN . It tells HDBSCAN what the minimum size of a cluster should be before it is accepted as a cluster. When you set this parameter very high, you will get very little clusters as they all need to be high. In constrast, if you set this too low you might end with too many extremely specific clusters. from bertopic import BERTopic model = BERTopic ( min_topic_size = 10 ) You can increase this value if you have more data available or if you expect clusters to be quite large.","title":"Minimum topic size"},{"location":"tutorial/topicreduction/topicreduction.html#local-neighborhood","text":"The n_neighbors parameter is used in UMAP when reducing the dimensionality. It is the size of the local neighborhood used for manifold approximation. If we set this relatively high, we get a more global view of the data which might reduce the number of clusters. Smaller values result in more local data being preseverd which could result in more clusters. from bertopic import BERTopic model = BERTopic ( n_neighbors = 15 ) If you have more data, you can increase the value of this parameter as you are more likely to have more neighbors.","title":"Local Neighborhood"},{"location":"tutorial/topicreduction/topicreduction.html#dimensionality","text":"The n_components refers to the dimension size we reduce the document embeddings to. This is necessary for HDBSCAN to properly find clusters. A higher value will preserve more local structure but makes clustering more complicated for HDBSCAN which can result in fewer clusters if set to high. A small value will preserve less of the local structure but makes clustering easier for HDBSCAN. Similarly, this can result in fewer clusters if set to low. from bertopic import BERTopic model = BERTopic ( n_components = 5 ) I would recommend a value between 3 and 10 dimensions.","title":"Dimensionality"},{"location":"tutorial/topicreduction/topicreduction.html#hierarchical-topic-reduction","text":"It is not possible for HDBSCAN to specify the number of clusters you would want. To a certain extent, this is actually an advantage, as we can trust HDBSCAN to be better in finding the number of clusters than we are. Instead, we can try to reduce the number of topics that have been created. Below, you will find three methods of doing so.","title":"Hierarchical Topic Reduction"},{"location":"tutorial/topicreduction/topicreduction.html#manual-topic-reduction","text":"Each resulting topic has its own feature vector constructed from c-TF-IDF. Using those feature vectors, we can find the most similar topics and merge them. If we do this iteratively, starting from the least frequent topic, we can reduce the number of topics quite easily. We do this until we reach the value of nr_topics : from bertopic import BERTopic model = BERTopic ( nr_topics = 20 )","title":"Manual Topic Reduction"},{"location":"tutorial/topicreduction/topicreduction.html#automatic-topic-reduction","text":"One issue with the approach above is that it will merge topics regardless of whether they are actually very similar. They are simply the most similar out of all options. This can be resolved by reducing the number of topics automatically. It will reduce the number of topics, starting from the least frequent topic, as long as it exceeds a minimum similarity of 0.9. To use this option, we simply set nr_topics to \"auto\" : from bertopic import BERTopic model = BERTopic ( nr_topics = \"auto\" )","title":"Automatic Topic Reduction"},{"location":"tutorial/topicreduction/topicreduction.html#topic-reduction-after-training","text":"Finally, we can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, is that you can decide the number of topics after knowing how many are actually created. It is difficult to predict before training your model how many topics that are in your documents and how many will be extracted. Instead, we can decide afterwards how many topics seems realistic: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics -> Typically over 50 topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probs = model . fit_transform ( docs ) # Further reduce topics new_topics , new_probs = model . reduce_topics ( docs , topics , probs , nr_topics = 30 ) The reasoning for putting docs , topics , and probs as parameters is that these values are not saved within BERTopic on purpose. If you were to have a million documents, it seems very inefficient to save those in BERTopic instead of a dedicated database.","title":"Topic Reduction after Training"},{"location":"tutorial/topicrepresentation/topicrepresentation.html","text":"The topics that are extracted from BERTopic are represented by words. These words are extracted from the documents occupying their topics using a class-based TF-IDF. This allows us to extract words that are interesting to a topic but less so to another. The result is clearly separated clusters. Update Topic Representation after Training \u00b6 When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. We can use the function update_topics to update the topic representation with new parameters for c-TF-IDF : from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic ( n_gram_range = ( 2 , 3 ), stop_words = \"english\" ) topics , probs = model . fit_transform ( docs ) From the model created above, one of the most frequent topics is the following: >>> model . get_topic ( 31 )[: 10 ] [( 'clipper chip' , 0.007240771542316232 ), ( 'key escrow' , 0.004601603973377443 ), ( 'law enforcement' , 0.004277247929596332 ), ( 'intercon com' , 0.0035961920238955824 ), ( 'amanda walker' , 0.003474856425297157 ), ( 'serial number' , 0.0029876119137150358 ), ( 'com amanda' , 0.002789303096817983 ), ( 'intercon com amanda' , 0.0027386688593327084 ), ( 'amanda intercon' , 0.002585262048515583 ), ( 'amanda intercon com' , 0.002585262048515583 )] Although there does seem to be some relation between words, it is difficult, at least for me, to intuitively understand what the topic is about. Instead, let's simplify the topic representation by setting n_gram_range to (1, 3) to also allow for single words. >>> model . update_topics ( docs , topics , n_gram_range = ( 1 , 3 ), stop_words = \"english\" ) >>> model . get_topic ( 31 )[: 10 ] [( 'encryption' , 0.008021846079148017 ), ( 'clipper' , 0.00789642647602742 ), ( 'chip' , 0.00637127942464045 ), ( 'key' , 0.006363124787175884 ), ( 'escrow' , 0.005030980365244285 ), ( 'clipper chip' , 0.0048271268437973395 ), ( 'keys' , 0.0043245812747907545 ), ( 'crypto' , 0.004311198708675516 ), ( 'intercon' , 0.0038772934659295076 ), ( 'amanda' , 0.003516026493904586 )] To me, the combination of the words above seem a bit more intuitive than the words we previously had! You can play around with n_gram_range and stop_words or use your own custom sklearn.feature_extraction.text.CountVectorizer .","title":"Topic Representation"},{"location":"tutorial/topicrepresentation/topicrepresentation.html#update-topic-representation-after-training","text":"When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. We can use the function update_topics to update the topic representation with new parameters for c-TF-IDF : from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic ( n_gram_range = ( 2 , 3 ), stop_words = \"english\" ) topics , probs = model . fit_transform ( docs ) From the model created above, one of the most frequent topics is the following: >>> model . get_topic ( 31 )[: 10 ] [( 'clipper chip' , 0.007240771542316232 ), ( 'key escrow' , 0.004601603973377443 ), ( 'law enforcement' , 0.004277247929596332 ), ( 'intercon com' , 0.0035961920238955824 ), ( 'amanda walker' , 0.003474856425297157 ), ( 'serial number' , 0.0029876119137150358 ), ( 'com amanda' , 0.002789303096817983 ), ( 'intercon com amanda' , 0.0027386688593327084 ), ( 'amanda intercon' , 0.002585262048515583 ), ( 'amanda intercon com' , 0.002585262048515583 )] Although there does seem to be some relation between words, it is difficult, at least for me, to intuitively understand what the topic is about. Instead, let's simplify the topic representation by setting n_gram_range to (1, 3) to also allow for single words. >>> model . update_topics ( docs , topics , n_gram_range = ( 1 , 3 ), stop_words = \"english\" ) >>> model . get_topic ( 31 )[: 10 ] [( 'encryption' , 0.008021846079148017 ), ( 'clipper' , 0.00789642647602742 ), ( 'chip' , 0.00637127942464045 ), ( 'key' , 0.006363124787175884 ), ( 'escrow' , 0.005030980365244285 ), ( 'clipper chip' , 0.0048271268437973395 ), ( 'keys' , 0.0043245812747907545 ), ( 'crypto' , 0.004311198708675516 ), ( 'intercon' , 0.0038772934659295076 ), ( 'amanda' , 0.003516026493904586 )] To me, the combination of the words above seem a bit more intuitive than the words we previously had! You can play around with n_gram_range and stop_words or use your own custom sklearn.feature_extraction.text.CountVectorizer .","title":"Update Topic Representation after Training"},{"location":"tutorial/visualization/visualization.html","text":"Visualize Topics \u00b6 After having trained our BERTopic model, we can iteratively go through perhaps a hundred topic to get a good understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis . We embed our c-TF-IDF representation of the topics in 2D using Umap and then visualize the two dimensions using plotly such that we can create an interactive view. First, we need to train our model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probs = model . fit_transform ( docs ) Then, we simply call model.visualize_topics() in order to visualize our topics. The resulting graph is a plotly interactive graph which can be converted to html. Thus, you can play around with the results below: You can use the slider to select the topic which then lights up red. If you hover over a topic, then general information is given about the topic, including size of the topic and its corresponding words. Visualize Probablities \u00b6 The variable probabilities that is returned from transform() or fit_transform() can be used to understand how confident BERTopic is that certain topics can be found in a document. from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probs = model . fit_transform ( docs ) To visualize the distributions, we simply call: model . visualize_distribution ( probabilities [ 0 ])","title":"Topic Visualization"},{"location":"tutorial/visualization/visualization.html#visualize-topics","text":"After having trained our BERTopic model, we can iteratively go through perhaps a hundred topic to get a good understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis . We embed our c-TF-IDF representation of the topics in 2D using Umap and then visualize the two dimensions using plotly such that we can create an interactive view. First, we need to train our model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probs = model . fit_transform ( docs ) Then, we simply call model.visualize_topics() in order to visualize our topics. The resulting graph is a plotly interactive graph which can be converted to html. Thus, you can play around with the results below: You can use the slider to select the topic which then lights up red. If you hover over a topic, then general information is given about the topic, including size of the topic and its corresponding words.","title":"Visualize Topics"},{"location":"tutorial/visualization/visualization.html#visualize-probablities","text":"The variable probabilities that is returned from transform() or fit_transform() can be used to understand how confident BERTopic is that certain topics can be found in a document. from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] model = BERTopic () topics , probs = model . fit_transform ( docs ) To visualize the distributions, we simply call: model . visualize_distribution ( probabilities [ 0 ])","title":"Visualize Probablities"}]}