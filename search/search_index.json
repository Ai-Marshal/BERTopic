{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"BERTopic \u00b6 BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Corresponding medium post can be found here . About the Project \u00b6 The initial purpose of this project was to generalize Top2Vec such that it could be used with state-of-art pre-trained transformer models. However, this proved difficult due to the different natures of Doc2Vec and transformer models. Instead, I decided to come up with a different algorithm that could use BERT and \ud83e\udd17 transformers embeddings. The results is BERTopic , an algorithm for generating topics using state-of-the-art embeddings. Installation \u00b6 PyTorch 1.2.0 or higher is recommended. If the install below gives an error, please install pytorch first here . Installation can be done using pypi : pip install bertopic Usage \u00b6 Below is an example of how to use the model. The example uses the 20 newsgroups dataset. from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics , probabilities = model . fit_transform ( docs ) The resulting topics can be accessed through model.get_topic(topic) : >>> model . get_topic ( 9 ) [( 'game' , 0.005251396890032802 ), ( 'team' , 0.00482651185323754 ), ( 'hockey' , 0.004335032060690186 ), ( 'players' , 0.0034782716706978963 ), ( 'games' , 0.0032873248432630227 ), ( 'season' , 0.003218987432255393 ), ( 'play' , 0.0031855141725669637 ), ( 'year' , 0.002962343114817677 ), ( 'nhl' , 0.0029577648449943144 ), ( 'baseball' , 0.0029245163154193524 )] You can find an overview of all models currently in BERTopic here and here . Custom Embeddings \u00b6 If you use BERTopic as shown above, then you are forced to use sentence-transformers as the main package for which to create embeddings. However, you might have your own model or package that you believe is better suited for representing documents. Fortunately, for those that want to use their own embeddings there is an option in BERTopic. For this example I will still be using sentence-transformers but the general principle holds: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Create topic model model = BERTopic ( verbose = True ) topics = model . fit_transform ( docs , embeddings ) Due to the stochastisch nature of UMAP, the results from BERTopic might differ even if you run the same code multiple times. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best. You only need to generate the embeddings itself once and run BERTopic several times with different parameters. Visualize Topic Probabilities \u00b6 The variable probabilities that is returned from transform() or fit_transform() can be used to understand how confident BERTopic is that certain topics can be found in a document. To visualize the distributions, we simply call: # Make sure to input the probabilities of a single document! model . visualize_distribution ( probabilities [ 0 ]) NOTE : The distribution of the probabilities does not give an indication to the distribution of the frequencies of topics across a document. It merely shows how confident BERTopic is that certain topics can be found in a document. Overview \u00b6 Methods Code Returns Access single topic model.get_topic(12) Tuple[Word, Score] Access all topics model.get_topics() List[Tuple[Word, Score]] Get single topic freq model.get_topic_freq(12) int Get all topic freq model.get_topics_freq() DataFrame Fit the model model.fit(docs]) - Fit the model and predict documents model.fit_transform(docs]) List[int], List[float] Predict new documents model.transform([new_doc]) List[int], List[float] Visualize Topic Probability Distribution model.visualize_distribution(probabilities) Matplotlib.Figure Save model model.save(\"my_model\") - Load model BERTopic.load(\"my_model\") - NOTE : The embeddings itself are not preserved in the model as they are only vital for creating the clusters. Therefore, it is advised to only use fit and then transform if you are looking to generalize the model to new documents. For existing documents, it is best to use fit_transform directly as it only needs to generate the document embeddings once. Google Colaboratory \u00b6 Since we are using transformer-based embeddings you might want to leverage gpu-acceleration to speed up the model. For that, I have created a tutorial Google Colab Notebook that you can use to run the model as shown above. If you want to tweak the inner workings or follow along with the medium post, use this notebook instead. References \u00b6 Angelov, D. (2020). Top2Vec: Distributed Representations of Topics. arXiv preprint arXiv :2008.09470.","title":"Index"},{"location":"index.html#bertopic","text":"BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Corresponding medium post can be found here .","title":"BERTopic"},{"location":"index.html#about-the-project","text":"The initial purpose of this project was to generalize Top2Vec such that it could be used with state-of-art pre-trained transformer models. However, this proved difficult due to the different natures of Doc2Vec and transformer models. Instead, I decided to come up with a different algorithm that could use BERT and \ud83e\udd17 transformers embeddings. The results is BERTopic , an algorithm for generating topics using state-of-the-art embeddings.","title":"About the Project"},{"location":"index.html#installation","text":"PyTorch 1.2.0 or higher is recommended. If the install below gives an error, please install pytorch first here . Installation can be done using pypi : pip install bertopic","title":"Installation"},{"location":"index.html#usage","text":"Below is an example of how to use the model. The example uses the 20 newsgroups dataset. from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics , probabilities = model . fit_transform ( docs ) The resulting topics can be accessed through model.get_topic(topic) : >>> model . get_topic ( 9 ) [( 'game' , 0.005251396890032802 ), ( 'team' , 0.00482651185323754 ), ( 'hockey' , 0.004335032060690186 ), ( 'players' , 0.0034782716706978963 ), ( 'games' , 0.0032873248432630227 ), ( 'season' , 0.003218987432255393 ), ( 'play' , 0.0031855141725669637 ), ( 'year' , 0.002962343114817677 ), ( 'nhl' , 0.0029577648449943144 ), ( 'baseball' , 0.0029245163154193524 )] You can find an overview of all models currently in BERTopic here and here .","title":"Usage"},{"location":"index.html#custom-embeddings","text":"If you use BERTopic as shown above, then you are forced to use sentence-transformers as the main package for which to create embeddings. However, you might have your own model or package that you believe is better suited for representing documents. Fortunately, for those that want to use their own embeddings there is an option in BERTopic. For this example I will still be using sentence-transformers but the general principle holds: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Create topic model model = BERTopic ( verbose = True ) topics = model . fit_transform ( docs , embeddings ) Due to the stochastisch nature of UMAP, the results from BERTopic might differ even if you run the same code multiple times. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best. You only need to generate the embeddings itself once and run BERTopic several times with different parameters.","title":"Custom Embeddings"},{"location":"index.html#visualize-topic-probabilities","text":"The variable probabilities that is returned from transform() or fit_transform() can be used to understand how confident BERTopic is that certain topics can be found in a document. To visualize the distributions, we simply call: # Make sure to input the probabilities of a single document! model . visualize_distribution ( probabilities [ 0 ]) NOTE : The distribution of the probabilities does not give an indication to the distribution of the frequencies of topics across a document. It merely shows how confident BERTopic is that certain topics can be found in a document.","title":"Visualize Topic Probabilities"},{"location":"index.html#overview","text":"Methods Code Returns Access single topic model.get_topic(12) Tuple[Word, Score] Access all topics model.get_topics() List[Tuple[Word, Score]] Get single topic freq model.get_topic_freq(12) int Get all topic freq model.get_topics_freq() DataFrame Fit the model model.fit(docs]) - Fit the model and predict documents model.fit_transform(docs]) List[int], List[float] Predict new documents model.transform([new_doc]) List[int], List[float] Visualize Topic Probability Distribution model.visualize_distribution(probabilities) Matplotlib.Figure Save model model.save(\"my_model\") - Load model BERTopic.load(\"my_model\") - NOTE : The embeddings itself are not preserved in the model as they are only vital for creating the clusters. Therefore, it is advised to only use fit and then transform if you are looking to generalize the model to new documents. For existing documents, it is best to use fit_transform directly as it only needs to generate the document embeddings once.","title":"Overview"},{"location":"index.html#google-colaboratory","text":"Since we are using transformer-based embeddings you might want to leverage gpu-acceleration to speed up the model. For that, I have created a tutorial Google Colab Notebook that you can use to run the model as shown above. If you want to tweak the inner workings or follow along with the medium post, use this notebook instead.","title":"Google Colaboratory"},{"location":"index.html#references","text":"Angelov, D. (2020). Top2Vec: Distributed Representations of Topics. arXiv preprint arXiv :2008.09470.","title":"References"},{"location":"algorithm.html","text":"The Algorithm \u00b6 The algorithm contains, roughly, 3 stages: Extract document embeddings with Sentence Transformers Cluster document embeddings to create groups of similar documents with UMAP and HDBSCAN Extract and reduce topics with c-TF-IDF Sentence Transformer \u00b6 We start by creating document embeddings from a set of documents using sentence-transformer . These models are pre-trained for many language and are great for creating either document- or sentence-embeddings. If you have long documents, I would advise you to split up your documents into paragraphs or sentences as a BERT-based model in sentence-transformer typically has a token limit. UMAP + HDBSCAN \u00b6 Next, in order to cluster the documents using a clustering algorithm such as HDBSCAN we first need to reduce its dimensionality as HDBCAN is prone to the curse of dimensionality. Thus, we first lower dimensionality with UMAP as it preserves local structure well after which we can use HDBSCAN to cluster similar documents. c-TF-IDF \u00b6 What we want to know from the clusters that we generated, is what makes one cluster, based on their content, different from another? To solve this, we can modify TF-IDF such that it allows for interesting words per topic instead of per document. When you apply TF-IDF as usual on a set of documents, what you are basically doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of topics ! Each cluster is converted to a single document instead of a set of documents. Then, the frequency of word t are extracted for each class i and divided by the total number of words w . This action can now be seen as a form of regularization of frequent words in the class. Next, the total, unjoined, number of documents m is divided by the total frequency of word t across all classes n .","title":"Algorithm"},{"location":"algorithm.html#the-algorithm","text":"The algorithm contains, roughly, 3 stages: Extract document embeddings with Sentence Transformers Cluster document embeddings to create groups of similar documents with UMAP and HDBSCAN Extract and reduce topics with c-TF-IDF","title":"The Algorithm"},{"location":"algorithm.html#sentence-transformer","text":"We start by creating document embeddings from a set of documents using sentence-transformer . These models are pre-trained for many language and are great for creating either document- or sentence-embeddings. If you have long documents, I would advise you to split up your documents into paragraphs or sentences as a BERT-based model in sentence-transformer typically has a token limit.","title":"Sentence Transformer"},{"location":"algorithm.html#umap-hdbscan","text":"Next, in order to cluster the documents using a clustering algorithm such as HDBSCAN we first need to reduce its dimensionality as HDBCAN is prone to the curse of dimensionality. Thus, we first lower dimensionality with UMAP as it preserves local structure well after which we can use HDBSCAN to cluster similar documents.","title":"UMAP + HDBSCAN"},{"location":"algorithm.html#c-tf-idf","text":"What we want to know from the clusters that we generated, is what makes one cluster, based on their content, different from another? To solve this, we can modify TF-IDF such that it allows for interesting words per topic instead of per document. When you apply TF-IDF as usual on a set of documents, what you are basically doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of topics ! Each cluster is converted to a single document instead of a set of documents. Then, the frequency of word t are extracted for each class i and divided by the total number of words w . This action can now be seen as a form of regularization of frequent words in the class. Next, the total, unjoined, number of documents m is divided by the total frequency of word t across all classes n .","title":"c-TF-IDF"},{"location":"api/bertopic.html","text":"BERTopic \u00b6 BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Parameters Name Type Description Default bert_model str Model to use. Overview of options can be found here https://www.sbert.net/docs/pretrained_models.html 'distilbert-base-nli-mean-tokens' top_n_words int The number of words per topic to extract 20 nr_topics Union[int, str] Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. IF this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that have a similarity of at least 0.9, do not maps all others. None n_gram_range Tuple[int, int] The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. Note that this will not be used if you pass in your own CountVectorizer. (1, 1) min_topic_size int The minimum size of the topic. 30 n_neighbors int The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation (UMAP). 15 n_components int The dimension of the space to embed into when reducing dimensionality with UMAP. 5 stop_words Union[str, List[str]] Stopwords that can be used as either a list of strings, or the name of the language as a string. For example: 'english' or ['the', 'and', 'I']. Note that this will not be used if you pass in your own CountVectorizer. None verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False vectorizer CountVectorizer Pass in your own CountVectorizer from scikit-learn None Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics = model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) topics = model . fit_transform ( docs , embeddings ) Due to the stochastisch nature of UMAP, the results from BERTopic might differ and the quality can degrade. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best. fit ( self , documents , embeddings = None ) \u00b6 Show source code in bertopic\\model.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def fit ( self , documents : List [ str ], embeddings : np . ndarray = None ): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True).fit(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True).fit(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) self . fit_transform ( documents , embeddings ) return self Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Parameters Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) . fit ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) . fit ( docs , embeddings ) fit_transform ( self , documents , embeddings = None ) \u00b6 Show source code in bertopic\\model.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def fit_transform ( self , documents : List [ str ], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True) topics = model.fit_transform(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True) topics = model.fit_transform(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract BERT sentence embeddings if not isinstance ( embeddings , np . ndarray ): check_embeddings_shape ( embeddings , documents ) embeddings = self . _extract_embeddings ( documents . Document ) # Reduce dimensionality with UMAP umap_embeddings = self . _reduce_dimensionality ( embeddings ) # Cluster UMAP embeddings with HDBSCAN documents , probabilities = self . _cluster_embeddings ( umap_embeddings , documents ) # Extract topics by calculating c-TF-IDF c_tf_idf = self . _extract_topics ( documents ) if self . nr_topics : documents = self . _reduce_topics ( documents , c_tf_idf ) probabilities = self . _map_probabilities ( probabilities ) predictions = documents . Topic . to_list () return predictions , probabilities Fit the models on a collection of documents, generate topics, and return the docs with topics Parameters Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None Returns Type Description Tuple[List[int], numpy.ndarray] predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics = model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) topics = model . fit_transform ( docs , embeddings ) get_topic ( self , topic ) \u00b6 Show source code in bertopic\\model.py 436 437 438 439 440 441 442 443 444 445 446 447 448 def get_topic ( self , topic : int ) -> Union [ Dict [ str , Tuple [ str , float ]], bool ]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores Usage: ```python topic = model.get_topic(12) ``` \"\"\" if self . topics . get ( topic ): return self . topics [ topic ] else : return False Return top n words for a specific topic and their c-TF-IDF scores Usage: topic = model . get_topic ( 12 ) get_topic_freq ( self , topic ) \u00b6 Show source code in bertopic\\model.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def get_topic_freq ( self , topic : int ) -> int : \"\"\" Return the the size of a topic Arguments: topic: the name of the topic as retrieved by get_topics Usage: ```python frequency = model.get_topic_freq(12) ``` \"\"\" return self . topic_sizes . items ()[ topic ] Return the the size of a topic Parameters Name Type Description Default topic int the name of the topic as retrieved by get_topics required Usage: frequency = model . get_topic_freq ( 12 ) get_topics ( self ) \u00b6 Show source code in bertopic\\model.py 425 426 427 428 429 430 431 432 433 434 def get_topics ( self ) -> Dict [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score Usage: ```python all_topics = model.get_topics() ``` \"\"\" return self . topics Return topics with top n words and their c-TF-IDF score Usage: all_topics = model . get_topics () get_topics_freq ( self ) \u00b6 Show source code in bertopic\\model.py 450 451 452 453 454 455 456 457 458 459 def get_topics_freq ( self ) -> pd . DataFrame : \"\"\" Return the the size of topics (descending order) Usage: ```python frequency = model.get_topics_freq() ``` \"\"\" return pd . DataFrame ( self . topic_sizes . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False ) Return the the size of topics (descending order) Usage: frequency = model . get_topics_freq () load ( path ) (classmethod) \u00b6 Show source code in bertopic\\model.py 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the BERTopic file you want to load Usage: ```python BERTopic.load(\"my_model\") ``` \"\"\" with open ( path , 'rb' ) as file : return joblib . load ( file ) Loads the model from the specified path Parameters Name Type Description Default path str the location and name of the BERTopic file you want to load required Usage: BERTopic . load ( \"my_model\" ) save ( self , path ) \u00b6 Show source code in bertopic\\model.py 669 670 671 672 673 674 675 676 677 678 679 680 681 682 def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save Usage: ```python model.save(\"my_model\") ``` \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file ) Saves the model to the specified path Parameters Name Type Description Default path str the location and name of the file you want to save required Usage: model . save ( \"my_model\" ) transform ( self , documents , embeddings = None ) \u00b6 Show source code in bertopic\\model.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def transform ( self , documents : Union [ str , List [ str ]], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" After having fit a model, use transform to predict new instances Arguments: documents: A single document or a list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True).fit(docs) topics = model.transform(docs) ``` If you want to use your own embeddings: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True).fit(docs, embeddings) topics = model.transform(docs, embeddings) ``` \"\"\" if isinstance ( documents , str ): documents = [ documents ] if not isinstance ( embeddings , np . ndarray ): check_embeddings_shape ( embeddings , documents ) embeddings = self . _extract_embeddings ( documents ) umap_embeddings = self . umap_model . transform ( embeddings ) probabilities = hdbscan . membership_vector ( self . cluster_model , umap_embeddings ) predictions , _ = hdbscan . approximate_predict ( self . cluster_model , umap_embeddings ) if self . mapped_topics : predictions = self . _map_predictions ( predictions ) probabilities = self . _map_probabilities ( probabilities ) if len ( documents ) == 1 : probabilities = probabilities . flatten () return predictions , probabilities After having fit a model, use transform to predict new instances Parameters Name Type Description Default documents Union[str, List[str]] A single document or a list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model. None Returns Type Description Tuple[List[int], numpy.ndarray] predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) . fit ( docs ) topics = model . transform ( docs ) If you want to use your own embeddings: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) . fit ( docs , embeddings ) topics = model . transform ( docs , embeddings ) visualize_distribution ( self , probabilities , min_probability = 0.015 , figsize = ( 10 , 5 ), save = False ) \u00b6 Show source code in bertopic\\model.py 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 def visualize_distribution ( self , probabilities : np . ndarray , min_probability : float = 0.015 , figsize : tuple = ( 10 , 5 ), save : bool = False ): \"\"\" Visualize the distribution of topic probabilities Arguments: probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. figsize: The size of the figure save: Whether to save the resulting graph to probility.png Usage: Make sure to fit the model before and only input the probabilities of a single document: ```python model.visualize_distribution(probabilities[0]) ``` ![](../img/probabilities.png) \"\"\" # Get values and indices equal or exceed the minimum probability labels_idx = np . argwhere ( probabilities >= min_probability ) . flatten () vals = probabilities [ labels_idx ] . tolist () # Create labels labels = [] for idx in labels_idx : label = [] words = self . get_topic ( idx ) if words : for word in words [: 5 ]: label . append ( word [ 0 ]) label = str ( r \"$\\bf{Topic }$ \" + r \"$\\bf{\" + str ( idx ) + \":}$ \" + \" \" . join ( label )) labels . append ( label ) else : print ( idx , probabilities [ idx ]) vals . remove ( probabilities [ idx ]) pos = range ( len ( vals )) # Create figure fig , ax = plt . subplots ( figsize = figsize ) plt . hlines ( y = pos , xmin = 0 , xmax = vals , color = '#333F4B' , alpha = 0.2 , linewidth = 15 ) plt . hlines ( y = np . argmax ( vals ), xmin = 0 , xmax = max ( vals ), color = '#333F4B' , alpha = 1 , linewidth = 15 ) # Set ticks and labels ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) ax . set_xlabel ( 'Probability' , fontsize = 15 , fontweight = 'black' , color = '#333F4B' ) ax . set_ylabel ( '' ) plt . yticks ( pos , labels ) fig . text ( 0 , 1 , 'Topic Probability Distribution' , fontsize = 15 , fontweight = 'black' , color = '#333F4B' ) # Update spine style ax . spines [ 'right' ] . set_visible ( False ) ax . spines [ 'top' ] . set_visible ( False ) ax . spines [ 'left' ] . set_bounds ( pos [ 0 ], pos [ - 1 ]) ax . spines [ 'bottom' ] . set_bounds ( 0 , max ( vals )) ax . spines [ 'bottom' ] . set_position (( 'axes' , - 0.02 )) ax . spines [ 'left' ] . set_position (( 'axes' , 0.02 )) fig . tight_layout () if save : fig . savefig ( \"probability.png\" , dpi = 300 , bbox_inches = 'tight' ) Visualize the distribution of topic probabilities Parameters Name Type Description Default probabilities ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 figsize tuple The size of the figure (10, 5) save bool Whether to save the resulting graph to probility.png False Usage: Make sure to fit the model before and only input the probabilities of a single document: model . visualize_distribution ( probabilities [ 0 ])","title":"BERTopic"},{"location":"api/bertopic.html#bertopic","text":"BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Parameters Name Type Description Default bert_model str Model to use. Overview of options can be found here https://www.sbert.net/docs/pretrained_models.html 'distilbert-base-nli-mean-tokens' top_n_words int The number of words per topic to extract 20 nr_topics Union[int, str] Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. IF this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that have a similarity of at least 0.9, do not maps all others. None n_gram_range Tuple[int, int] The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. Note that this will not be used if you pass in your own CountVectorizer. (1, 1) min_topic_size int The minimum size of the topic. 30 n_neighbors int The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation (UMAP). 15 n_components int The dimension of the space to embed into when reducing dimensionality with UMAP. 5 stop_words Union[str, List[str]] Stopwords that can be used as either a list of strings, or the name of the language as a string. For example: 'english' or ['the', 'and', 'I']. Note that this will not be used if you pass in your own CountVectorizer. None verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False vectorizer CountVectorizer Pass in your own CountVectorizer from scikit-learn None Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics = model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) topics = model . fit_transform ( docs , embeddings ) Due to the stochastisch nature of UMAP, the results from BERTopic might differ and the quality can degrade. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best.","title":"BERTopic"},{"location":"api/bertopic.html#bertopic.model.BERTopic.fit","text":"Show source code in bertopic\\model.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def fit ( self , documents : List [ str ], embeddings : np . ndarray = None ): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True).fit(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True).fit(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) self . fit_transform ( documents , embeddings ) return self Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Parameters Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) . fit ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) . fit ( docs , embeddings )","title":"fit()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.fit_transform","text":"Show source code in bertopic\\model.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def fit_transform ( self , documents : List [ str ], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True) topics = model.fit_transform(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True) topics = model.fit_transform(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract BERT sentence embeddings if not isinstance ( embeddings , np . ndarray ): check_embeddings_shape ( embeddings , documents ) embeddings = self . _extract_embeddings ( documents . Document ) # Reduce dimensionality with UMAP umap_embeddings = self . _reduce_dimensionality ( embeddings ) # Cluster UMAP embeddings with HDBSCAN documents , probabilities = self . _cluster_embeddings ( umap_embeddings , documents ) # Extract topics by calculating c-TF-IDF c_tf_idf = self . _extract_topics ( documents ) if self . nr_topics : documents = self . _reduce_topics ( documents , c_tf_idf ) probabilities = self . _map_probabilities ( probabilities ) predictions = documents . Topic . to_list () return predictions , probabilities Fit the models on a collection of documents, generate topics, and return the docs with topics Parameters Name Type Description Default documents List[str] A list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None Returns Type Description Tuple[List[int], numpy.ndarray] predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) topics = model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) topics = model . fit_transform ( docs , embeddings )","title":"fit_transform()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.get_topic","text":"Show source code in bertopic\\model.py 436 437 438 439 440 441 442 443 444 445 446 447 448 def get_topic ( self , topic : int ) -> Union [ Dict [ str , Tuple [ str , float ]], bool ]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores Usage: ```python topic = model.get_topic(12) ``` \"\"\" if self . topics . get ( topic ): return self . topics [ topic ] else : return False Return top n words for a specific topic and their c-TF-IDF scores Usage: topic = model . get_topic ( 12 )","title":"get_topic()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.get_topic_freq","text":"Show source code in bertopic\\model.py 461 462 463 464 465 466 467 468 469 470 471 472 473 def get_topic_freq ( self , topic : int ) -> int : \"\"\" Return the the size of a topic Arguments: topic: the name of the topic as retrieved by get_topics Usage: ```python frequency = model.get_topic_freq(12) ``` \"\"\" return self . topic_sizes . items ()[ topic ] Return the the size of a topic Parameters Name Type Description Default topic int the name of the topic as retrieved by get_topics required Usage: frequency = model . get_topic_freq ( 12 )","title":"get_topic_freq()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.get_topics","text":"Show source code in bertopic\\model.py 425 426 427 428 429 430 431 432 433 434 def get_topics ( self ) -> Dict [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score Usage: ```python all_topics = model.get_topics() ``` \"\"\" return self . topics Return topics with top n words and their c-TF-IDF score Usage: all_topics = model . get_topics ()","title":"get_topics()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.get_topics_freq","text":"Show source code in bertopic\\model.py 450 451 452 453 454 455 456 457 458 459 def get_topics_freq ( self ) -> pd . DataFrame : \"\"\" Return the the size of topics (descending order) Usage: ```python frequency = model.get_topics_freq() ``` \"\"\" return pd . DataFrame ( self . topic_sizes . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False ) Return the the size of topics (descending order) Usage: frequency = model . get_topics_freq ()","title":"get_topics_freq()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.load","text":"Show source code in bertopic\\model.py 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the BERTopic file you want to load Usage: ```python BERTopic.load(\"my_model\") ``` \"\"\" with open ( path , 'rb' ) as file : return joblib . load ( file ) Loads the model from the specified path Parameters Name Type Description Default path str the location and name of the BERTopic file you want to load required Usage: BERTopic . load ( \"my_model\" )","title":"load()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.save","text":"Show source code in bertopic\\model.py 669 670 671 672 673 674 675 676 677 678 679 680 681 682 def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save Usage: ```python model.save(\"my_model\") ``` \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file ) Saves the model to the specified path Parameters Name Type Description Default path str the location and name of the file you want to save required Usage: model . save ( \"my_model\" )","title":"save()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.transform","text":"Show source code in bertopic\\model.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def transform ( self , documents : Union [ str , List [ str ]], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" After having fit a model, use transform to predict new instances Arguments: documents: A single document or a list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] model = BERTopic(\"distilbert-base-nli-mean-tokens\", verbose=True).fit(docs) topics = model.transform(docs) ``` If you want to use your own embeddings: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model model = BERTopic(None, verbose=True).fit(docs, embeddings) topics = model.transform(docs, embeddings) ``` \"\"\" if isinstance ( documents , str ): documents = [ documents ] if not isinstance ( embeddings , np . ndarray ): check_embeddings_shape ( embeddings , documents ) embeddings = self . _extract_embeddings ( documents ) umap_embeddings = self . umap_model . transform ( embeddings ) probabilities = hdbscan . membership_vector ( self . cluster_model , umap_embeddings ) predictions , _ = hdbscan . approximate_predict ( self . cluster_model , umap_embeddings ) if self . mapped_topics : predictions = self . _map_predictions ( predictions ) probabilities = self . _map_probabilities ( probabilities ) if len ( documents ) == 1 : probabilities = probabilities . flatten () return predictions , probabilities After having fit a model, use transform to predict new instances Parameters Name Type Description Default documents Union[str, List[str]] A single document or a list of documents to fit on required embeddings ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model. None Returns Type Description Tuple[List[int], numpy.ndarray] predictions: Topic predictions for each documents probabilities: The topic probability distribution Usage: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] model = BERTopic ( \"distilbert-base-nli-mean-tokens\" , verbose = True ) . fit ( docs ) topics = model . transform ( docs ) If you want to use your own embeddings: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model model = BERTopic ( None , verbose = True ) . fit ( docs , embeddings ) topics = model . transform ( docs , embeddings )","title":"transform()"},{"location":"api/bertopic.html#bertopic.model.BERTopic.visualize_distribution","text":"Show source code in bertopic\\model.py 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 def visualize_distribution ( self , probabilities : np . ndarray , min_probability : float = 0.015 , figsize : tuple = ( 10 , 5 ), save : bool = False ): \"\"\" Visualize the distribution of topic probabilities Arguments: probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. figsize: The size of the figure save: Whether to save the resulting graph to probility.png Usage: Make sure to fit the model before and only input the probabilities of a single document: ```python model.visualize_distribution(probabilities[0]) ``` ![](../img/probabilities.png) \"\"\" # Get values and indices equal or exceed the minimum probability labels_idx = np . argwhere ( probabilities >= min_probability ) . flatten () vals = probabilities [ labels_idx ] . tolist () # Create labels labels = [] for idx in labels_idx : label = [] words = self . get_topic ( idx ) if words : for word in words [: 5 ]: label . append ( word [ 0 ]) label = str ( r \"$\\bf{Topic }$ \" + r \"$\\bf{\" + str ( idx ) + \":}$ \" + \" \" . join ( label )) labels . append ( label ) else : print ( idx , probabilities [ idx ]) vals . remove ( probabilities [ idx ]) pos = range ( len ( vals )) # Create figure fig , ax = plt . subplots ( figsize = figsize ) plt . hlines ( y = pos , xmin = 0 , xmax = vals , color = '#333F4B' , alpha = 0.2 , linewidth = 15 ) plt . hlines ( y = np . argmax ( vals ), xmin = 0 , xmax = max ( vals ), color = '#333F4B' , alpha = 1 , linewidth = 15 ) # Set ticks and labels ax . tick_params ( axis = 'both' , which = 'major' , labelsize = 12 ) ax . set_xlabel ( 'Probability' , fontsize = 15 , fontweight = 'black' , color = '#333F4B' ) ax . set_ylabel ( '' ) plt . yticks ( pos , labels ) fig . text ( 0 , 1 , 'Topic Probability Distribution' , fontsize = 15 , fontweight = 'black' , color = '#333F4B' ) # Update spine style ax . spines [ 'right' ] . set_visible ( False ) ax . spines [ 'top' ] . set_visible ( False ) ax . spines [ 'left' ] . set_bounds ( pos [ 0 ], pos [ - 1 ]) ax . spines [ 'bottom' ] . set_bounds ( 0 , max ( vals )) ax . spines [ 'bottom' ] . set_position (( 'axes' , - 0.02 )) ax . spines [ 'left' ] . set_position (( 'axes' , 0.02 )) fig . tight_layout () if save : fig . savefig ( \"probability.png\" , dpi = 300 , bbox_inches = 'tight' ) Visualize the distribution of topic probabilities Parameters Name Type Description Default probabilities ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 figsize tuple The size of the figure (10, 5) save bool Whether to save the resulting graph to probility.png False Usage: Make sure to fit the model before and only input the probabilities of a single document: model . visualize_distribution ( probabilities [ 0 ])","title":"visualize_distribution()"},{"location":"api/ctfidf.html","text":"c-TF-IDF \u00b6 A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base. C-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes by joining all documents per class. Thus, each class is converted to a single document instead of set of documents. Then, the frequency of words t are extracted for each class i and divided by the total number of words w . Next, the total, unjoined, number of documents across all classes m is divided by the total sum of word i across all classes. fit ( self , X , n_samples ) \u00b6 Show source code in bertopic\\ctfidf.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def fit ( self , X : sp . csr_matrix , n_samples : int ): \"\"\"Learn the idf vector (global term weights). Arguments: X: A matrix of term/token counts. n_samples: Number of total documents \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape self . df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) idf = np . log ( n_samples / self . df ) self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self Learn the idf vector (global term weights). Parameters Name Type Description Default X csr_matrix A matrix of term/token counts. required n_samples int Number of total documents required transform ( self , X , copy = True ) \u00b6 Show source code in bertopic\\ctfidf.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def transform ( self , X : sp . csr_matrix , copy = True ): \"\"\"Transform a count-based matrix to c-TF-IDF Arguments: X (sparse matrix): A matrix of term/token counts. Returns: X (sparse matrix): A c-TF-IDF matrix \"\"\" if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) X = X * self . _idf_diag return X Transform a count-based matrix to c-TF-IDF Parameters Name Type Description Default X csr_matrix A matrix of term/token counts. required Returns Type Description `` X (sparse matrix): A c-TF-IDF matrix","title":"cTFIDF"},{"location":"api/ctfidf.html#c-tf-idf","text":"A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base. C-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes by joining all documents per class. Thus, each class is converted to a single document instead of set of documents. Then, the frequency of words t are extracted for each class i and divided by the total number of words w . Next, the total, unjoined, number of documents across all classes m is divided by the total sum of word i across all classes.","title":"c-TF-IDF"},{"location":"api/ctfidf.html#bertopic.ctfidf.ClassTFIDF.fit","text":"Show source code in bertopic\\ctfidf.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def fit ( self , X : sp . csr_matrix , n_samples : int ): \"\"\"Learn the idf vector (global term weights). Arguments: X: A matrix of term/token counts. n_samples: Number of total documents \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape self . df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) idf = np . log ( n_samples / self . df ) self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self Learn the idf vector (global term weights). Parameters Name Type Description Default X csr_matrix A matrix of term/token counts. required n_samples int Number of total documents required","title":"fit()"},{"location":"api/ctfidf.html#bertopic.ctfidf.ClassTFIDF.transform","text":"Show source code in bertopic\\ctfidf.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def transform ( self , X : sp . csr_matrix , copy = True ): \"\"\"Transform a count-based matrix to c-TF-IDF Arguments: X (sparse matrix): A matrix of term/token counts. Returns: X (sparse matrix): A c-TF-IDF matrix \"\"\" if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) X = X * self . _idf_diag return X Transform a count-based matrix to c-TF-IDF Parameters Name Type Description Default X csr_matrix A matrix of term/token counts. required Returns Type Description `` X (sparse matrix): A c-TF-IDF matrix","title":"transform()"}]}