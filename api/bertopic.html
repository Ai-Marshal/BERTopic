
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="Leveraging BERT and a class-based TF-IDF to create easily interpretable topics." name="description"/>
<link href="https://maartengr.github.io/BERTopic/api/bertopic.html" rel="canonical"/>
<meta content="Maarten P. Grootendorst" name="author"/>
<link href="../icon.png" rel="shortcut icon"/>
<meta content="mkdocs-1.1.2, mkdocs-material-6.1.7" name="generator"/>
<title>BERTopic - BERTopic</title>
<link href="../assets/stylesheets/main.19753c6b.min.css" rel="stylesheet"/>
<link href="../assets/stylesheets/palette.196e0c26.min.css" rel="stylesheet"/>
<meta content="#000000" name="theme-color"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,400i,700%7CUbuntu+Mono&amp;display=fallback" rel="stylesheet"/>
<style>body,input{font-family:"Ubuntu",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Ubuntu Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
<link href="../style.css" rel="stylesheet"/>
</head>
<body data-md-color-accent="grey" data-md-color-primary="black" data-md-color-scheme="" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#bertopic">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="Header" class="md-header-nav md-grid">
<a aria-label="BERTopic" class="md-header-nav__button md-logo" href="https://maartengr.github.io/BERTopic/" title="BERTopic">
<img alt="logo" src="../icon_white.png"/>
</a>
<label class="md-header-nav__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"></path></svg>
</label>
<div class="md-header-nav__title" data-md-component="header-title">
<div class="md-header-nav__ellipsis md-ellipsis">
          BERTopic
        </div>
</div>
<label class="md-header-nav__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" data-md-state="active" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</label>
<button aria-label="Clear" class="md-search__icon md-icon" data-md-component="search-reset" tabindex="-1" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"></path></svg>
</button>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header-nav__source">
<a class="md-source" href="https://github.com/MaartenGr/BERTopic/" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="BERTopic" class="md-nav__button md-logo" href="https://maartengr.github.io/BERTopic/" title="BERTopic">
<img alt="logo" src="../icon_white.png"/>
</a>
    BERTopic
  </label>
<div class="md-nav__source">
<a class="md-source" href="https://github.com/MaartenGr/BERTopic/" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../index.html">
      Home
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/algorithm/algorithm.html">
      The Algorithm
    </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" id="nav-3" type="checkbox"/>
<label class="md-nav__link" for="nav-3">
      Guides
      <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Guides" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="nav-3">
<span class="md-nav__icon md-icon"></span>
        Guides
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/quickstart/quickstart.html">
      Getting Started
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/embeddings/embeddings.html">
      Embeddings
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/topicsovertime/topicsovertime.html">
      Dynamic Topic Modeling
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/visualization/visualization.html">
      Topic Visualization
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/topicreduction/topicreduction.html">
      Topic Reduction
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/topicrepresentation/topicrepresentation.html">
      Topic Representation
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/search/search.html">
      Search Topics
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../tutorial/models/models.html">
      Custom Models
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" data-md-toggle="nav-4" id="nav-4" type="checkbox"/>
<label class="md-nav__link" for="nav-4">
      API
      <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="API" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="nav-4">
<span class="md-nav__icon md-icon"></span>
        API
      </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
        BERTopic
        <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="bertopic.html">
      BERTopic
    </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic">
    bertopic._bertopic.BERTopic
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.__init__">
    __init__()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.find_topics">
    find_topics()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.fit">
    fit()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.fit_transform">
    fit_transform()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.get_params">
    get_params()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.get_topic">
    get_topic()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.get_topic_freq">
    get_topic_freq()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.get_topic_info">
    get_topic_info()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.get_topics">
    get_topics()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.load">
    load()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.reduce_topics">
    reduce_topics()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.save">
    save()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.topics_over_time">
    topics_over_time()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.transform">
    transform()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.update_topics">
    update_topics()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.visualize_distribution">
    visualize_distribution()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.visualize_topics">
    visualize_topics()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.visualize_topics_over_time">
    visualize_topics_over_time()
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="ctfidf.html">
      cTFIDF
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="mmr.html">
      MMR
    </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../faq.html">
      FAQ
    </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../changelog.html">
      Changelog
    </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic">
    bertopic._bertopic.BERTopic
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.__init__">
    __init__()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.find_topics">
    find_topics()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.fit">
    fit()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.fit_transform">
    fit_transform()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.get_params">
    get_params()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.get_topic">
    get_topic()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.get_topic_freq">
    get_topic_freq()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.get_topic_info">
    get_topic_info()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.get_topics">
    get_topics()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.load">
    load()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.reduce_topics">
    reduce_topics()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.save">
    save()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.topics_over_time">
    topics_over_time()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.transform">
    transform()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.update_topics">
    update_topics()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.visualize_distribution">
    visualize_distribution()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.visualize_topics">
    visualize_topics()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#bertopic._bertopic.BERTopic.visualize_topics_over_time">
    visualize_topics_over_time()
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/MaartenGr/BERTopic/edit/master/docs/api/bertopic.md" title="Edit this page">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"></path></svg>
</a>
<h1 id="bertopic"><code>BERTopic</code><a class="headerlink" href="#bertopic" title="Permanent link">¶</a></h1>
<div class="doc doc-object doc-class">
<h2 class="hidden-toc" href="#bertopic._bertopic.BERTopic" id="bertopic._bertopic.BERTopic" style="visibility: hidden; position: absolute;">
<a class="headerlink" href="#bertopic._bertopic.BERTopic" title="Permanent link">¶</a></h2>
<div class="doc doc-contents first">
<p>BERTopic is a topic modeling technique that leverages BERT embeddings and
c-TF-IDF to create dense clusters allowing for easily interpretable topics
whilst keeping important words in the topic descriptions.</p>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probabilities</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre>
</div>
<p>If you want to use your own embedding model, use it as follows:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">embedding_model</span><span class="o">=</span><span class="n">sentence_model</span><span class="p">)</span>
</code></pre>
</div>
<p>Due to the stochastisch nature of UMAP, the results from BERTopic might differ
and the quality can degrade. Using your own embeddings allows you to
try out BERTopic several times until you find the topics that suit
you best.</p>
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.__init__">
<code class="highlight language-python">
__init__<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="s1">'english'</span><span class="p">,</span> <span class="n">top_n_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">min_topic_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">embedding_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">umap_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hdbscan_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vectorizer_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-special"><code>special</code></small>
</span>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.__init__" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>BERTopic initialization</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>language</code></td>
<td><code>str</code></td>
<td>
<p>The main language used in your documents. For a full overview of
      supported languages see bertopic.embeddings.languages. Select
      "multilingual" to load in a model that support 50+ languages.</p>
</td>
<td><code>'english'</code></td>
</tr>
<tr>
<td><code>top_n_words</code></td>
<td><code>int</code></td>
<td>
<p>The number of words per topic to extract</p>
</td>
<td><code>10</code></td>
</tr>
<tr>
<td><code>n_gram_range</code></td>
<td><code>Tuple[int, int]</code></td>
<td>
<p>The n-gram range for the CountVectorizer.
          Advised to keep high values between 1 and 3.
          More would likely lead to memory issues.
          NOTE: This param will not be used if you pass in your own
          CountVectorizer.</p>
</td>
<td><code>(1, 1)</code></td>
</tr>
<tr>
<td><code>min_topic_size</code></td>
<td><code>int</code></td>
<td>
<p>The minimum size of the topic. Increasing this value will lead
            to a lower number of clusters/topics.</p>
</td>
<td><code>10</code></td>
</tr>
<tr>
<td><code>nr_topics</code></td>
<td><code>Union[int, str]</code></td>
<td>
<p>Specifying the number of topics will reduce the initial
       number of topics to the value specified. This reduction can take
       a while as each reduction in topics (-1) activates a c-TF-IDF
       calculation. If this is set to None, no reduction is applied. Use
       "auto" to automatically reduce topics that have a similarity of at
       least 0.9, do not maps all others.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>low_memory</code></td>
<td><code>bool</code></td>
<td>
<p>Sets UMAP low memory to True to make sure less memory is used.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>calculate_probabilities</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to calculate the topic probabilities. This could
                     slow down the extraction of topics if you have many
                     documents (&gt; 100_000). Set this only to True if you
                     have a low amount of documents or if you do not mind
                     more computation time.
                     NOTE: since probabilities are not calculated, you cannot
                     use the corresponding visualization <code>visualize_probabilities</code>.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>verbose</code></td>
<td><code>bool</code></td>
<td>
<p>Changes the verbosity of the model, Set to True if you want
     to track the stages of the model.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>embedding_model</code></td>
<td><code>Union[str, sentence_transformers.SentenceTransformer.SentenceTransformer, flair.embeddings.document.DocumentEmbeddings, flair.embeddings.token.TokenEmbeddings]</code></td>
<td>
<p>Use a custom embedding model. You can pass in a string related
             to one of the following models:
             https://www.sbert.net/docs/pretrained_models.html
             You can also pass in a SentenceTransformer() model or a Flair
             DocumentEmbedding model.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>umap_model</code></td>
<td><code>UMAP</code></td>
<td>
<p>Pass in a umap.UMAP model to be used instead of the default</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>hdbscan_model</code></td>
<td><code>HDBSCAN</code></td>
<td>
<p>Pass in a hdbscan.HDBSCAN model to be used instead of the default</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>vectorizer_model</code></td>
<td><code>CountVectorizer</code></td>
<td>
<p>Pass in a CountVectorizer instead of the default</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">language</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"english"</span><span class="p">,</span>
             <span class="n">top_n_words</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
             <span class="n">n_gram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
             <span class="n">min_topic_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
             <span class="n">nr_topics</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">low_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
             <span class="n">calculate_probabilities</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
             <span class="n">embedding_model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span>
                                    <span class="n">SentenceTransformer</span><span class="p">,</span>
                                    <span class="n">DocumentEmbeddings</span><span class="p">,</span>
                                    <span class="n">TokenEmbeddings</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">umap_model</span><span class="p">:</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">hdbscan_model</span><span class="p">:</span> <span class="n">hdbscan</span><span class="o">.</span><span class="n">HDBSCAN</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">vectorizer_model</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
             <span class="p">):</span>
    <span class="sd">"""BERTopic initialization</span>

<span class="sd">    Arguments:</span>
<span class="sd">        language: The main language used in your documents. For a full overview of</span>
<span class="sd">                  supported languages see bertopic.embeddings.languages. Select</span>
<span class="sd">                  "multilingual" to load in a model that support 50+ languages.</span>
<span class="sd">        top_n_words: The number of words per topic to extract</span>
<span class="sd">        n_gram_range: The n-gram range for the CountVectorizer.</span>
<span class="sd">                      Advised to keep high values between 1 and 3.</span>
<span class="sd">                      More would likely lead to memory issues.</span>
<span class="sd">                      NOTE: This param will not be used if you pass in your own</span>
<span class="sd">                      CountVectorizer.</span>
<span class="sd">        min_topic_size: The minimum size of the topic. Increasing this value will lead</span>
<span class="sd">                        to a lower number of clusters/topics.</span>
<span class="sd">        nr_topics: Specifying the number of topics will reduce the initial</span>
<span class="sd">                   number of topics to the value specified. This reduction can take</span>
<span class="sd">                   a while as each reduction in topics (-1) activates a c-TF-IDF</span>
<span class="sd">                   calculation. If this is set to None, no reduction is applied. Use</span>
<span class="sd">                   "auto" to automatically reduce topics that have a similarity of at</span>
<span class="sd">                   least 0.9, do not maps all others.</span>
<span class="sd">        low_memory: Sets UMAP low memory to True to make sure less memory is used.</span>
<span class="sd">        calculate_probabilities: Whether to calculate the topic probabilities. This could</span>
<span class="sd">                                 slow down the extraction of topics if you have many</span>
<span class="sd">                                 documents (&gt; 100_000). Set this only to True if you</span>
<span class="sd">                                 have a low amount of documents or if you do not mind</span>
<span class="sd">                                 more computation time.</span>
<span class="sd">                                 NOTE: since probabilities are not calculated, you cannot</span>
<span class="sd">                                 use the corresponding visualization `visualize_probabilities`.</span>
<span class="sd">        verbose: Changes the verbosity of the model, Set to True if you want</span>
<span class="sd">                 to track the stages of the model.</span>
<span class="sd">        embedding_model: Use a custom embedding model. You can pass in a string related</span>
<span class="sd">                         to one of the following models:</span>
<span class="sd">                         https://www.sbert.net/docs/pretrained_models.html</span>
<span class="sd">                         You can also pass in a SentenceTransformer() model or a Flair</span>
<span class="sd">                         DocumentEmbedding model.</span>
<span class="sd">        umap_model: Pass in a umap.UMAP model to be used instead of the default</span>
<span class="sd">        hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default</span>
<span class="sd">        vectorizer_model: Pass in a CountVectorizer instead of the default</span>
<span class="sd">    """</span>
    <span class="c1"># Topic-based parameters</span>
    <span class="k">if</span> <span class="n">top_n_words</span> <span class="o">&gt;</span> <span class="mi">30</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"top_n_words should be lower or equal to 30. The preferred value is 10."</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">top_n_words</span> <span class="o">=</span> <span class="n">top_n_words</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_topic_size</span> <span class="o">=</span> <span class="n">min_topic_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nr_topics</span> <span class="o">=</span> <span class="n">nr_topics</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">low_memory</span> <span class="o">=</span> <span class="n">low_memory</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">calculate_probabilities</span> <span class="o">=</span> <span class="n">calculate_probabilities</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

    <span class="c1"># Embedding model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">language</span> <span class="o">=</span> <span class="n">language</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">embedding_model</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">embedding_model</span>

    <span class="c1"># Vectorizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_gram_range</span> <span class="o">=</span> <span class="n">n_gram_range</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer_model</span> <span class="o">=</span> <span class="n">vectorizer_model</span> <span class="ow">or</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_gram_range</span><span class="p">)</span>

    <span class="c1"># UMAP</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">umap_model</span> <span class="o">=</span> <span class="n">umap_model</span> <span class="ow">or</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                                              <span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                              <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                                              <span class="n">metric</span><span class="o">=</span><span class="s1">'cosine'</span><span class="p">,</span>
                                              <span class="n">low_memory</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">low_memory</span><span class="p">)</span>

    <span class="c1"># HDBSCAN</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hdbscan_model</span> <span class="o">=</span> <span class="n">hdbscan_model</span> <span class="ow">or</span> <span class="n">hdbscan</span><span class="o">.</span><span class="n">HDBSCAN</span><span class="p">(</span><span class="n">min_cluster_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_topic_size</span><span class="p">,</span>
                                                          <span class="n">metric</span><span class="o">=</span><span class="s1">'euclidean'</span><span class="p">,</span>
                                                          <span class="n">cluster_selection_method</span><span class="o">=</span><span class="s1">'eom'</span><span class="p">,</span>
                                                          <span class="n">prediction_data</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">topics</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reduced_topics_mapped</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mapped_topics</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">topic_embeddings</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">topic_sim_matrix</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">custom_embeddings</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">set_level</span><span class="p">(</span><span class="s2">"DEBUG"</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.find_topics">
<code class="highlight language-python">
find_topics<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">search_term</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.find_topics" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Find topics most similar to a search_term</p>
<p>Creates an embedding for search_term and compares that with
the topic embeddings. The most similar topics are returned
along with their similarity values.</p>
<p>The search_term can be of any size but since it compares
with the topic representation it is advised to keep it
below 5 words.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>search_term</code></td>
<td><code>str</code></td>
<td>
<p>the term you want to use to search for topics</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>top_n</code></td>
<td><code>int</code></td>
<td>
<p>the number of topics to return</p>
</td>
<td><code>5</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tuple[List[int], List[float]]</code></td>
<td>
<p>similar_topics: the most similar topics from high to low
similarity: the similarity scores from high to low</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<p>You can use the underlying embedding model to find topics that
best represent the search term:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">"sports"</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre>
</div>
<p>Note that the search query is typically more accurate if the
search_term consists of a phrase or multiple words.</p>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">find_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">search_term</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                <span class="n">top_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
    <span class="sd">""" Find topics most similar to a search_term</span>

<span class="sd">    Creates an embedding for search_term and compares that with</span>
<span class="sd">    the topic embeddings. The most similar topics are returned</span>
<span class="sd">    along with their similarity values.</span>

<span class="sd">    The search_term can be of any size but since it compares</span>
<span class="sd">    with the topic representation it is advised to keep it</span>
<span class="sd">    below 5 words.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        search_term: the term you want to use to search for topics</span>
<span class="sd">        top_n: the number of topics to return</span>

<span class="sd">    Returns:</span>
<span class="sd">        similar_topics: the most similar topics from high to low</span>
<span class="sd">        similarity: the similarity scores from high to low</span>

<span class="sd">    Usage:</span>

<span class="sd">    You can use the underlying embedding model to find topics that</span>
<span class="sd">    best represent the search term:</span>

<span class="sd">    ```python</span>
<span class="sd">    topics, similarity = model.find_topics("sports", top_n=5)</span>
<span class="sd">    ```</span>

<span class="sd">    Note that the search query is typically more accurate if the</span>
<span class="sd">    search_term consists of a phrase or multiple words.</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_embeddings</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">"This method can only be used if you did not use custom embeddings."</span><span class="p">)</span>

    <span class="n">topic_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">topic_list</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="c1"># Extract search_term embeddings and compare with topic embeddings</span>
    <span class="n">search_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_embeddings</span><span class="p">([</span><span class="n">search_term</span><span class="p">],</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">sims</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">search_embedding</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="c1"># Extract topics most similar to search_term</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">sims</span><span class="p">)[</span><span class="o">-</span><span class="n">top_n</span><span class="p">:]</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="p">[</span><span class="n">sims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">similar_topics</span> <span class="o">=</span> <span class="p">[</span><span class="n">topic_list</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.fit">
<code class="highlight language-python">
fit<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.fit" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>documents</code></td>
<td><code>List[str]</code></td>
<td>
<p>A list of documents to fit on</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>embeddings</code></td>
<td><code>ndarray</code></td>
<td>
<p>Pre-trained document embeddings. These can be used
        instead of the sentence-transformer model</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre>
</div>
<p>If you want to use your own embeddings, use it as follows:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Create embeddings</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">sentence_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create topic model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">documents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">""" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics</span>

<span class="sd">    Arguments:</span>
<span class="sd">        documents: A list of documents to fit on</span>
<span class="sd">        embeddings: Pre-trained document embeddings. These can be used</span>
<span class="sd">                    instead of the sentence-transformer model</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>

<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>
<span class="sd">    model = BERTopic(verbose=True).fit(docs)</span>
<span class="sd">    ```</span>

<span class="sd">    If you want to use your own embeddings, use it as follows:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>
<span class="sd">    from sentence_transformers import SentenceTransformer</span>

<span class="sd">    # Create embeddings</span>
<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>
<span class="sd">    sentence_model = SentenceTransformer("distilbert-base-nli-mean-tokens")</span>
<span class="sd">    embeddings = sentence_model.encode(docs, show_progress_bar=True)</span>

<span class="sd">    # Create topic model</span>
<span class="sd">    model = BERTopic(verbose=True).fit(docs, embeddings)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.fit_transform">
<code class="highlight language-python">
fit_transform<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.fit_transform" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Fit the models on a collection of documents, generate topics, and return the docs with topics</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>documents</code></td>
<td><code>List[str]</code></td>
<td>
<p>A list of documents to fit on</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>embeddings</code></td>
<td><code>ndarray</code></td>
<td>
<p>Pre-trained document embeddings. These can be used
        instead of the sentence-transformer model</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tuple[List[int], Optional[numpy.ndarray]]</code></td>
<td>
<p>predictions: Topic predictions for each documents
probabilities: The topic probability distribution which is returned by default.
               If <code>low_memory</code> in BERTopic is set to False, then the
               probabilities are not calculated to speed up computation and
               decrease memory usage.</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre>
</div>
<p>If you want to use your own embeddings, use it as follows:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Create embeddings</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">sentence_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create topic model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">documents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                  <span class="n">embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                                          <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="kc">None</span><span class="p">]]:</span>
    <span class="sd">""" Fit the models on a collection of documents, generate topics, and return the docs with topics</span>

<span class="sd">    Arguments:</span>
<span class="sd">        documents: A list of documents to fit on</span>
<span class="sd">        embeddings: Pre-trained document embeddings. These can be used</span>
<span class="sd">                    instead of the sentence-transformer model</span>

<span class="sd">    Returns:</span>
<span class="sd">        predictions: Topic predictions for each documents</span>
<span class="sd">        probabilities: The topic probability distribution which is returned by default.</span>
<span class="sd">                       If `low_memory` in BERTopic is set to False, then the</span>
<span class="sd">                       probabilities are not calculated to speed up computation and</span>
<span class="sd">                       decrease memory usage.</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>

<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>

<span class="sd">    model = BERTopic(verbose=True)</span>
<span class="sd">    topics = model.fit_transform(docs)</span>
<span class="sd">    ```</span>

<span class="sd">    If you want to use your own embeddings, use it as follows:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>
<span class="sd">    from sentence_transformers import SentenceTransformer</span>

<span class="sd">    # Create embeddings</span>
<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>
<span class="sd">    sentence_model = SentenceTransformer("distilbert-base-nli-mean-tokens")</span>
<span class="sd">    embeddings = sentence_model.encode(docs, show_progress_bar=True)</span>

<span class="sd">    # Create topic model</span>
<span class="sd">    model = BERTopic(verbose=True)</span>
<span class="sd">    topics = model.fit_transform(docs, embeddings)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_documents_type</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
    <span class="n">check_embeddings_shape</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">documents</span><span class="p">)</span>

    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"Document"</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span>
                              <span class="s2">"ID"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">)),</span>
                              <span class="s2">"Topic"</span><span class="p">:</span> <span class="kc">None</span><span class="p">})</span>

    <span class="c1"># Extract embeddings</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">),</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">csr_matrix</span><span class="p">)]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_select_embedding_model</span><span class="p">()</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_embeddings</span><span class="p">(</span><span class="n">documents</span><span class="o">.</span><span class="n">Document</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Transformed documents to Embeddings"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">custom_embeddings</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Reduce dimensionality with UMAP</span>
    <span class="n">umap_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_dimensionality</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

    <span class="c1"># Cluster UMAP embeddings with HDBSCAN</span>
    <span class="n">documents</span><span class="p">,</span> <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cluster_embeddings</span><span class="p">(</span><span class="n">umap_embeddings</span><span class="p">,</span> <span class="n">documents</span><span class="p">)</span>

    <span class="c1"># Extract topics by calculating c-TF-IDF</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_extract_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nr_topics</span><span class="p">:</span>
        <span class="n">documents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">probabilities</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.get_params">
<code class="highlight language-python">
get_params<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deep</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.get_params" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Get parameters for this estimator.</p>
<p>Adapted from:
    https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>deep</code></td>
<td><code>bool</code></td>
<td>
<p>bool, default=True
  If True, will return the parameters for this estimator and
  contained subobjects that are estimators.</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Mapping[str, Any]</code></td>
<td>
<p>out: Parameter names mapped to their values.</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deep</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sd">""" Get parameters for this estimator.</span>

<span class="sd">    Adapted from:</span>
<span class="sd">        https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178</span>

<span class="sd">    Arguments:</span>
<span class="sd">        deep: bool, default=True</span>
<span class="sd">              If True, will return the parameters for this estimator and</span>
<span class="sd">              contained subobjects that are estimators.</span>

<span class="sd">    Returns:</span>
<span class="sd">        out: Parameter names mapped to their values.</span>
<span class="sd">    """</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_param_names</span><span class="p">():</span>
        <span class="n">value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">deep</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">'get_params'</span><span class="p">):</span>
            <span class="n">deep_items</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="n">out</span><span class="o">.</span><span class="n">update</span><span class="p">((</span><span class="n">key</span> <span class="o">+</span> <span class="s1">'__'</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">deep_items</span><span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.get_topic">
<code class="highlight language-python">
get_topic<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.get_topic" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Return top n words for a specific topic and their c-TF-IDF scores</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>topic</code></td>
<td><code>int</code></td>
<td>
<p>A specific topic for which you want its representation</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Union[Mapping[str, Tuple[str, float]], bool]</code></td>
<td>
<p>The top n words for a specific word and its respective c-TF-IDF scores</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">topic</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">get_topic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">]:</span>
    <span class="sd">""" Return top n words for a specific topic and their c-TF-IDF scores</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topic: A specific topic for which you want its representation</span>

<span class="sd">    Returns:</span>
<span class="sd">        The top n words for a specific word and its respective c-TF-IDF scores</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic = model.get_topic(12)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">topic</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.get_topic_freq">
<code class="highlight language-python">
get_topic_freq<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.get_topic_freq" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Return the the size of topics (descending order)</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>topic</code></td>
<td><code>int</code></td>
<td>
<p>A specific topic for which you want the frequency</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Union[pandas.core.frame.DataFrame, int]</code></td>
<td>
<p>Either the frequency of a single topic or dataframe with
the frequencies of all topics</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<p>To extract the frequency of all topics:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">frequency</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_freq</span><span class="p">()</span>
</code></pre>
</div>
<p>To get the frequency of a single topic:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">frequency</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_freq</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">get_topic_freq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="sd">""" Return the the size of topics (descending order)</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topic: A specific topic for which you want the frequency</span>

<span class="sd">    Returns:</span>
<span class="sd">        Either the frequency of a single topic or dataframe with</span>
<span class="sd">        the frequencies of all topics</span>

<span class="sd">    Usage:</span>

<span class="sd">    To extract the frequency of all topics:</span>

<span class="sd">    ```python</span>
<span class="sd">    frequency = model.get_topic_freq()</span>
<span class="sd">    ```</span>

<span class="sd">    To get the frequency of a single topic:</span>

<span class="sd">    ```python</span>
<span class="sd">    frequency = model.get_topic_freq(12)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'Topic'</span><span class="p">,</span> <span class="s1">'Count'</span><span class="p">])</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">"Count"</span><span class="p">,</span>
                                                                                              <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.get_topic_info">
<code class="highlight language-python">
get_topic_info<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.get_topic_info" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Get information about each topic including its id, frequency, and name</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>topic</code></td>
<td><code>int</code></td>
<td>
<p>A specific topic for which you want the frequency</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DataFrame</code></td>
<td>
<p>info: The information relating to either a single topic or all topics</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">info_df</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_info</span><span class="p">()</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">get_topic_info</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sd">""" Get information about each topic including its id, frequency, and name</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topic: A specific topic for which you want the frequency</span>

<span class="sd">    Returns:</span>
<span class="sd">        info: The information relating to either a single topic or all topics</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    info_df = model.get_topic_info()</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="n">info</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'Topic'</span><span class="p">,</span> <span class="s1">'Count'</span><span class="p">])</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">"Count"</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">info</span><span class="p">[</span><span class="s2">"Name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_names</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">topic</span><span class="p">:</span>
        <span class="n">info</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">info</span><span class="o">.</span><span class="n">Topic</span> <span class="o">==</span> <span class="n">topic</span><span class="p">,</span> <span class="p">:]</span>

    <span class="k">return</span> <span class="n">info</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.get_topics">
<code class="highlight language-python">
get_topics<span class="p">(</span><span class="bp">self</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.get_topics" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Return topics with top n words and their c-TF-IDF score</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Mapping[str, Tuple[str, float]]</code></td>
<td>
<p>self.topic: The top n words per topic and the corresponding c-TF-IDF score</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">all_topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topics</span><span class="p">()</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">get_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>
    <span class="sd">""" Return topics with top n words and their c-TF-IDF score</span>

<span class="sd">    Returns:</span>
<span class="sd">        self.topic: The top n words per topic and the corresponding c-TF-IDF score</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    all_topics = model.get_topics()</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">topics</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.load">
<code class="highlight language-python">
load<span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">embedding_model</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
<span class="doc doc-properties">
<small class="doc doc-property doc-property-classmethod"><code>classmethod</code></small>
</span>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.load" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Loads the model from the specified path</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>path</code></td>
<td><code>str</code></td>
<td>
<p>the location and name of the BERTopic file you want to load</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>embedding_model</code></td>
<td><code>Union[str, sentence_transformers.SentenceTransformer.SentenceTransformer, flair.embeddings.document.DocumentEmbeddings, flair.embeddings.token.TokenEmbeddings]</code></td>
<td>
<p>If the embedding_model was not saved to save space or to load
             it in from the cloud, you can load it in by specifying it here.</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">BERTopic</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"my_model"</span><span class="p">)</span>
</code></pre>
</div>
<p>or if you did not save the embedding model:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">BERTopic</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"my_model"</span><span class="p">,</span> <span class="n">embedding_model</span><span class="o">=</span><span class="s2">"xlm-r-bert-base-nli-stsb-mean-tokens"</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span>
         <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
         <span class="n">embedding_model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">DocumentEmbeddings</span><span class="p">,</span> <span class="n">TokenEmbeddings</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">""" Loads the model from the specified path</span>

<span class="sd">    Arguments:</span>
<span class="sd">        path: the location and name of the BERTopic file you want to load</span>
<span class="sd">        embedding_model: If the embedding_model was not saved to save space or to load</span>
<span class="sd">                         it in from the cloud, you can load it in by specifying it here.</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    BERTopic.load("my_model")</span>
<span class="sd">    ```</span>

<span class="sd">    or if you did not save the embedding model:</span>

<span class="sd">    ```python</span>
<span class="sd">    BERTopic.load("my_model", embedding_model="xlm-r-bert-base-nli-stsb-mean-tokens")</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">embedding_model</span><span class="p">:</span>
            <span class="n">topic_model</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
            <span class="n">topic_model</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">embedding_model</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">topic_model</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">topic_model</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.reduce_topics">
<code class="highlight language-python">
reduce_topics<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.reduce_topics" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Further reduce the number of topics to nr_topics.</p>
<p>The number of topics is further reduced by calculating the c-TF-IDF matrix
of the documents and then reducing them by iteratively merging the least
frequent topic with the most similar one based on their c-TF-IDF matrices.
The topics, their sizes, and representations are updated.</p>
<p>The reasoning for putting <code>docs</code>, <code>topics</code>, and <code>probs</code> as parameters is that
these values are not saved within BERTopic on purpose. If you were to have a
million documents, it seems very inefficient to save those in BERTopic
instead of a dedicated database.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>docs</code></td>
<td><code>List[str]</code></td>
<td>
<p>The docs you used when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>topics</code></td>
<td><code>List[int]</code></td>
<td>
<p>The topics that were returned when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>probabilities</code></td>
<td><code>ndarray</code></td>
<td>
<p>The probabilities that were returned when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>nr_topics</code></td>
<td><code>int</code></td>
<td>
<p>The number of topics you want reduced to</p>
</td>
<td><code>20</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tuple[List[int], numpy.ndarray]</code></td>
<td>
<p>new_topics: Updated topics
new_probabilities: Updated probabilities</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<p>You can further reduce the topics by passing the documents with its
topics and probabilities (if they were calculated):</p>
<div class="codehilite">
<pre><span></span><code><span class="n">new_topics</span><span class="p">,</span> <span class="n">new_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">reduce_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre>
</div>
<p>If probabilities were not calculated simply run the function without them:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">new_topics</span><span class="p">,</span> <span class="n">_</span><span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">reduce_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">reduce_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">docs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                  <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">probabilities</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                  <span class="n">nr_topics</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="sd">""" Further reduce the number of topics to nr_topics.</span>

<span class="sd">    The number of topics is further reduced by calculating the c-TF-IDF matrix</span>
<span class="sd">    of the documents and then reducing them by iteratively merging the least</span>
<span class="sd">    frequent topic with the most similar one based on their c-TF-IDF matrices.</span>
<span class="sd">    The topics, their sizes, and representations are updated.</span>

<span class="sd">    The reasoning for putting `docs`, `topics`, and `probs` as parameters is that</span>
<span class="sd">    these values are not saved within BERTopic on purpose. If you were to have a</span>
<span class="sd">    million documents, it seems very inefficient to save those in BERTopic</span>
<span class="sd">    instead of a dedicated database.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        docs: The docs you used when calling either `fit` or `fit_transform`</span>
<span class="sd">        topics: The topics that were returned when calling either `fit` or `fit_transform`</span>
<span class="sd">        probabilities: The probabilities that were returned when calling either `fit` or `fit_transform`</span>
<span class="sd">        nr_topics: The number of topics you want reduced to</span>

<span class="sd">    Returns:</span>
<span class="sd">        new_topics: Updated topics</span>
<span class="sd">        new_probabilities: Updated probabilities</span>

<span class="sd">    Usage:</span>

<span class="sd">    You can further reduce the topics by passing the documents with its</span>
<span class="sd">    topics and probabilities (if they were calculated):</span>

<span class="sd">    ```python</span>
<span class="sd">    new_topics, new_probs = model.reduce_topics(docs, topics, probabilities, nr_topics=30)</span>
<span class="sd">    ```</span>

<span class="sd">    If probabilities were not calculated simply run the function without them:</span>

<span class="sd">    ```python</span>
<span class="sd">    new_topics, _= model.reduce_topics(docs, topics, nr_topics=30)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nr_topics</span> <span class="o">=</span> <span class="n">nr_topics</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"Document"</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">"Topic"</span><span class="p">:</span> <span class="n">topics</span><span class="p">})</span>

    <span class="c1"># Reduce number of topics</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_extract_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
    <span class="n">new_topics</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
    <span class="n">new_probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">new_topics</span><span class="p">,</span> <span class="n">new_probabilities</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.save">
<code class="highlight language-python">
save<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">save_embedding_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.save" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Saves the model to the specified path</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>path</code></td>
<td><code>str</code></td>
<td>
<p>the location and name of the file you want to save</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>save_embedding_model</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to save the embedding model in this class
                  as you might have selected a local model or one that
                  is downloaded automatically from the cloud.</p>
</td>
<td><code>True</code></td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"my_model"</span><span class="p">)</span>
</code></pre>
</div>
<p>or if you do not want the embedding_model to be saved locally:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"my_model"</span><span class="p">,</span> <span class="n">save_embedding_model</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
         <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
         <span class="n">save_embedding_model</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">""" Saves the model to the specified path</span>

<span class="sd">    Arguments:</span>
<span class="sd">        path: the location and name of the file you want to save</span>
<span class="sd">        save_embedding_model: Whether to save the embedding model in this class</span>
<span class="sd">                              as you might have selected a local model or one that</span>
<span class="sd">                              is downloaded automatically from the cloud.</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    model.save("my_model")</span>
<span class="sd">    ```</span>

<span class="sd">    or if you do not want the embedding_model to be saved locally:</span>

<span class="sd">    ```python</span>
<span class="sd">    model.save("my_model", save_embedding_model=False)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">save_embedding_model</span><span class="p">:</span>
            <span class="n">embedding_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">embedding_model</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.topics_over_time">
<code class="highlight language-python">
topics_over_time<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">timestamps</span><span class="p">,</span> <span class="n">nr_bins</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">datetime_format</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">evolution_tuning</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">global_tuning</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.topics_over_time" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Create topics over time</p>
<p>To create the topics over time, BERTopic needs to be already fitted once.
From the fitted models, the c-TF-IDF representations are calculate at
each timestamp t. Then, the c-TF-IDF representations at timestamp t are
averaged with the global c-TF-IDF representations in order to fine-tune the
local representations.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure to use a limited number of unique timestamps (&lt;100) as the
c-TF-IDF representation will be calculated at each single unique timestamp.
Having a large number of unique timestamps can take some time to be calculated.
Moreover, there aren't many use-cased where you would like to see the difference
in topic representations over more than 100 different timestamps.</p>
</div>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>docs</code></td>
<td><code>List[str]</code></td>
<td>
<p>The documents you used when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>topics</code></td>
<td><code>List[int]</code></td>
<td>
<p>The topics that were returned when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>timestamps</code></td>
<td><code>Union[List[str], List[int]]</code></td>
<td>
<p>The timestamp of each document. This can be either a list of strings or ints.
        If it is a list of strings, then the datetime format will be automatically
        inferred. If it is a list of ints, then the documents will be ordered by
        ascending order.</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>nr_bins</code></td>
<td><code>int</code></td>
<td>
<p>The number of bins you want to create for the timestamps. The left interval will
     be chosen as the timestamp. An additional column will be created with the
     entire interval.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>datetime_format</code></td>
<td><code>str</code></td>
<td>
<p>The datetime format of the timestamps if they are strings, eg “%d/%m/%Y”.
             Set this to None if you want to have it automatically detect the format.
             See strftime documentation for more information on choices:
             https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>evolution_tuning</code></td>
<td><code>bool</code></td>
<td>
<p>Fine-tune each topic representation at timestamp t by averaging its
              c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1. This creates
              evolutionary topic representations.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>global_tuning</code></td>
<td><code>bool</code></td>
<td>
<p>Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix
       with the global c-TF-IDF matrix. Turn this off if you want to prevent words in
       topic representations that could not be found in the documents at timestamp t.</p>
</td>
<td><code>True</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DataFrame</code></td>
<td>
<p>topics_over_time: A dataframe that contains the topic, words, and frequency of topic
                  at timestamp t.</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<p>The timestamps variable represent the timestamp of each document. If you have over
100 unique timestamps, it is advised to bin the timestamps as shown below:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">topics_over_time</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">topics_over_time</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">timestamps</span><span class="p">,</span> <span class="n">nr_bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">topics_over_time</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">docs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                     <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">timestamps</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                                       <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                     <span class="n">nr_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                     <span class="n">datetime_format</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                     <span class="n">evolution_tuning</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="n">global_tuning</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sd">""" Create topics over time</span>

<span class="sd">    To create the topics over time, BERTopic needs to be already fitted once.</span>
<span class="sd">    From the fitted models, the c-TF-IDF representations are calculate at</span>
<span class="sd">    each timestamp t. Then, the c-TF-IDF representations at timestamp t are</span>
<span class="sd">    averaged with the global c-TF-IDF representations in order to fine-tune the</span>
<span class="sd">    local representations.</span>

<span class="sd">    NOTE:</span>
<span class="sd">        Make sure to use a limited number of unique timestamps (&lt;100) as the</span>
<span class="sd">        c-TF-IDF representation will be calculated at each single unique timestamp.</span>
<span class="sd">        Having a large number of unique timestamps can take some time to be calculated.</span>
<span class="sd">        Moreover, there aren't many use-cased where you would like to see the difference</span>
<span class="sd">        in topic representations over more than 100 different timestamps.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        docs: The documents you used when calling either `fit` or `fit_transform`</span>
<span class="sd">        topics: The topics that were returned when calling either `fit` or `fit_transform`</span>
<span class="sd">        timestamps: The timestamp of each document. This can be either a list of strings or ints.</span>
<span class="sd">                    If it is a list of strings, then the datetime format will be automatically</span>
<span class="sd">                    inferred. If it is a list of ints, then the documents will be ordered by</span>
<span class="sd">                    ascending order.</span>
<span class="sd">        nr_bins: The number of bins you want to create for the timestamps. The left interval will</span>
<span class="sd">                 be chosen as the timestamp. An additional column will be created with the</span>
<span class="sd">                 entire interval.</span>
<span class="sd">        datetime_format: The datetime format of the timestamps if they are strings, eg “%d/%m/%Y”.</span>
<span class="sd">                         Set this to None if you want to have it automatically detect the format.</span>
<span class="sd">                         See strftime documentation for more information on choices:</span>
<span class="sd">                         https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.</span>
<span class="sd">        evolution_tuning: Fine-tune each topic representation at timestamp t by averaging its</span>
<span class="sd">                          c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1. This creates</span>
<span class="sd">                          evolutionary topic representations.</span>
<span class="sd">        global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix</span>
<span class="sd">                   with the global c-TF-IDF matrix. Turn this off if you want to prevent words in</span>
<span class="sd">                   topic representations that could not be found in the documents at timestamp t.</span>

<span class="sd">    Returns:</span>
<span class="sd">        topics_over_time: A dataframe that contains the topic, words, and frequency of topic</span>
<span class="sd">                          at timestamp t.</span>

<span class="sd">    Usage:</span>

<span class="sd">    The timestamps variable represent the timestamp of each document. If you have over</span>
<span class="sd">    100 unique timestamps, it is advised to bin the timestamps as shown below:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    model = BERTopic(verbose=True).fit(docs)</span>
<span class="sd">    topics = model.transform(docs)</span>
<span class="sd">    topics_over_time = model.topics_over_time(docs, topics, timestamps, nr_bins=20)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">check_documents_type</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"Document"</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">"Topic"</span><span class="p">:</span> <span class="n">topics</span><span class="p">,</span> <span class="s2">"Timestamps"</span><span class="p">:</span> <span class="n">timestamps</span><span class="p">})</span>
    <span class="n">global_c_tf_idf</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">'l1'</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timestamps</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">infer_datetime_format</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">datetime_format</span> <span class="k">else</span> <span class="kc">False</span>
        <span class="n">documents</span><span class="p">[</span><span class="s2">"Timestamps"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">documents</span><span class="p">[</span><span class="s2">"Timestamps"</span><span class="p">],</span>
                                                 <span class="n">infer_datetime_format</span><span class="o">=</span><span class="n">infer_datetime_format</span><span class="p">,</span>
                                                 <span class="nb">format</span><span class="o">=</span><span class="n">datetime_format</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">nr_bins</span><span class="p">:</span>
        <span class="n">documents</span><span class="p">[</span><span class="s2">"Bins"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">documents</span><span class="o">.</span><span class="n">Timestamps</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">nr_bins</span><span class="p">)</span>
        <span class="n">documents</span><span class="p">[</span><span class="s2">"Timestamps"</span><span class="p">]</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="o">.</span><span class="n">Bins</span><span class="o">.</span><span class="n">left</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Sort documents in chronological order</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">"Timestamps"</span><span class="p">)</span>
    <span class="n">timestamps</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">Timestamps</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">timestamps</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">"There are more than 100 unique timestamps (i.e., {len(timestamps)}) "</span>
                      <span class="s2">"which significantly slows down the application. Consider setting `nr_bins` "</span>
                      <span class="s2">"to a value lower than 100 to speed up calculation. "</span><span class="p">)</span>

    <span class="c1"># For each unique timestamp, create topic representations</span>
    <span class="n">topics_over_time</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">timestamp</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">timestamps</span><span class="p">),</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">):</span>

        <span class="c1"># Calculate c-TF-IDF representation for a specific timestamp</span>
        <span class="n">selection</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">documents</span><span class="o">.</span><span class="n">Timestamps</span> <span class="o">==</span> <span class="n">timestamp</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">documents_per_topic</span> <span class="o">=</span> <span class="n">selection</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">'Topic'</span><span class="p">],</span> <span class="n">as_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s1">'Document'</span><span class="p">:</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">,</span>
                                                                                <span class="s2">"Timestamps"</span><span class="p">:</span> <span class="s2">"count"</span><span class="p">})</span>
        <span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">words</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_c_tf_idf</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">selection</span><span class="p">),</span> <span class="n">fit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">global_tuning</span> <span class="ow">or</span> <span class="n">evolution_tuning</span><span class="p">:</span>
            <span class="n">c_tf_idf</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">'l1'</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the c-TF-IDF</span>
        <span class="c1"># matrix at timestamp t-1</span>
        <span class="k">if</span> <span class="n">evolution_tuning</span> <span class="ow">and</span> <span class="n">index</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">current_topics</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
            <span class="n">overlapping_topics</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">previous_topics</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">current_topics</span><span class="p">))))</span>

            <span class="n">current_overlap_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">current_topics</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">topic</span><span class="p">)</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">overlapping_topics</span><span class="p">]</span>
            <span class="n">previous_overlap_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">previous_topics</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">topic</span><span class="p">)</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">overlapping_topics</span><span class="p">]</span>

            <span class="n">c_tf_idf</span><span class="o">.</span><span class="n">tolil</span><span class="p">()[</span><span class="n">current_overlap_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">c_tf_idf</span><span class="p">[</span><span class="n">current_overlap_idx</span><span class="p">]</span> <span class="o">+</span>
                                                      <span class="n">previous_c_tf_idf</span><span class="p">[</span><span class="n">previous_overlap_idx</span><span class="p">])</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">tolil</span><span class="p">()</span>

        <span class="c1"># Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation</span>
        <span class="c1"># by simply taking the average of the two</span>
        <span class="k">if</span> <span class="n">global_tuning</span><span class="p">:</span>
            <span class="n">c_tf_idf</span> <span class="o">=</span> <span class="p">(</span><span class="n">global_c_tf_idf</span><span class="p">[</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">values</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">c_tf_idf</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>

        <span class="c1"># Extract the words per topic</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">unique</span><span class="p">()))</span>
        <span class="n">words_per_topic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_words_per_topic</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">c_tf_idf</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">topic_frequency</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Timestamps</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                    <span class="n">index</span><span class="o">=</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

        <span class="c1"># Fill dataframe with results</span>
        <span class="n">topics_at_timestamp</span> <span class="o">=</span> <span class="p">[(</span><span class="n">topic</span><span class="p">,</span>
                                <span class="s2">", "</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">values</span><span class="p">][:</span><span class="mi">5</span><span class="p">]),</span>
                                <span class="n">topic_frequency</span><span class="p">[</span><span class="n">topic</span><span class="p">],</span>
                                <span class="n">timestamp</span><span class="p">)</span> <span class="k">for</span> <span class="n">topic</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span> <span class="n">words_per_topic</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
        <span class="n">topics_over_time</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">topics_at_timestamp</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">evolution_tuning</span><span class="p">:</span>
            <span class="n">previous_topics</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">documents_per_topic</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
            <span class="n">previous_c_tf_idf</span> <span class="o">=</span> <span class="n">c_tf_idf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">topics_over_time</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">"Topic"</span><span class="p">,</span> <span class="s2">"Words"</span><span class="p">,</span> <span class="s2">"Frequency"</span><span class="p">,</span> <span class="s2">"Timestamp"</span><span class="p">])</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.transform">
<code class="highlight language-python">
transform<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.transform" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>After having fit a model, use transform to predict new instances</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>documents</code></td>
<td><code>Union[str, List[str]]</code></td>
<td>
<p>A single document or a list of documents to fit on</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>embeddings</code></td>
<td><code>ndarray</code></td>
<td>
<p>Pre-trained document embeddings. These can be used
        instead of the sentence-transformer model.</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tuple[List[int], numpy.ndarray]</code></td>
<td>
<p>predictions: Topic predictions for each documents
probabilities: The topic probability distribution which is returned by default.
               If <code>low_memory</code> in BERTopic is set to False, then the
               probabilities are not calculated to speed up computation and
               decrease memory usage.</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre>
</div>
<p>If you want to use your own embeddings:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Create embeddings</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">sentence_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create topic model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">documents</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
              <span class="n">embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="sd">""" After having fit a model, use transform to predict new instances</span>

<span class="sd">    Arguments:</span>
<span class="sd">        documents: A single document or a list of documents to fit on</span>
<span class="sd">        embeddings: Pre-trained document embeddings. These can be used</span>
<span class="sd">                    instead of the sentence-transformer model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        predictions: Topic predictions for each documents</span>
<span class="sd">        probabilities: The topic probability distribution which is returned by default.</span>
<span class="sd">                       If `low_memory` in BERTopic is set to False, then the</span>
<span class="sd">                       probabilities are not calculated to speed up computation and</span>
<span class="sd">                       decrease memory usage.</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>

<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>
<span class="sd">    model = BERTopic(verbose=True).fit(docs)</span>
<span class="sd">    topics = model.transform(docs)</span>
<span class="sd">    ```</span>

<span class="sd">    If you want to use your own embeddings:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>
<span class="sd">    from sentence_transformers import SentenceTransformer</span>

<span class="sd">    # Create embeddings</span>
<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>
<span class="sd">    sentence_model = SentenceTransformer("distilbert-base-nli-mean-tokens")</span>
<span class="sd">    embeddings = sentence_model.encode(docs, show_progress_bar=True)</span>

<span class="sd">    # Create topic model</span>
<span class="sd">    model = BERTopic(verbose=True).fit(docs, embeddings)</span>
<span class="sd">    topics = model.transform(docs, embeddings)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">check_embeddings_shape</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">documents</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">documents</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_select_embedding_model</span><span class="p">()</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_embeddings</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>

    <span class="n">umap_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">umap_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hdbscan</span><span class="o">.</span><span class="n">approximate_predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hdbscan_model</span><span class="p">,</span> <span class="n">umap_embeddings</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_probabilities</span><span class="p">:</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">hdbscan</span><span class="o">.</span><span class="n">membership_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hdbscan_model</span><span class="p">,</span> <span class="n">umap_embeddings</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapped_topics</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_predictions</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">probabilities</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.update_topics">
<code class="highlight language-python">
update_topics<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vectorizer_model</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.update_topics" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Updates the topic representation by recalculating c-TF-IDF with the new
parameters as defined in this function.</p>
<p>When you have trained a model and viewed the topics and the words that represent them,
you might not be satisfied with the representation. Perhaps you forgot to remove
stop_words or you want to try out a different n_gram_range. This function allows you
to update the topic representation after they have been formed.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>docs</code></td>
<td><code>List[str]</code></td>
<td>
<p>The documents you used when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>topics</code></td>
<td><code>List[int]</code></td>
<td>
<p>The topics that were returned when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>n_gram_range</code></td>
<td><code>Tuple[int, int]</code></td>
<td>
<p>The n-gram range for the CountVectorizer.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>vectorizer_model</code></td>
<td><code>CountVectorizer</code></td>
<td>
<p>Pass in your own CountVectorizer from scikit-learn</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<p>In order to update the topic representation, you will need to first fit the topic
model and extract topics from them. Based on these, you can update the representation:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">update_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre>
</div>
<p>YOu can also use a custom vectorizer to update the representation:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="n">vectorizer_model</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="s2">"english"</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">update_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">vectorizer_model</span><span class="o">=</span><span class="n">vectorizer_model</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">update_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">docs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                  <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">n_gram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                  <span class="n">vectorizer_model</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">""" Updates the topic representation by recalculating c-TF-IDF with the new</span>
<span class="sd">    parameters as defined in this function.</span>

<span class="sd">    When you have trained a model and viewed the topics and the words that represent them,</span>
<span class="sd">    you might not be satisfied with the representation. Perhaps you forgot to remove</span>
<span class="sd">    stop_words or you want to try out a different n_gram_range. This function allows you</span>
<span class="sd">    to update the topic representation after they have been formed.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        docs: The documents you used when calling either `fit` or `fit_transform`</span>
<span class="sd">        topics: The topics that were returned when calling either `fit` or `fit_transform`</span>
<span class="sd">        n_gram_range: The n-gram range for the CountVectorizer.</span>
<span class="sd">        vectorizer_model: Pass in your own CountVectorizer from scikit-learn</span>

<span class="sd">    Usage:</span>

<span class="sd">    In order to update the topic representation, you will need to first fit the topic</span>
<span class="sd">    model and extract topics from them. Based on these, you can update the representation:</span>

<span class="sd">    ```python</span>
<span class="sd">    model.update_topics(docs, topics, n_gram_range=(2, 3))</span>
<span class="sd">    ```</span>

<span class="sd">    YOu can also use a custom vectorizer to update the representation:</span>

<span class="sd">    ```python</span>
<span class="sd">    from sklearn.feature_extraction.text import CountVectorizer</span>
<span class="sd">    vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")</span>
<span class="sd">    model.update_topics(docs, topics, vectorizer_model=vectorizer_model)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">n_gram_range</span><span class="p">:</span>
        <span class="n">n_gram_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_gram_range</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer_model</span> <span class="o">=</span> <span class="n">vectorizer_model</span> <span class="ow">or</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="n">n_gram_range</span><span class="p">)</span>

    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"Document"</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">"Topic"</span><span class="p">:</span> <span class="n">topics</span><span class="p">})</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_extract_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.visualize_distribution">
<code class="highlight language-python">
visualize_distribution<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">,</span> <span class="n">min_probability</span><span class="o">=</span><span class="mf">0.015</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">save</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.visualize_distribution" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Visualize the distribution of topic probabilities</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>probabilities</code></td>
<td><code>ndarray</code></td>
<td>
<p>An array of probability scores</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>min_probability</code></td>
<td><code>float</code></td>
<td>
<p>The minimum probability score to visualize.
             All others are ignored.</p>
</td>
<td><code>0.015</code></td>
</tr>
<tr>
<td><code>figsize</code></td>
<td><code>tuple</code></td>
<td>
<p>The size of the figure</p>
</td>
<td><code>(10, 5)</code></td>
</tr>
<tr>
<td><code>save</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to save the resulting graph to probility.png</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<p>Make sure to fit the model before and only input the
probabilities of a single document:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">visualize_distribution</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre>
</div>
<p><img alt="" src="../img/probabilities.png"/></p>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">visualize_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">probabilities</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                           <span class="n">min_probability</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.015</span><span class="p">,</span>
                           <span class="n">figsize</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                           <span class="n">save</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="sd">""" Visualize the distribution of topic probabilities</span>

<span class="sd">    Arguments:</span>
<span class="sd">        probabilities: An array of probability scores</span>
<span class="sd">        min_probability: The minimum probability score to visualize.</span>
<span class="sd">                         All others are ignored.</span>
<span class="sd">        figsize: The size of the figure</span>
<span class="sd">        save: Whether to save the resulting graph to probility.png</span>

<span class="sd">    Usage:</span>

<span class="sd">    Make sure to fit the model before and only input the</span>
<span class="sd">    probabilities of a single document:</span>

<span class="sd">    ```python</span>
<span class="sd">    model.visualize_distribution(probabilities[0])</span>
<span class="sd">    ```</span>

<span class="sd">    ![](../img/probabilities.png)</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_HAS_VIZ</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">ModuleNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"In order to use this function you'll need to install "</span>
                                  <span class="sa">f</span><span class="s2">"additional dependencies;</span><span class="se">\n</span><span class="s2">pip install bertopic[visualization]"</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="n">probabilities</span> <span class="o">&gt;</span> <span class="n">min_probability</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"There are no values where `min_probability` is higher than the "</span>
                         <span class="s2">"probabilities that were supplied. Lower `min_probability` to prevent this error."</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_probabilities</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"This visualization cannot be used if you have set `calculate_probabilities` to False "</span>
                         <span class="s2">"as it uses the topic probabilities. "</span><span class="p">)</span>

    <span class="c1"># Get values and indices equal or exceed the minimum probability</span>
    <span class="n">labels_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">probabilities</span> <span class="o">&gt;=</span> <span class="n">min_probability</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">vals</span> <span class="o">=</span> <span class="n">probabilities</span><span class="p">[</span><span class="n">labels_idx</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Create labels</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">labels_idx</span><span class="p">:</span>
        <span class="n">label</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">words</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">words</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
                <span class="n">label</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">label</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="sa">r</span><span class="s2">"$\bf{Topic }$ "</span> <span class="o">+</span>
                        <span class="sa">r</span><span class="s2">"$\bf{"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">+</span> <span class="s2">":}$ "</span> <span class="o">+</span>
                        <span class="s2">" "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>
            <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vals</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>

    <span class="c1"># Create figure</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">pos</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'#333F4B'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">vals</span><span class="p">),</span> <span class="n">xmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">vals</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">'#333F4B'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

    <span class="c1"># Set ticks and labels</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">'both'</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Probability'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'#333F4B'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'Topic Probability Distribution'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'#333F4B'</span><span class="p">)</span>

    <span class="c1"># Update spine style</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'right'</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'top'</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'left'</span><span class="p">]</span><span class="o">.</span><span class="n">set_bounds</span><span class="p">(</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'bottom'</span><span class="p">]</span><span class="o">.</span><span class="n">set_bounds</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'bottom'</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">((</span><span class="s1">'axes'</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'left'</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">((</span><span class="s1">'axes'</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">))</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">save</span><span class="p">:</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">"probability.png"</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">'tight'</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.visualize_topics">
<code class="highlight language-python">
visualize_topics<span class="p">(</span><span class="bp">self</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.visualize_topics" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Visualize topics, their sizes, and their corresponding words</p>
<p>This visualization is highly inspired by LDAvis, a great visualization
technique typically reserved for LDA.</p>
<p>Usage:</p>
<p>To visualize the topics simply run:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">visualize_topics</span><span class="p">()</span>
</code></pre>
</div>
<p>Or if you want to save the resulting figure:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">visualize_topics</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s2">"path/to/file.html"</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">visualize_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">""" Visualize topics, their sizes, and their corresponding words</span>

<span class="sd">    This visualization is highly inspired by LDAvis, a great visualization</span>
<span class="sd">    technique typically reserved for LDA.</span>

<span class="sd">    Usage:</span>

<span class="sd">    To visualize the topics simply run:</span>

<span class="sd">    ```python</span>
<span class="sd">    model.visualize_topics()</span>
<span class="sd">    ```</span>

<span class="sd">    Or if you want to save the resulting figure:</span>

<span class="sd">    ```python</span>
<span class="sd">    fig = model.visualize_topics()</span>
<span class="sd">    fig.write_html("path/to/file.html")</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_HAS_VIZ</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">ModuleNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"In order to use this function you'll need to install "</span>
                                  <span class="sa">f</span><span class="s2">"additional dependencies;</span><span class="se">\n</span><span class="s2">pip install bertopic[visualization]"</span><span class="p">)</span>

    <span class="c1"># Extract topic words and their frequencies</span>
    <span class="n">topic_list</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="n">frequencies</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topic_list</span><span class="p">]</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">" | "</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="n">topic</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]])</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topic_list</span><span class="p">]</span>

    <span class="c1"># Embed c-TF-IDF into 2D</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_tf_idf</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">'hellinger'</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

    <span class="c1"># Visualize with plotly</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"x"</span><span class="p">:</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">"y"</span><span class="p">:</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">1</span><span class="p">],</span>
                       <span class="s2">"Topic"</span><span class="p">:</span> <span class="n">topic_list</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s2">"Words"</span><span class="p">:</span> <span class="n">words</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s2">"Size"</span><span class="p">:</span> <span class="n">frequencies</span><span class="p">[</span><span class="mi">1</span><span class="p">:]})</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_plotly_topic_visualization</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">topic_list</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.visualize_topics_over_time">
<code class="highlight language-python">
visualize_topics_over_time<span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topics_over_time</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">topics</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> </code>
<a class="headerlink" href="#bertopic._bertopic.BERTopic.visualize_topics_over_time" title="Permanent link">¶</a></h2>
<div class="doc doc-contents">
<p>Visualize topics over time</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>topics_over_time</code></td>
<td><code>DataFrame</code></td>
<td>
<p>The topics you would like to be visualized with the
              corresponding topic representation</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>top_n</code></td>
<td><code>int</code></td>
<td>
<p>To visualize the most frequent topics instead of all</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>topics</code></td>
<td><code>List[int]</code></td>
<td>
<p>Select which topics you would like to be visualized</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code></code></td>
<td>
<p>A plotly.graph_objects.Figure including all traces</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<p>To visualize the topics over time, simply run:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">visualize_topics_over_time</span><span class="p">(</span><span class="n">topics_over_time</span><span class="p">)</span>
</code></pre>
</div>
<p>Or if you want to save the resulting figure:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">fig</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">visualize_topics_over_time</span><span class="p">(</span><span class="n">topics_over_time</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s2">"path/to/file.html"</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
<summary>Source code in <code>bertopic\_bertopic.py</code></summary>
<div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">visualize_topics_over_time</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">topics_over_time</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                               <span class="n">top_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                               <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">""" Visualize topics over time</span>

<span class="sd">    Arguments:</span>
<span class="sd">        topics_over_time: The topics you would like to be visualized with the</span>
<span class="sd">                          corresponding topic representation</span>
<span class="sd">        top_n: To visualize the most frequent topics instead of all</span>
<span class="sd">        topics: Select which topics you would like to be visualized</span>

<span class="sd">    Returns:</span>
<span class="sd">        A plotly.graph_objects.Figure including all traces</span>

<span class="sd">    Usage:</span>

<span class="sd">    To visualize the topics over time, simply run:</span>

<span class="sd">    ```python</span>
<span class="sd">    model.visualize_topics_over_time(topics_over_time)</span>
<span class="sd">    ```</span>

<span class="sd">    Or if you want to save the resulting figure:</span>

<span class="sd">    ```python</span>
<span class="sd">    fig = model.visualize_topics_over_time(topics_over_time)</span>
<span class="sd">    fig.write_html("path/to/file.html")</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_HAS_VIZ</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">ModuleNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"In order to use this function you'll need to install "</span>
                                  <span class="sa">f</span><span class="s2">"additional dependencies;</span><span class="se">\n</span><span class="s2">pip install bertopic[visualization]"</span><span class="p">)</span>

    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"#E69F00"</span><span class="p">,</span> <span class="s2">"#56B4E9"</span><span class="p">,</span> <span class="s2">"#009E73"</span><span class="p">,</span> <span class="s2">"#F0E442"</span><span class="p">,</span> <span class="s2">"#D55E00"</span><span class="p">,</span> <span class="s2">"#0072B2"</span><span class="p">,</span> <span class="s2">"#CC79A7"</span><span class="p">]</span>

    <span class="c1"># Select topics</span>
    <span class="k">if</span> <span class="n">topics</span><span class="p">:</span>
        <span class="n">selected_topics</span> <span class="o">=</span> <span class="n">topics</span>
    <span class="k">elif</span> <span class="n">top_n</span><span class="p">:</span>
        <span class="n">selected_topics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_topic_freq</span><span class="p">()</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">top_n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">values</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">selected_topics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_topic_freq</span><span class="p">()</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">values</span>

    <span class="c1"># Prepare data</span>
    <span class="n">topic_names</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[:</span><span class="mi">40</span><span class="p">]</span> <span class="o">+</span> <span class="s2">"..."</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">40</span> <span class="k">else</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_names</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">topics_over_time</span><span class="p">[</span><span class="s2">"Name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">topics_over_time</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">topic_names</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">topics_over_time</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">topics_over_time</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">selected_topics</span><span class="p">),</span> <span class="p">:]</span>

    <span class="c1"># Add traces</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">topic</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">unique</span><span class="p">()):</span>
        <span class="n">trace_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">Topic</span> <span class="o">==</span> <span class="n">topic</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">topic_name</span> <span class="o">=</span> <span class="n">trace_data</span><span class="o">.</span><span class="n">Name</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">trace_data</span><span class="o">.</span><span class="n">Words</span><span class="o">.</span><span class="n">values</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">trace_data</span><span class="o">.</span><span class="n">Timestamp</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">trace_data</span><span class="o">.</span><span class="n">Frequency</span><span class="p">,</span>
                                 <span class="n">mode</span><span class="o">=</span><span class="s1">'lines'</span><span class="p">,</span>
                                 <span class="n">marker_color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">index</span> <span class="o">%</span> <span class="mi">7</span><span class="p">],</span>
                                 <span class="n">hoverinfo</span><span class="o">=</span><span class="s2">"text"</span><span class="p">,</span>
                                 <span class="n">name</span><span class="o">=</span><span class="n">topic_name</span><span class="p">,</span>
                                 <span class="n">hovertext</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s1">'&lt;b&gt;Topic </span><span class="si">{topic}</span><span class="s1">&lt;/b&gt;&lt;br&gt;Words: </span><span class="si">{word}</span><span class="s1">'</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]))</span>

    <span class="c1"># Styling of the visualization</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">update_xaxes</span><span class="p">(</span><span class="n">showgrid</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">update_yaxes</span><span class="p">(</span><span class="n">showgrid</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span>
        <span class="n">yaxis_title</span><span class="o">=</span><span class="s2">"Frequency"</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="p">{</span>
            <span class="s1">'text'</span><span class="p">:</span> <span class="s2">"&lt;b&gt;Topics over Time"</span><span class="p">,</span>
            <span class="s1">'y'</span><span class="p">:</span> <span class="o">.</span><span class="mi">95</span><span class="p">,</span>
            <span class="s1">'x'</span><span class="p">:</span> <span class="mf">0.40</span><span class="p">,</span>
            <span class="s1">'xanchor'</span><span class="p">:</span> <span class="s1">'center'</span><span class="p">,</span>
            <span class="s1">'yanchor'</span><span class="p">:</span> <span class="s1">'top'</span><span class="p">,</span>
            <span class="s1">'font'</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">size</span><span class="o">=</span><span class="mi">22</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s2">"Black"</span><span class="p">)</span>
        <span class="p">},</span>
        <span class="n">template</span><span class="o">=</span><span class="s2">"simple_white"</span><span class="p">,</span>
        <span class="n">width</span><span class="o">=</span><span class="mi">1250</span><span class="p">,</span>
        <span class="n">height</span><span class="o">=</span><span class="mi">450</span><span class="p">,</span>
        <span class="n">hoverlabel</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">bgcolor</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span>
            <span class="n">font_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
            <span class="n">font_family</span><span class="o">=</span><span class="s2">"Rockwell"</span>
        <span class="p">),</span>
        <span class="n">legend</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">title</span><span class="o">=</span><span class="s2">"&lt;b&gt;Global Topic Representation"</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">fig</span>
</code></pre>
</div>
</details>
</div>
</div>
</div>
</div>
</div>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-nav">
<nav aria-label="Footer" class="md-footer-nav__inner md-grid">
<a class="md-footer-nav__link md-footer-nav__link--prev" href="../tutorial/models/models.html" rel="prev">
<div class="md-footer-nav__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"></path></svg>
</div>
<div class="md-footer-nav__title">
<div class="md-ellipsis">
<span class="md-footer-nav__direction">
                  Previous
                </span>
                Custom Models
              </div>
</div>
</a>
<a class="md-footer-nav__link md-footer-nav__link--next" href="ctfidf.html" rel="next">
<div class="md-footer-nav__title">
<div class="md-ellipsis">
<span class="md-footer-nav__direction">
                  Next
                </span>
                cTFIDF
              </div>
</div>
<div class="md-footer-nav__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"></path></svg>
</div>
</a>
</nav>
</div>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-footer-copyright">
<div class="md-footer-copyright__highlight">
            Copyright © 2020 Maintained by <a href="https://github.com/MaartenGr">Maarten</a>.
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
          Material for MkDocs
        </a>
</div>
</div>
</div>
</footer>
</div>
<script src="../assets/javascripts/vendor.0ac82a11.min.js"></script>
<script src="../assets/javascripts/bundle.f81dfb4d.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
<script>
        app = initialize({
          base: "..",
          features: [],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
</body>
</html>