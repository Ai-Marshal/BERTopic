
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Leveraging BERT and a class-based TF-IDF to create easily interpretable topics.">
      
      
      
        <meta name="author" content="Maarten P. Grootendorst">
      
      
        <link rel="canonical" href="https://maartengr.github.io/BERTopic/api/bertopic.html">
      
      <link rel="shortcut icon" href="../icon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.2.3">
    
    
      
        <title>BERTopic - BERTopic</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.3b61ea93.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.39b8e14a.min.css">
        
          
          
          <meta name="theme-color" content="#ffffff">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,400i,700%7CUbuntu+Mono&display=fallback">
        <style>body,input{font-family:"Ubuntu",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Ubuntu Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
      <link rel="stylesheet" href="../style.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="white" data-md-color-accent="blue">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#bertopic" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="https://maartengr.github.io/BERTopic/" title="BERTopic" class="md-header-nav__button md-logo" aria-label="BERTopic">
      
  <img src="../icon.png" alt="logo">

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      <div class="md-header-nav__ellipsis">
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            BERTopic
          </span>
        </div>
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            
              BERTopic
            
          </span>
        </div>
      </div>
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/MaartenGr/BERTopic/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://maartengr.github.io/BERTopic/" title="BERTopic" class="md-nav__button md-logo" aria-label="BERTopic">
      
  <img src="../icon.png" alt="logo">

    </a>
    BERTopic
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/MaartenGr/BERTopic/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      



  <li class="md-nav__item">
    <a href="../index.html" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      



  
  <li class="md-nav__item md-nav__item--nested">
    
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" >
    
    <label class="md-nav__link" for="nav-2">
      Guides
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Guides" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        <span class="md-nav__icon md-icon"></span>
        Guides
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          



  <li class="md-nav__item">
    <a href="../tutorial/quickstart/quickstart.html" class="md-nav__link">
      Getting Started
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../tutorial/embeddings/embeddings.html" class="md-nav__link">
      Custom Embeddings
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../tutorial/visualization/visualization.html" class="md-nav__link">
      Topic Visualization
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../tutorial/topicreduction/topicreduction.html" class="md-nav__link">
      Topic Reduction
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../tutorial/topicrepresentation/topicrepresentation.html" class="md-nav__link">
      Topic Representation
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../tutorial/search/search.html" class="md-nav__link">
      Search Topics
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      



  <li class="md-nav__item">
    <a href="../tutorial/algorithm/algorithm.html" class="md-nav__link">
      The Algorithm
    </a>
  </li>

    
      
      
      


  


  
  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      API
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="API" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon"></span>
        API
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        BERTopic
        <span class="md-nav__icon md-icon"></span>
      </label>
    
    <a href="bertopic.html" class="md-nav__link md-nav__link--active">
      BERTopic
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic" class="md-nav__link">
    bertopic._bertopic.BERTopic
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.find_topics" class="md-nav__link">
    find_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.fit" class="md-nav__link">
    fit()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.fit_transform" class="md-nav__link">
    fit_transform()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topic" class="md-nav__link">
    get_topic()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topic_freq" class="md-nav__link">
    get_topic_freq()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topics" class="md-nav__link">
    get_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.load" class="md-nav__link">
    load()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.reduce_topics" class="md-nav__link">
    reduce_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.save" class="md-nav__link">
    save()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.transform" class="md-nav__link">
    transform()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.update_topics" class="md-nav__link">
    update_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_distribution" class="md-nav__link">
    visualize_distribution()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_topics" class="md-nav__link">
    visualize_topics()
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="ctfidf.html" class="md-nav__link">
      cTFIDF
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      



  <li class="md-nav__item">
    <a href="../changelog.html" class="md-nav__link">
      Changelog
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic" class="md-nav__link">
    bertopic._bertopic.BERTopic
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.find_topics" class="md-nav__link">
    find_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.fit" class="md-nav__link">
    fit()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.fit_transform" class="md-nav__link">
    fit_transform()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topic" class="md-nav__link">
    get_topic()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topic_freq" class="md-nav__link">
    get_topic_freq()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.get_topics" class="md-nav__link">
    get_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.load" class="md-nav__link">
    load()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.reduce_topics" class="md-nav__link">
    reduce_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.save" class="md-nav__link">
    save()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.transform" class="md-nav__link">
    transform()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.update_topics" class="md-nav__link">
    update_topics()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_distribution" class="md-nav__link">
    visualize_distribution()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bertopic._bertopic.BERTopic.visualize_topics" class="md-nav__link">
    visualize_topics()
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/MaartenGr/BERTopic/edit/master/docs/api/bertopic.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="bertopic"><code>BERTopic</code><a class="headerlink" href="#bertopic" title="Permanent link">&para;</a></h1>
<div class="doc doc-object doc-class">
<h2 class="hidden-toc" id="bertopic._bertopic.BERTopic" style="visibility: hidden; position: absolute;">
        <a class="headerlink" href="#bertopic._bertopic.BERTopic" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents first">
<p>BERTopic is a topic modeling technique that leverages BERT embeddings and
c-TF-IDF to create dense clusters allowing for easily interpretable topics
whilst keeping important words in the topic descriptions.</p>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre>
</div>
<p>If you want to use your own embeddings, use it as follows:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Create embeddings</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">sentence_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create topic model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre>
</div>
<p>Due to the stochastisch nature of UMAP, the results from BERTopic might differ
and the quality can degrade. Using your own embeddings allows you to
try out BERTopic several times until you find the topics that suit
you best.</p>
<div class="doc doc-children">
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.__init__">

<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="s1">'english'</span><span class="p">,</span> <span class="n">embedding_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">top_n_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">min_topic_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_st_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

      <a class="headerlink" href="#bertopic._bertopic.BERTopic.__init__" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>BERTopic initialization</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>language</code></td>
<td><code>str</code></td>
<td>
<p>The main language used in your documents. For a full overview of supported languages
      see bertopic.embeddings.languages. Select "multilingual" to load in a model that
      support 50+ languages.</p>
</td>
<td><code>'english'</code></td>
</tr>
<tr>
<td><code>embedding_model</code></td>
<td><code>str</code></td>
<td>
<p>Model to use. Overview of options can be found here
            https://www.sbert.net/docs/pretrained_models.html</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>top_n_words</code></td>
<td><code>int</code></td>
<td>
<p>The number of words per topic to extract</p>
</td>
<td><code>10</code></td>
</tr>
<tr>
<td><code>nr_topics</code></td>
<td><code>Union[int, str]</code></td>
<td>
<p>Specifying the number of topics will reduce the initial
       number of topics to the value specified. This reduction can take
       a while as each reduction in topics (-1) activates a c-TF-IDF calculation.
       IF this is set to None, no reduction is applied. Use "auto" to automatically
       reduce topics that have a similarity of at least 0.9, do not maps all others.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>n_gram_range</code></td>
<td><code>Tuple[int, int]</code></td>
<td>
<p>The n-gram range for the CountVectorizer.
          Advised to keep high values between 1 and 3.
          More would likely lead to memory issues.
          Note that this will not be used if you pass in your own CountVectorizer.</p>
</td>
<td><code>(1, 1)</code></td>
</tr>
<tr>
<td><code>min_topic_size</code></td>
<td><code>int</code></td>
<td>
<p>The minimum size of the topic.</p>
</td>
<td><code>10</code></td>
</tr>
<tr>
<td><code>n_neighbors</code></td>
<td><code>int</code></td>
<td>
<p>The size of local neighborhood (in terms of number of neighboring sample points) used
         for manifold approximation (UMAP).</p>
</td>
<td><code>15</code></td>
</tr>
<tr>
<td><code>n_components</code></td>
<td><code>int</code></td>
<td>
<p>The dimension of the space to embed into when reducing dimensionality with UMAP.</p>
</td>
<td><code>5</code></td>
</tr>
<tr>
<td><code>stop_words</code></td>
<td><code>Union[str, List[str]]</code></td>
<td>
<p>Stopwords that can be used as either a list of strings, or the name of the
        language as a string. For example: 'english' or ['the', 'and', 'I'].
        Note that this will not be used if you pass in your own CountVectorizer.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>verbose</code></td>
<td><code>bool</code></td>
<td>
<p>Changes the verbosity of the model, Set to True if you want
     to track the stages of the model.</p>
</td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>vectorizer</code></td>
<td><code>CountVectorizer</code></td>
<td>
<p>Pass in your own CountVectorizer from scikit-learn</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>calculate_probabilities</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to calculate the topic probabilities. This could slow down
                     extraction of topics if you have many documents (&gt;100_000). If so,
                     set this to False to increase speed.</p>
</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>allow_st_model</code></td>
<td><code>bool</code></td>
<td>
<p>This allows BERTopic to use a multi-lingual version of SentenceTransformer
            to be used to fine-tune the topic words extracted from the c-TF-IDF representation.
            Moreover, it will allow you to search for topics based on search queries.</p>
</td>
<td><code>True</code></td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">language</span> <span class="o">=</span> <span class="s2">"english"</span><span class="p">,</span>
                 <span class="n">embedding_model</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">top_n_words</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                 <span class="n">nr_topics</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span>
                 <span class="n">n_gram_range</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                 <span class="n">min_topic_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                 <span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
                 <span class="n">n_components</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                 <span class="n">stop_words</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">vectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">allow_st_model</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">language</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"english"</span><span class="p">,</span>
             <span class="n">embedding_model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">top_n_words</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
             <span class="n">nr_topics</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">n_gram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
             <span class="n">min_topic_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
             <span class="n">n_neighbors</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
             <span class="n">n_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
             <span class="n">stop_words</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
             <span class="n">vectorizer</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">calculate_probabilities</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
             <span class="n">allow_st_model</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sd">"""BERTopic initialization</span>

<span class="sd">    Args:</span>
<span class="sd">        language: The main language used in your documents. For a full overview of supported languages</span>
<span class="sd">                  see bertopic.embeddings.languages. Select "multilingual" to load in a model that</span>
<span class="sd">                  support 50+ languages.</span>
<span class="sd">        embedding_model: Model to use. Overview of options can be found here</span>
<span class="sd">                        https://www.sbert.net/docs/pretrained_models.html</span>
<span class="sd">        top_n_words: The number of words per topic to extract</span>
<span class="sd">        nr_topics: Specifying the number of topics will reduce the initial</span>
<span class="sd">                   number of topics to the value specified. This reduction can take</span>
<span class="sd">                   a while as each reduction in topics (-1) activates a c-TF-IDF calculation.</span>
<span class="sd">                   IF this is set to None, no reduction is applied. Use "auto" to automatically</span>
<span class="sd">                   reduce topics that have a similarity of at least 0.9, do not maps all others.</span>
<span class="sd">        n_gram_range: The n-gram range for the CountVectorizer.</span>
<span class="sd">                      Advised to keep high values between 1 and 3.</span>
<span class="sd">                      More would likely lead to memory issues.</span>
<span class="sd">                      Note that this will not be used if you pass in your own CountVectorizer.</span>
<span class="sd">        min_topic_size: The minimum size of the topic.</span>
<span class="sd">        n_neighbors: The size of local neighborhood (in terms of number of neighboring sample points) used</span>
<span class="sd">                     for manifold approximation (UMAP).</span>
<span class="sd">        n_components: The dimension of the space to embed into when reducing dimensionality with UMAP.</span>
<span class="sd">        stop_words: Stopwords that can be used as either a list of strings, or the name of the</span>
<span class="sd">                    language as a string. For example: 'english' or ['the', 'and', 'I'].</span>
<span class="sd">                    Note that this will not be used if you pass in your own CountVectorizer.</span>
<span class="sd">        verbose: Changes the verbosity of the model, Set to True if you want</span>
<span class="sd">                 to track the stages of the model.</span>
<span class="sd">        vectorizer: Pass in your own CountVectorizer from scikit-learn</span>
<span class="sd">        calculate_probabilities: Whether to calculate the topic probabilities. This could slow down</span>
<span class="sd">                                 extraction of topics if you have many documents (&gt;100_000). If so,</span>
<span class="sd">                                 set this to False to increase speed.</span>
<span class="sd">        allow_st_model: This allows BERTopic to use a multi-lingual version of SentenceTransformer</span>
<span class="sd">                        to be used to fine-tune the topic words extracted from the c-TF-IDF representation.</span>
<span class="sd">                        Moreover, it will allow you to search for topics based on search queries.</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    model = BERTopic(language = "english",</span>
<span class="sd">                     embedding_model = None,</span>
<span class="sd">                     top_n_words = 10,</span>
<span class="sd">                     nr_topics = 30,</span>
<span class="sd">                     n_gram_range = (1, 1),</span>
<span class="sd">                     min_topic_size = 10,</span>
<span class="sd">                     n_neighbors = 15,</span>
<span class="sd">                     n_components = 5,</span>
<span class="sd">                     stop_words = None,</span>
<span class="sd">                     verbose = True,</span>
<span class="sd">                     vectorizer = None,</span>
<span class="sd">                     allow_st_model = True)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>

    <span class="c1"># Embedding model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">language</span> <span class="o">=</span> <span class="n">language</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">embedding_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">allow_st_model</span> <span class="o">=</span> <span class="n">allow_st_model</span>

    <span class="c1"># Topic-based parameters</span>
    <span class="k">if</span> <span class="n">top_n_words</span> <span class="o">&gt;</span> <span class="mi">30</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"top_n_words should be lower or equal to 30. The preferred value is 10."</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">top_n_words</span> <span class="o">=</span> <span class="n">top_n_words</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nr_topics</span> <span class="o">=</span> <span class="n">nr_topics</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_topic_size</span> <span class="o">=</span> <span class="n">min_topic_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">calculate_probabilities</span> <span class="o">=</span> <span class="n">calculate_probabilities</span>

    <span class="c1"># Umap parameters</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">n_neighbors</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>

    <span class="c1"># Vectorizer parameters</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stop_words</span> <span class="o">=</span> <span class="n">stop_words</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_gram_range</span> <span class="o">=</span> <span class="n">n_gram_range</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">vectorizer</span> <span class="ow">or</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_gram_range</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stop_words</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">umap_model</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cluster_model</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">topics</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reduced_topics_mapped</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mapped_topics</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">topic_embeddings</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">topic_sim_matrix</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">custom_embeddings</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">set_level</span><span class="p">(</span><span class="s2">"DEBUG"</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.find_topics">

<code class="highlight language-python"><span class="n">find_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">search_term</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.find_topics" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Find topics most similar to a search_term</p>
<p>Creates an embedding for search_term and compares that with
the topic embeddings. The most similar topics are returned
along with their similarity values.</p>
<p>The search_term can be of any size but since it compares
with the topic representation it is advised to keep it
below 5 words.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>search_term</code></td>
<td><code>str</code></td>
<td>
<p>the term you want to use to search for topics</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>top_n</code></td>
<td><code>int</code></td>
<td>
<p>the number of topics to return</p>
</td>
<td><code>5</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tuple[List[int], List[float]]</code></td>
<td>
<p>similar_topics: the most similar topics from high to low
similarity: the similarity scores from high to low</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">find_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">search_term</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                <span class="n">top_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
    <span class="sd">""" Find topics most similar to a search_term</span>

<span class="sd">    Creates an embedding for search_term and compares that with</span>
<span class="sd">    the topic embeddings. The most similar topics are returned</span>
<span class="sd">    along with their similarity values.</span>

<span class="sd">    The search_term can be of any size but since it compares</span>
<span class="sd">    with the topic representation it is advised to keep it</span>
<span class="sd">    below 5 words.</span>

<span class="sd">    Args:</span>
<span class="sd">        search_term: the term you want to use to search for topics</span>
<span class="sd">        top_n: the number of topics to return</span>

<span class="sd">    Returns:</span>
<span class="sd">        similar_topics: the most similar topics from high to low</span>
<span class="sd">        similarity: the similarity scores from high to low</span>

<span class="sd">    """</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_embeddings</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">allow_st_model</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">"This method can only be used if you set `allow_st_model` to True when "</span>
                        <span class="s2">"using custom embeddings."</span><span class="p">)</span>

    <span class="n">topic_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">topic_list</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="c1"># Extract search_term embeddings and compare with topic embeddings</span>
    <span class="n">search_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_embeddings</span><span class="p">([</span><span class="n">search_term</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">sims</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">search_embedding</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="c1"># Extract topics most similar to search_term</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">sims</span><span class="p">)[</span><span class="o">-</span><span class="n">top_n</span><span class="p">:]</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="p">[</span><span class="n">sims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">similar_topics</span> <span class="o">=</span> <span class="p">[</span><span class="n">topic_list</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">ids</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.fit">

<code class="highlight language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.fit" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>documents</code></td>
<td><code>List[str]</code></td>
<td>
<p>A list of documents to fit on</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>embeddings</code></td>
<td><code>ndarray</code></td>
<td>
<p>Pre-trained document embeddings. These can be used
        instead of the sentence-transformer model</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre>
</div>
<p>If you want to use your own embeddings, use it as follows:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Create embeddings</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">sentence_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create topic model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">documents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">""" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics</span>

<span class="sd">    Arguments:</span>
<span class="sd">        documents: A list of documents to fit on</span>
<span class="sd">        embeddings: Pre-trained document embeddings. These can be used</span>
<span class="sd">                    instead of the sentence-transformer model</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>

<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>
<span class="sd">    model = BERTopic("distilbert-base-nli-mean-tokens", verbose=True).fit(docs)</span>
<span class="sd">    ```</span>

<span class="sd">    If you want to use your own embeddings, use it as follows:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>
<span class="sd">    from sentence_transformers import SentenceTransformer</span>

<span class="sd">    # Create embeddings</span>
<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>
<span class="sd">    sentence_model = SentenceTransformer("distilbert-base-nli-mean-tokens")</span>
<span class="sd">    embeddings = sentence_model.encode(docs, show_progress_bar=True)</span>

<span class="sd">    # Create topic model</span>
<span class="sd">    model = BERTopic(None, verbose=True).fit(docs, embeddings)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.fit_transform">

<code class="highlight language-python"><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.fit_transform" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Fit the models on a collection of documents, generate topics, and return the docs with topics</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>documents</code></td>
<td><code>List[str]</code></td>
<td>
<p>A list of documents to fit on</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>embeddings</code></td>
<td><code>ndarray</code></td>
<td>
<p>Pre-trained document embeddings. These can be used
        instead of the sentence-transformer model</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tuple[List[int], Optional[numpy.ndarray]]</code></td>
<td>
<p>predictions: Topic predictions for each documents
probabilities: The topic probability distribution</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre>
</div>
<p>If you want to use your own embeddings, use it as follows:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Create embeddings</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">sentence_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create topic model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">documents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                  <span class="n">embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                                          <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="kc">None</span><span class="p">]]:</span>
    <span class="sd">""" Fit the models on a collection of documents, generate topics, and return the docs with topics</span>

<span class="sd">    Arguments:</span>
<span class="sd">        documents: A list of documents to fit on</span>
<span class="sd">        embeddings: Pre-trained document embeddings. These can be used</span>
<span class="sd">                    instead of the sentence-transformer model</span>

<span class="sd">    Returns:</span>
<span class="sd">        predictions: Topic predictions for each documents</span>
<span class="sd">        probabilities: The topic probability distribution</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>

<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>

<span class="sd">    model = BERTopic("distilbert-base-nli-mean-tokens", verbose=True)</span>
<span class="sd">    topics = model.fit_transform(docs)</span>
<span class="sd">    ```</span>

<span class="sd">    If you want to use your own embeddings, use it as follows:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>
<span class="sd">    from sentence_transformers import SentenceTransformer</span>

<span class="sd">    # Create embeddings</span>
<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>
<span class="sd">    sentence_model = SentenceTransformer("distilbert-base-nli-mean-tokens")</span>
<span class="sd">    embeddings = sentence_model.encode(docs, show_progress_bar=True)</span>

<span class="sd">    # Create topic model</span>
<span class="sd">    model = BERTopic(None, verbose=True)</span>
<span class="sd">    topics = model.fit_transform(docs, embeddings)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_documents_type</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
    <span class="n">check_embeddings_shape</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">documents</span><span class="p">)</span>

    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"Document"</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span>
                              <span class="s2">"ID"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">)),</span>
                              <span class="s2">"Topic"</span><span class="p">:</span> <span class="kc">None</span><span class="p">})</span>

    <span class="c1"># Extract embeddings</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">),</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">csr_matrix</span><span class="p">)]):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_embeddings</span><span class="p">(</span><span class="n">documents</span><span class="o">.</span><span class="n">Document</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">custom_embeddings</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Reduce dimensionality with UMAP</span>
    <span class="n">umap_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_dimensionality</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

    <span class="c1"># Cluster UMAP embeddings with HDBSCAN</span>
    <span class="n">documents</span><span class="p">,</span> <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cluster_embeddings</span><span class="p">(</span><span class="n">umap_embeddings</span><span class="p">,</span> <span class="n">documents</span><span class="p">)</span>

    <span class="c1"># Extract topics by calculating c-TF-IDF</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_extract_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nr_topics</span><span class="p">:</span>
        <span class="n">documents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">probabilities</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.get_topic">

<code class="highlight language-python"><span class="n">get_topic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.get_topic" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Return top n words for a specific topic and their c-TF-IDF scores</p>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">topic</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">get_topic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">]:</span>
    <span class="sd">""" Return top n words for a specific topic and their c-TF-IDF scores</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    topic = model.get_topic(12)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">topic</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.get_topic_freq">

<code class="highlight language-python"><span class="n">get_topic_freq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.get_topic_freq" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Return the the size of topics (descending order)</p>
<p>Usage:</p>
<p>To extract the frequency of all topics:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">frequency</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_freq</span><span class="p">()</span>
</code></pre>
</div>
<p>To get the frequency of a single topic:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">frequency</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_freq</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">get_topic_freq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="sd">""" Return the the size of topics (descending order)</span>

<span class="sd">    Usage:</span>

<span class="sd">    To extract the frequency of all topics:</span>

<span class="sd">    ```python</span>
<span class="sd">    frequency = model.get_topic_freq()</span>
<span class="sd">    ```</span>

<span class="sd">    To get the frequency of a single topic:</span>

<span class="sd">    ```python</span>
<span class="sd">    frequency = model.get_topic_freq(12)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'Topic'</span><span class="p">,</span> <span class="s1">'Count'</span><span class="p">])</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">"Count"</span><span class="p">,</span>
                                                                                              <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.get_topics">

<code class="highlight language-python"><span class="n">get_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.get_topics" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Return topics with top n words and their c-TF-IDF score</p>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">all_topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topics</span><span class="p">()</span>
</code></pre>
</div>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">get_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>
    <span class="sd">""" Return topics with top n words and their c-TF-IDF score</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    all_topics = model.get_topics()</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">topics</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.load">

<code class="highlight language-python"><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-classmethod"><code>classmethod</code></small>
  </span>

      <a class="headerlink" href="#bertopic._bertopic.BERTopic.load" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Loads the model from the specified path</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>path</code></td>
<td><code>str</code></td>
<td>
<p>the location and name of the BERTopic file you want to load</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">BERTopic</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"my_model"</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="sd">""" Loads the model from the specified path</span>

<span class="sd">    Arguments:</span>
<span class="sd">        path: the location and name of the BERTopic file you want to load</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    BERTopic.load("my_model")</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.reduce_topics">

<code class="highlight language-python"><span class="n">reduce_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">probabilities</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.reduce_topics" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Further reduce the number of topics to nr_topics.</p>
<p>The number of topics is further reduced by calculating the c-TF-IDF matrix
of the documents and then reducing them by iteratively merging the least
frequent topic with the most similar one based on their c-TF-IDF matrices.
The topics, their sizes, and representations are updated.</p>
<p>The reasoning for putting <code>docs</code>, <code>topics</code>, and <code>probs</code> as parameters is that
these values are not saved within BERTopic on purpose. If you were to have a
million documents, it seems very inefficient to save those in BERTopic
instead of a dedicated database.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>docs</code></td>
<td><code>List[str]</code></td>
<td>
<p>The docs you used when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>topics</code></td>
<td><code>List[int]</code></td>
<td>
<p>The topics that were returned when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>nr_topics</code></td>
<td><code>int</code></td>
<td>
<p>The number of topics you want reduced to</p>
</td>
<td><code>20</code></td>
</tr>
<tr>
<td><code>probabilities</code></td>
<td><code>ndarray</code></td>
<td>
<p>The probabilities that were returned when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tuple[List[int], numpy.ndarray]</code></td>
<td>
<p>new_topics: Updated topics
new_probabilities: Updated probabilities</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="c1"># Create topics -&gt; Typically over 50 topics</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'train'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">()</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="c1"># Further reduce topics</span>
<span class="n">new_topics</span><span class="p">,</span> <span class="n">new_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">reduce_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">reduce_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">docs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                  <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">probabilities</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                  <span class="n">nr_topics</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="sd">""" Further reduce the number of topics to nr_topics.</span>

<span class="sd">    The number of topics is further reduced by calculating the c-TF-IDF matrix</span>
<span class="sd">    of the documents and then reducing them by iteratively merging the least</span>
<span class="sd">    frequent topic with the most similar one based on their c-TF-IDF matrices.</span>
<span class="sd">    The topics, their sizes, and representations are updated.</span>

<span class="sd">    The reasoning for putting `docs`, `topics`, and `probs` as parameters is that</span>
<span class="sd">    these values are not saved within BERTopic on purpose. If you were to have a</span>
<span class="sd">    million documents, it seems very inefficient to save those in BERTopic</span>
<span class="sd">    instead of a dedicated database.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        docs: The docs you used when calling either `fit` or `fit_transform`</span>
<span class="sd">        topics: The topics that were returned when calling either `fit` or `fit_transform`</span>
<span class="sd">        nr_topics: The number of topics you want reduced to</span>
<span class="sd">        probabilities: The probabilities that were returned when calling either `fit` or `fit_transform`</span>

<span class="sd">    Returns:</span>
<span class="sd">        new_topics: Updated topics</span>
<span class="sd">        new_probabilities: Updated probabilities</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>

<span class="sd">    # Create topics -&gt; Typically over 50 topics</span>
<span class="sd">    docs = fetch_20newsgroups(subset='train')['data']</span>
<span class="sd">    model = BERTopic()</span>
<span class="sd">    topics, probs = model.fit_transform(docs)</span>

<span class="sd">    # Further reduce topics</span>
<span class="sd">    new_topics, new_probs = model.reduce_topics(docs, topics, probs, nr_topics=30)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nr_topics</span> <span class="o">=</span> <span class="n">nr_topics</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"Document"</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">"Topic"</span><span class="p">:</span> <span class="n">topics</span><span class="p">})</span>

    <span class="c1"># Reduce number of topics</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_extract_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
    <span class="n">new_topics</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">Topic</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
    <span class="n">new_probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">new_topics</span><span class="p">,</span> <span class="n">new_probabilities</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.save">

<code class="highlight language-python"><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.save" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Saves the model to the specified path</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>path</code></td>
<td><code>str</code></td>
<td>
<p>the location and name of the file you want to save</p>
</td>
<td><em>required</em></td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"my_model"</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">""" Saves the model to the specified path</span>

<span class="sd">    Arguments:</span>
<span class="sd">        path: the location and name of the file you want to save</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    model.save("my_model")</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.transform">

<code class="highlight language-python"><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.transform" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>After having fit a model, use transform to predict new instances</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>documents</code></td>
<td><code>Union[str, List[str]]</code></td>
<td>
<p>A single document or a list of documents to fit on</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>embeddings</code></td>
<td><code>ndarray</code></td>
<td>
<p>Pre-trained document embeddings. These can be used
        instead of the sentence-transformer model.</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Tuple[List[int], numpy.ndarray]</code></td>
<td>
<p>predictions: Topic predictions for each documents
probabilities: The topic probability distribution</p>
</td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre>
</div>
<p>If you want to use your own embeddings:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Create embeddings</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'all'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">sentence_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">"distilbert-base-nli-mean-tokens"</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">sentence_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create topic model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="n">topics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">documents</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
              <span class="n">embeddings</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="sd">""" After having fit a model, use transform to predict new instances</span>

<span class="sd">    Arguments:</span>
<span class="sd">        documents: A single document or a list of documents to fit on</span>
<span class="sd">        embeddings: Pre-trained document embeddings. These can be used</span>
<span class="sd">                    instead of the sentence-transformer model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        predictions: Topic predictions for each documents</span>
<span class="sd">        probabilities: The topic probability distribution</span>

<span class="sd">    Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>

<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>
<span class="sd">    model = BERTopic("distilbert-base-nli-mean-tokens", verbose=True).fit(docs)</span>
<span class="sd">    topics = model.transform(docs)</span>
<span class="sd">    ```</span>

<span class="sd">    If you want to use your own embeddings:</span>

<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>
<span class="sd">    from sentence_transformers import SentenceTransformer</span>

<span class="sd">    # Create embeddings</span>
<span class="sd">    docs = fetch_20newsgroups(subset='all')['data']</span>
<span class="sd">    sentence_model = SentenceTransformer("distilbert-base-nli-mean-tokens")</span>
<span class="sd">    embeddings = sentence_model.encode(docs, show_progress_bar=True)</span>

<span class="sd">    # Create topic model</span>
<span class="sd">    model = BERTopic(None, verbose=True).fit(docs, embeddings)</span>
<span class="sd">    topics = model.transform(docs, embeddings)</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">check_embeddings_shape</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">documents</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">documents</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_embeddings</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

    <span class="n">umap_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">umap_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hdbscan</span><span class="o">.</span><span class="n">approximate_predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster_model</span><span class="p">,</span> <span class="n">umap_embeddings</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_probabilities</span><span class="p">:</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">hdbscan</span><span class="o">.</span><span class="n">membership_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cluster_model</span><span class="p">,</span> <span class="n">umap_embeddings</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapped_topics</span><span class="p">:</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_predictions</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_map_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">probabilities</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.update_topics">

<code class="highlight language-python"><span class="n">update_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.update_topics" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Updates the topic representation by recalculating c-TF-IDF with the new
parameters as defined in this function.</p>
<p>When you have trained a model and viewed the topics and the words that represent them,
you might not be satisfied with the representation. Perhaps you forgot to remove
stop_words or you want to try out a different n_gram_range. This function allows you
to update the topic representation after they have been formed.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>docs</code></td>
<td><code>List[str]</code></td>
<td>
<p>The docs you used when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>topics</code></td>
<td><code>List[int]</code></td>
<td>
<p>The topics that were returned when calling either <code>fit</code> or <code>fit_transform</code></p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>n_gram_range</code></td>
<td><code>Tuple[int, int]</code></td>
<td>
<p>The n-gram range for the CountVectorizer.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>stop_words</code></td>
<td><code>str</code></td>
<td>
<p>Stopwords that can be used as either a list of strings, or the name of the
        language as a string. For example: 'english' or ['the', 'and', 'I'].
        Note that this will not be used if you pass in your own CountVectorizer.</p>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>vectorizer</code></td>
<td><code>CountVectorizer</code></td>
<td>
<p>Pass in your own CountVectorizer from scikit-learn</p>
</td>
<td><code>None</code></td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<div class="codehilite">
<pre><span></span><code><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="c1"># Create topics</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'train'</span><span class="p">)[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

<span class="c1"># Update topic representation</span>
<span class="n">model</span><span class="o">.</span><span class="n">update_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="s2">"english"</span><span class="p">)</span>
</code></pre>
</div>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">update_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">docs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
                  <span class="n">topics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">n_gram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                  <span class="n">stop_words</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                  <span class="n">vectorizer</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">""" Updates the topic representation by recalculating c-TF-IDF with the new</span>
<span class="sd">    parameters as defined in this function.</span>

<span class="sd">    When you have trained a model and viewed the topics and the words that represent them,</span>
<span class="sd">    you might not be satisfied with the representation. Perhaps you forgot to remove</span>
<span class="sd">    stop_words or you want to try out a different n_gram_range. This function allows you</span>
<span class="sd">    to update the topic representation after they have been formed.</span>

<span class="sd">    Args:</span>
<span class="sd">        docs: The docs you used when calling either `fit` or `fit_transform`</span>
<span class="sd">        topics: The topics that were returned when calling either `fit` or `fit_transform`</span>
<span class="sd">        n_gram_range: The n-gram range for the CountVectorizer.</span>
<span class="sd">        stop_words: Stopwords that can be used as either a list of strings, or the name of the</span>
<span class="sd">                    language as a string. For example: 'english' or ['the', 'and', 'I'].</span>
<span class="sd">                    Note that this will not be used if you pass in your own CountVectorizer.</span>
<span class="sd">        vectorizer: Pass in your own CountVectorizer from scikit-learn</span>

<span class="sd">    Usage:</span>
<span class="sd">    ```python</span>
<span class="sd">    from bertopic import BERTopic</span>
<span class="sd">    from sklearn.datasets import fetch_20newsgroups</span>

<span class="sd">    # Create topics</span>
<span class="sd">    docs = fetch_20newsgroups(subset='train')['data']</span>
<span class="sd">    model = BERTopic(n_gram_range=(1, 1), stop_words=None)</span>
<span class="sd">    topics, probs = model.fit_transform(docs)</span>

<span class="sd">    # Update topic representation</span>
<span class="sd">    model.update_topics(docs, topics, n_gram_range=(2, 3), stop_words="english")</span>
<span class="sd">    ```</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">n_gram_range</span><span class="p">:</span>
        <span class="n">n_gram_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_gram_range</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">stop_words</span><span class="p">:</span>
        <span class="n">stop_words</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_words</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">vectorizer</span> <span class="ow">or</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="n">n_gram_range</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span><span class="p">)</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"Document"</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">"Topic"</span><span class="p">:</span> <span class="n">topics</span><span class="p">})</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_extract_topics</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.visualize_distribution">

<code class="highlight language-python"><span class="n">visualize_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">,</span> <span class="n">min_probability</span><span class="o">=</span><span class="mf">0.015</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">save</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.visualize_distribution" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Visualize the distribution of topic probabilities</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>probabilities</code></td>
<td><code>ndarray</code></td>
<td>
<p>An array of probability scores</p>
</td>
<td><em>required</em></td>
</tr>
<tr>
<td><code>min_probability</code></td>
<td><code>float</code></td>
<td>
<p>The minimum probability score to visualize.
             All others are ignored.</p>
</td>
<td><code>0.015</code></td>
</tr>
<tr>
<td><code>figsize</code></td>
<td><code>tuple</code></td>
<td>
<p>The size of the figure</p>
</td>
<td><code>(10, 5)</code></td>
</tr>
<tr>
<td><code>save</code></td>
<td><code>bool</code></td>
<td>
<p>Whether to save the resulting graph to probility.png</p>
</td>
<td><code>False</code></td>
</tr>
</tbody>
</table>
<p>Usage:</p>
<p>Make sure to fit the model before and only input the
probabilities of a single document:</p>
<div class="codehilite">
<pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">visualize_distribution</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre>
</div>
<p><img alt="" src="../img/probabilities.png" /></p>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">visualize_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">probabilities</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                           <span class="n">min_probability</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.015</span><span class="p">,</span>
                           <span class="n">figsize</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                           <span class="n">save</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="sd">""" Visualize the distribution of topic probabilities</span>

<span class="sd">    Arguments:</span>
<span class="sd">        probabilities: An array of probability scores</span>
<span class="sd">        min_probability: The minimum probability score to visualize.</span>
<span class="sd">                         All others are ignored.</span>
<span class="sd">        figsize: The size of the figure</span>
<span class="sd">        save: Whether to save the resulting graph to probility.png</span>

<span class="sd">    Usage:</span>

<span class="sd">    Make sure to fit the model before and only input the</span>
<span class="sd">    probabilities of a single document:</span>

<span class="sd">    ```python</span>
<span class="sd">    model.visualize_distribution(probabilities[0])</span>
<span class="sd">    ```</span>

<span class="sd">    ![](../img/probabilities.png)</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_HAS_VIZ</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ModuleNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"In order to use this function you'll need to install "</span>
                                  <span class="sa">f</span><span class="s2">"additional dependencies;</span><span class="se">\n</span><span class="s2">pip install bertopic[visualization]"</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="n">probabilities</span> <span class="o">&gt;</span> <span class="n">min_probability</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"There are no values where `min_probability` is higher than the "</span>
                         <span class="s2">"probabilities that were supplied. Lower `min_probability` to prevent this error."</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_probabilities</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"This visualization cannot be used if you have set `calculate_probabilities` to False "</span>
                         <span class="s2">"as it uses the topic probabilities. "</span><span class="p">)</span>

    <span class="c1"># Get values and indices equal or exceed the minimum probability</span>
    <span class="n">labels_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">probabilities</span> <span class="o">&gt;=</span> <span class="n">min_probability</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">vals</span> <span class="o">=</span> <span class="n">probabilities</span><span class="p">[</span><span class="n">labels_idx</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Create labels</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">labels_idx</span><span class="p">:</span>
        <span class="n">label</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">words</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">words</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
                <span class="n">label</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">label</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="sa">r</span><span class="s2">"$\bf{Topic }$ "</span> <span class="o">+</span>
                        <span class="sa">r</span><span class="s2">"$\bf{"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">+</span> <span class="s2">":}$ "</span> <span class="o">+</span>
                        <span class="s2">" "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>
            <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="n">vals</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>

    <span class="c1"># Create figure</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">pos</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">vals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'#333F4B'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">vals</span><span class="p">),</span> <span class="n">xmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">vals</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">'#333F4B'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

    <span class="c1"># Set ticks and labels</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">'both'</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Probability'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'#333F4B'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">'Topic Probability Distribution'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'#333F4B'</span><span class="p">)</span>

    <span class="c1"># Update spine style</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'right'</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'top'</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'left'</span><span class="p">]</span><span class="o">.</span><span class="n">set_bounds</span><span class="p">(</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'bottom'</span><span class="p">]</span><span class="o">.</span><span class="n">set_bounds</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'bottom'</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">((</span><span class="s1">'axes'</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">'left'</span><span class="p">]</span><span class="o">.</span><span class="n">set_position</span><span class="p">((</span><span class="s1">'axes'</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">))</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">save</span><span class="p">:</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">"probability.png"</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">'tight'</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
<div class="doc doc-object doc-method">
<h2 class="doc doc-heading" id="bertopic._bertopic.BERTopic.visualize_topics">

<code class="highlight language-python"><span class="n">visualize_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


      <a class="headerlink" href="#bertopic._bertopic.BERTopic.visualize_topics" title="Permanent link">&para;</a></h2>
<div class="doc doc-contents ">
<p>Visualize topics, their sizes, and their corresponding words</p>
<p>This visualization is highly inspired by LDAvis, a great visualization
technique typically reserved for LDA.</p>
<details class="quote">
          <summary>Source code in <code>bertopic\_bertopic.py</code></summary>
          <div class="highlight">
<pre><span></span><code><span class="k">def</span> <span class="nf">visualize_topics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">""" Visualize topics, their sizes, and their corresponding words</span>

<span class="sd">    This visualization is highly inspired by LDAvis, a great visualization</span>
<span class="sd">    technique typically reserved for LDA.</span>
<span class="sd">    """</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_HAS_VIZ</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ModuleNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"In order to use this function you'll need to install "</span>
                                  <span class="sa">f</span><span class="s2">"additional dependencies;</span><span class="se">\n</span><span class="s2">pip install bertopic[visualization]"</span><span class="p">)</span>

    <span class="c1"># Extract topic words and their frequencies</span>
    <span class="n">topic_list</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="n">frequencies</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">topic_sizes</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topic_list</span><span class="p">]</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">" | "</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="n">topic</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]])</span> <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="n">topic_list</span><span class="p">]</span>

    <span class="c1"># Embed c-TF-IDF into 2D</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_tf_idf</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">'hellinger'</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

    <span class="c1"># Visualize with plotly</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">"x"</span><span class="p">:</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">"y"</span><span class="p">:</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">1</span><span class="p">],</span>
                       <span class="s2">"Topic"</span><span class="p">:</span> <span class="n">topic_list</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s2">"Words"</span><span class="p">:</span> <span class="n">words</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s2">"Size"</span><span class="p">:</span> <span class="n">frequencies</span><span class="p">[</span><span class="mi">1</span><span class="p">:]})</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_plotly_topic_visualization</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">topic_list</span><span class="p">)</span>
</code></pre>
</div>
</details>
</div>
</div>
</div>
</div>
</div>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../tutorial/algorithm/algorithm.html" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                The Algorithm
              </div>
            </div>
          </a>
        
        
          <a href="ctfidf.html" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                cTFIDF
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2020 Maintained by <a href="https://github.com/MaartenGr">Maarten</a>.
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.08c56446.min.js"></script>
      <script src="../assets/javascripts/bundle.6ced434e.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: [],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.8c7e0a7e.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>